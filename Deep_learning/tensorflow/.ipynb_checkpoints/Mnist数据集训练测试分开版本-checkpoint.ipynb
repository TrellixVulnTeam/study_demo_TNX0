{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1201 11:33:49.798818 140421678028608 deprecation.py:323] From <ipython-input-1-39a51f8e8617>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1201 11:33:49.800527 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1201 11:33:49.801787 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 11:33:50.042194 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1201 11:33:50.045698 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1201 11:33:50.095971 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 11:33:50.342240 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 3.10689.\n",
      "After 1001 training step(s), loss on training batch is 0.251102.\n",
      "After 2001 training step(s), loss on training batch is 0.166322.\n",
      "After 3001 training step(s), loss on training batch is 0.131995.\n",
      "After 4001 training step(s), loss on training batch is 0.11263.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 11:34:27.204476 140421678028608 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5001 training step(s), loss on training batch is 0.115287.\n",
      "After 6001 training step(s), loss on training batch is 0.0956464.\n",
      "After 7001 training step(s), loss on training batch is 0.0801559.\n",
      "After 8001 training step(s), loss on training batch is 0.0718334.\n",
      "After 9001 training step(s), loss on training batch is 0.0667971.\n",
      "After 10001 training step(s), loss on training batch is 0.0624781.\n",
      "After 11001 training step(s), loss on training batch is 0.060866.\n",
      "After 12001 training step(s), loss on training batch is 0.0541651.\n",
      "After 13001 training step(s), loss on training batch is 0.0514227.\n",
      "After 14001 training step(s), loss on training batch is 0.0548598.\n",
      "After 15001 training step(s), loss on training batch is 0.0456667.\n",
      "After 16001 training step(s), loss on training batch is 0.044763.\n",
      "After 17001 training step(s), loss on training batch is 0.0460693.\n",
      "After 18001 training step(s), loss on training batch is 0.0438037.\n",
      "After 19001 training step(s), loss on training batch is 0.037962.\n",
      "After 20001 training step(s), loss on training batch is 0.040088.\n",
      "After 21001 training step(s), loss on training batch is 0.0363493.\n",
      "After 22001 training step(s), loss on training batch is 0.0345501.\n",
      "After 23001 training step(s), loss on training batch is 0.0380763.\n",
      "After 24001 training step(s), loss on training batch is 0.0354545.\n",
      "After 25001 training step(s), loss on training batch is 0.0329066.\n",
      "After 26001 training step(s), loss on training batch is 0.0329953.\n",
      "After 27001 training step(s), loss on training batch is 0.0318183.\n",
      "After 28001 training step(s), loss on training batch is 0.0392175.\n",
      "After 29001 training step(s), loss on training batch is 0.0356826.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "batch_size = 100\n",
    "learning_rate = 0.8\n",
    "learning_rate_decay = 0.999\n",
    "max_steps = 30000\n",
    "\n",
    "#更改前向传播算法的定义,将得到权重参数和偏执参数的过程封装到了一个函数中\n",
    "def hidden_layer(input_tensor,regularizer,name):\n",
    "#要多多体会使用变量空间来管理变量的方便性。使用get_variable()函数会在训练神经\n",
    "#网络时创建这些变量而在测试过程中通过保存的模型加载这些变量的取值，在测试过程中\n",
    "#可以在加载变量时将滑动平均变量重命名，这样就会在测试过程中使用变量的滑动平均值\n",
    "    with tf.variable_scope(\"hidden_layer\"):\n",
    "        weights = tf.get_variable(\"weights\", [784, 500],\n",
    "                         initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        #如果调用该函数时传入了正则化的方法 ，那么在这里将参数求\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection(\"losses\",regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [500], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_output\"):\n",
    "        weights = tf.get_variable(\"weights\", [500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection(\"losses\",regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer_output = tf.matmul(hidden_layer, weights) + biases\n",
    "    return hidden_layer_output\n",
    "\n",
    "#定义输出输出的部分没变\n",
    "x = tf.placeholder(tf.float32, [None,784],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None,10],name=\"y-output\")\n",
    "\n",
    "#定义L2正则化的办法被提前\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "#将L2正则化的办法传入到hidden_layer()函数中\n",
    "y = hidden_layer(x,regularizer,name=\"y\")\n",
    "\n",
    "training_step = tf.Variable(0,trainable=False)\n",
    "averages_class = tf.train.ExponentialMovingAverage(0.99,training_step)\n",
    "averages_op = averages_class.apply(tf.trainable_variables())\n",
    "\n",
    "#不再定义average_y，因为average_y只在比较正确率时有用，\n",
    "#在模型保存的程序中我们只输出损失\n",
    "#average_y = hidden_layer(x,averages_class,name=\"average_y\",reuse=True)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                 labels=tf.argmax(y_, 1)) #tf.argmax()\n",
    "#计算总损失\n",
    "loss = tf.reduce_mean(cross_entropy)+tf.add_n(tf.get_collection(\"losses\"))\n",
    "\n",
    "laerning_rate = tf.train.exponential_decay(learning_rate,training_step,\n",
    "                mnist.train.num_examples/batch_size,learning_rate_decay)\n",
    "train_step= tf.train.GradientDescentOptimizer(learning_rate).\\\n",
    "                                minimize(loss,global_step=training_step)\n",
    "\n",
    "#也可以采用train_op = tf.group(train_step,averages_op)的形式\n",
    "with tf.control_dependencies([train_step, averages_op]):\n",
    "    train_op = tf.no_op(name=\"train\")\n",
    "\n",
    "#初始化Saver持久化类\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    #进行30000轮到训练\n",
    "    for i in range(max_steps):\n",
    "        x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "        _, loss_value, step = sess.run([train_op, loss, training_step],\n",
    "                                          feed_dict={x: x_train, y_: y_train})\n",
    "\n",
    "        #每隔1000轮训练就输出当前训练batch上的损失函数大小，并保存一次模型\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), loss on training batch is \"\n",
    "                                                    \"%g.\" % (step, loss_value))\n",
    "            #保存模型的时候给出了global_step参数，这样可以让每个模型文件都添加\n",
    "            #代表了训练轮数的后缀，这样做的原因是方便检索\n",
    "            saver.save(sess, \"mnist_model/mnist_model.ckpt\",global_step=training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable hidden_layer/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-39a51f8e8617>\", line 16, in hidden_layer\n    initializer=tf.truncated_normal_initializer(stddev=0.1))\n  File \"<ipython-input-1-39a51f8e8617>\", line 41, in <module>\n    y = hidden_layer(x,regularizer,name=\"y\")\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-09e44113461a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 因为测试时不必关注正则化损失的值，所以不会传入正则化的办法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 计算正确率的过程也基本和第六章的样例一致\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-09e44113461a>\u001b[0m in \u001b[0;36mhidden_layer\u001b[0;34m(input_tensor, regularizer, name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden_layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         weights = tf.get_variable(\"weights\", [784, 500],\n\u001b[0;32m---> 11\u001b[0;31m                                   initializer=tf.truncated_normal_initializer(stddev=0.1))\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable hidden_layer/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-39a51f8e8617>\", line 16, in hidden_layer\n    initializer=tf.truncated_normal_initializer(stddev=0.1))\n  File \"<ipython-input-1-39a51f8e8617>\", line 41, in <module>\n    y = hidden_layer(x,regularizer,name=\"y\")\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/liyuan3970/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "\n",
    "# 定义相同的前向传播过程，要保持命名空间和变量名的一致\n",
    "def hidden_layer(input_tensor, regularizer, name):\n",
    "    with tf.variable_scope(\"hidden_layer\"):\n",
    "        weights = tf.get_variable(\"weights\", [784, 500],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection(\"losses\", regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [500], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_output\"):\n",
    "        weights = tf.get_variable(\"weights\", [500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection(\"losses\", regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer_output = tf.matmul(hidden_layer, weights) + biases\n",
    "    return hidden_layer_output\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "# 因为测试时不必关注正则化损失的值，所以不会传入正则化的办法\n",
    "y = hidden_layer(x, None, name=\"y\")\n",
    "\n",
    "# 计算正确率的过程也基本和第六章的样例一致\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99)\n",
    "\n",
    "# 通过变量重命名的方式加载模型，这里使用了滑动平均类提供的variables_to_restore()\n",
    "# 于是就免去了在前向传播过程中调用求解滑动平均的函数来获取滑动平均值的过程\n",
    "saver = tf.train.Saver(variable_averages.variables_to_restore())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "    test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "    # get_checkpoint_state()函数会通过checkpoint文件自动找到目录中最新模型的文件名\n",
    "    # 函数原型get_checkpoint_state(checkpoint_dir,latest_filename)\n",
    "    ckpt = tf.train.get_checkpoint_state(\"mnist_model/\")\n",
    "\n",
    "    # 加载模型\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    # 通过文件名得到模型保存时迭代的轮数\n",
    "    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    print(\"The latest ckpt is mnist_model.ckpt-%s\" % (global_step))\n",
    "    # 输出The latest ckpt is mnist_model.ckpt-29001\n",
    "\n",
    "    # 计算在验证数据集上的准确率并打印出来\n",
    "    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n",
    "    print(\"After %s training step(s), validation accuracy = %g%%\"\n",
    "          % (global_step, accuracy_score * 100))\n",
    "    # 输出After 29001 training step(s), validation accuracy = 98.62%\n",
    "\n",
    "    # 计算在测试数据集上的准确率并打印出来\n",
    "    test_accuracy = sess.run(accuracy, feed_dict=test_feed)\n",
    "    print(\"After %s trainging step(s) ,test accuracy = %g%%\"\n",
    "          % (global_step, test_accuracy * 100))\n",
    "    # 输出After 29001 trainging step(s) ,test accuracy = 98.51%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
