{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN的基本模板（面向过程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_pool2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a12ca1afa36b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mW_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mb_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mh_pool2_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_pool2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# 依然需要线性变化和激活函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_pool2_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_fc1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_fc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h_pool2' is not defined"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "## 基本卷基层的数据结构\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# x为训练图像的占位符、y_为训练图像标签的占位符\n",
    "\n",
    "# 一般定义输入层数据和输出层数据用占位符\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "\n",
    "# 将单张图片从784维向量重新还原为28x28的矩阵图片，这个的意思就是输入的图片要是二维的结构\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "\n",
    "# 定义一层卷积层\n",
    "# 定义权重，满足正态分布，且要有一定的深度(32)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# 以为结构和权重的深度是一样的维度32\n",
    "b_conv1 = bias_variable([32])\n",
    "# 进行卷积运算，实际上是一元回归套一个relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# 池化作用，其实是一个选择最大特征的过程，维度是２＊２可以自己定义\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "# 全连接层，输出为1024维的向量\n",
    "# 第一层的全连接层，实质是最后一层卷积池化后的扁平化数据\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "# 依然需要线性变化和激活函数\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# 把1024维的向量转换成10维，对应10个类别，对应的生成一个10维的数据\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "#　经过深度网络的最终目的是将原始的数据经过一些列的线性和为线性变化，把数据变成和标签一样维度的数据然后用梯度下降拟合\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " # 定义模型的梯度 并以一定的学习率进行计算梯度 ，这里的步骤基本上是死的！\n",
    "# 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算\n",
    "cross_entropy = tf.reduce_mean(\n",
    "     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "# 同样定义train_step\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 训练20000步\n",
    "for i in range(20000):\n",
    "    \n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # 每100步报告一次在验证集上的准确度\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    # sess的核心用法，随即梯度batch的循环的跑模型\n",
    "    sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})   \n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# 定义测试的准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#这是在测试集测试模型的精准度，其实也是一个死的过程。\n",
    "print(\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN基本模板（面向对象版本）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见的图像识别网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1108 23:36:28.674476 139901794420544 deprecation.py:323] From <ipython-input-1-dd5a4145c554>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1108 23:36:28.676693 139901794420544 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "/home/jiangziyang; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dd5a4145c554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jiangziyang/MNIST_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[0;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[0;32m--> 260\u001b[0;31m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[1;32m    261\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \"\"\"\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m   \u001b[0mrecursive_create_dir_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \"\"\"\n\u001b[0;32m--> 453\u001b[0;31m   \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /home/jiangziyang; Permission denied"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/jiangziyang/MNIST_data\", one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.99\n",
    "max_steps = 30000\n",
    "\n",
    "def hidden_layer(input_tensor,regularizer,avg_class,resuse):\n",
    "    #创建第一个卷积层，得到特征图大小为32@28x28\n",
    "    with tf.variable_scope(\"C1-conv\",reuse=resuse):\n",
    "        conv1_weights = tf.get_variable(\"weight\", [5, 5, 1, 32],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    #创建第一个池化层，池化后的结果为32@14x14\n",
    "    with tf.name_scope(\"S2-max_pool\",):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    # 创建第二个卷积层，得到特征图大小为64@14x14。注意，第一个池化层之后得到了32个\n",
    "    # 特征图，所以这里设输入的深度为32，我们在这一层选择的卷积核数量为64，所以输出\n",
    "    # 的深度是64，也就是说有64个特征图\n",
    "    with tf.variable_scope(\"C3-conv\",reuse=resuse):\n",
    "        conv2_weights = tf.get_variable(\"weight\", [5, 5, 32, 64],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    #创建第二个池化层，池化后结果为64@7x7\n",
    "    with tf.name_scope(\"S4-max_pool\",):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        #get_shape()函数可以得到这一层维度信息，由于每一层网络的输入输出都是一个batch的矩阵，\n",
    "        #所以通过get_shape()函数得到的维度信息会包含这个batch中数据的个数信息\n",
    "        #shape[1]是长度方向，shape[2]是宽度方向，shape[3]是深度方向\n",
    "        #shape[0]是一个batch中数据的个数，reshape()函数原型reshape(tensor,shape,name)\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        nodes = shape[1] * shape[2] * shape[3]    #nodes=3136\n",
    "        reshaped = tf.reshape(pool2, [shape[0], nodes])\n",
    "\n",
    "    #创建第一个全连层\n",
    "    with tf.variable_scope(\"layer5-full1\",reuse=resuse):\n",
    "        Full_connection1_weights = tf.get_variable(\"weight\", [nodes, 512],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection1_weights))\n",
    "        Full_connection1_biases = tf.get_variable(\"bias\", [512],\n",
    "                                                     initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class ==None:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, Full_connection1_weights) + \\\n",
    "                                                                   Full_connection1_biases)\n",
    "        else:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, avg_class.average(Full_connection1_weights))\n",
    "                                                   + avg_class.average(Full_connection1_biases))\n",
    "\n",
    "    #创建第二个全连层\n",
    "    with tf.variable_scope(\"layer6-full2\",reuse=resuse):\n",
    "        Full_connection2_weights = tf.get_variable(\"weight\", [512, 10],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection2_weights))\n",
    "        Full_connection2_biases = tf.get_variable(\"bias\", [10],\n",
    "                                                   initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class == None:\n",
    "            result = tf.matmul(Full_1, Full_connection2_weights) + Full_connection2_biases\n",
    "        else:\n",
    "            result = tf.matmul(Full_1, avg_class.average(Full_connection2_weights)) + \\\n",
    "                                                  avg_class.average(Full_connection2_biases)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size ,28,28,1],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "y = hidden_layer(x,regularizer,avg_class=None,resuse=False)\n",
    "\n",
    "training_step = tf.Variable(0, trainable=False)\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99, training_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "average_y = hidden_layer(x,regularizer,variable_averages,resuse=True)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(learning_rate,\n",
    "                                 training_step, mnist.train.num_examples /batch_size ,\n",
    "                                 learning_rate_decay, staircase=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate). \\\n",
    "    minimize(loss, global_step=training_step)\n",
    "\n",
    "with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "crorent_predicition = tf.equal(tf.arg_max(average_y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(max_steps):\n",
    "        if i %1000==0:\n",
    "            x_val, y_val = mnist.validation.next_batch(batch_size)\n",
    "            reshaped_x2 = np.reshape(x_val, (batch_size,28,28, 1))\n",
    "            validate_feed = {x: reshaped_x2, y_: y_val}\n",
    "\n",
    "            validate_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "            print(\"After %d trainging step(s) ,validation accuracy\"\n",
    "                  \"using average model is %g%%\" % (i, validate_accuracy * 100))\n",
    "\n",
    "        x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        reshaped_xs = np.reshape(x_train, (batch_size ,28,28,1))\n",
    "        sess.run(train_op, feed_dict={x: reshaped_xs, y_: y_train})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "# 在函数inference_op()内定义前向传播的过程\n",
    "def inference_op(images):\n",
    "    parameters = []\n",
    "\n",
    "    # 在命名空间conv1下实现第一个卷积层\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 96], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[96], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv1 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "\n",
    "        # 打印第一个卷积层的网络结构\n",
    "        print(conv1.op.name, ' ', conv1.get_shape().as_list())\n",
    "\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "    # 添加一个LRN层和最大池化层\n",
    "    lrn1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\"lrn1\")\n",
    "    pool1 = tf.nn.max_pool(lrn1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool1\")\n",
    "\n",
    "    # 打印池化层网络结构\n",
    "    print(pool1.op.name, ' ', pool1.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv2下实现第二个卷积层\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([5, 5, 96, 256], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv2 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第二个卷积层的网络结构\n",
    "        print(conv2.op.name, ' ', conv2.get_shape().as_list())\n",
    "\n",
    "    # 添加一个LRN层和最大池化层\n",
    "    lrn2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\"lrn2\")\n",
    "    pool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool2\")\n",
    "    # 打印池化层的网络结构\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv3下实现第三个卷积层\n",
    "    with tf.name_scope(\"conv3\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv3 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第三个卷积层的网络结构\n",
    "        print(conv3.op.name, ' ', conv3.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv4下实现第四个卷积层\n",
    "    with tf.name_scope(\"conv4\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv4 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第四个卷积层的网络结构\n",
    "        print(conv4.op.name, ' ', conv4.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv5下实现第五个卷积层\n",
    "    with tf.name_scope(\"conv5\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "\n",
    "        conv5 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第五个卷积层的网络结构\n",
    "        print(conv5.op.name, ' ', conv5.get_shape().as_list())\n",
    "\n",
    "    # 添加一个最大池化层\n",
    "    pool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool5\")\n",
    "    # 打印最大池化层的网络结构\n",
    "    print(pool5.op.name, ' ', pool5.get_shape().as_list())\n",
    "\n",
    "    # 将pool5输出的矩阵汇总为向量的形式，为的是方便作为全连层的输入\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshaped = tf.reshape(pool5, [pool_shape[0], nodes])\n",
    "\n",
    "    # 创建第一个全连接层\n",
    "    with tf.name_scope(\"fc_1\"):\n",
    "        fc1_weights = tf.Variable(tf.truncated_normal([nodes, 4096], dtype=tf.float32,\n",
    "                                                      stddev=1e-1), name=\"weights\")\n",
    "        fc1_bias = tf.Variable(tf.constant(0.0, shape=[4096],\n",
    "                                           dtype=tf.float32), trainable=True, name=\"biases\")\n",
    "        fc_1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_bias)\n",
    "        parameters += [fc1_weights, fc1_bias]\n",
    "\n",
    "        # 打印第一个全连接层的网络结构信息\n",
    "        print(fc_1.op.name, ' ', fc_1.get_shape().as_list())\n",
    "\n",
    "    # 创建第二个全连接层\n",
    "    with tf.name_scope(\"fc_2\"):\n",
    "        fc2_weights = tf.Variable(tf.truncated_normal([4096, 4096], dtype=tf.float32,\n",
    "                                                      stddev=1e-1), name=\"weights\")\n",
    "        fc2_bias = tf.Variable(tf.constant(0.0, shape=[4096],\n",
    "                                           dtype=tf.float32), trainable=True, name=\"biases\")\n",
    "        fc_2 = tf.nn.relu(tf.matmul(fc_1, fc2_weights) + fc2_bias)\n",
    "        parameters += [fc2_weights, fc2_bias]\n",
    "\n",
    "        # 打印第二个全连接层的网络结构信息\n",
    "        print(fc_2.op.name, ' ', fc_2.get_shape().as_list())\n",
    "\n",
    "    # 返回全连接层处理的结果\n",
    "    return fc_2, parameters\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # 创建模拟的图片数据.\n",
    "    image_size = 224\n",
    "    images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                          dtype=tf.float32, stddev=1e-1))\n",
    "\n",
    "    # 在计算图中定义前向传播模型的运行，并得到不包括全连部分的参数\n",
    "    # 这些参数用于之后的梯度计算\n",
    "    fc_2, parameters = inference_op(images)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 配置会话，gpu_options.allocator_type 用于设置GPU的分配策略，值为\"BFC\"表示\n",
    "    # 采用最佳适配合并算法\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = \"BFC\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        num_steps_burn_in = 10\n",
    "        total_dura = 0.0\n",
    "        total_dura_squared = 0.0\n",
    "\n",
    "        back_total_dura = 0.0\n",
    "        back_total_dura_squared = 0.0\n",
    "\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(fc_2)\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print('%s: step %d, duration = %.3f' %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_dura += duration\n",
    "                total_dura_squared += duration * duration\n",
    "        average_time = total_dura / num_batches\n",
    "\n",
    "        # 打印前向传播的运算时间信息\n",
    "        print('%s: Forward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "              (datetime.now(), num_batches, average_time,\n",
    "               math.sqrt(total_dura_squared / num_batches - average_time * average_time)))\n",
    "\n",
    "        # 使用gradients()求相对于pool5的L2 loss的所有模型参数的梯度\n",
    "        # 函数原型gradients(ys,xs,grad_ys,name,colocate_gradients_with_ops,gate_gradients,\n",
    "        # aggregation_method=None)\n",
    "        # 一般情况下我们只需对参数ys、xs传递参数，他会计算ys相对于xs的偏导数，并将\n",
    "        # 结果作为一个长度为len(xs)的列表返回，其他参数在函数定义时都带有默认值，\n",
    "        # 比如grad_ys默认为None，name默认为gradients，colocate_gradients_with_ops默认\n",
    "        # 为False，gate_gradients默认为False\n",
    "        grad = tf.gradients(tf.nn.l2_loss(fc_2), parameters)\n",
    "\n",
    "        # 运行反向传播测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(grad)\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print('%s: step %d, duration = %.3f' %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                back_total_dura += duration\n",
    "                back_total_dura_squared += duration * duration\n",
    "        back_avg_t = back_total_dura / num_batches\n",
    "\n",
    "        # 打印反向传播的运算时间信息\n",
    "        print('%s: Forward-backward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "              (datetime.now(), num_batches, back_avg_t,\n",
    "               math.sqrt(back_total_dura_squared / num_batches - back_avg_t * back_avg_t)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "batch_size = 12\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "# 定义卷积操作\n",
    "def conv_op(input, name, kernel_h, kernel_w, num_out, step_h, step_w, para):\n",
    "    # num_in是输入的深度，这个参数被用来确定过滤器的输入通道数\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(scope + \"w\", shape=[kernel_h, kernel_w, num_in, num_out],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        conv = tf.nn.conv2d(input, kernel, (1, step_h, step_w, 1), padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[num_out], dtype=tf.float32),\n",
    "                             trainable=True, name=\"b\")\n",
    "        # 计算relu后的激活值\n",
    "        activation = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        para += [kernel, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义全连操作\n",
    "def fc_op(input, name, num_out, para):\n",
    "    # num_in为输入单元的数量\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        weights = tf.get_variable(scope + \"w\", shape=[num_in, num_out], dtype=tf.float32,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_out], dtype=tf.float32), name=\"b\")\n",
    "\n",
    "        # tf.nn.relu_layer()函数会同时完成矩阵乘法以加和偏置项并计算relu激活值\n",
    "        # 这是分步编程的良好替代\n",
    "        activation = tf.nn.relu_layer(input, weights, biases)\n",
    "\n",
    "        para += [weights, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义前向传播的计算过程，input参数的大小为224x224x3，也就是输入的模拟图片数据\n",
    "def inference_op(input, keep_prob):\n",
    "    parameters = []\n",
    "\n",
    "    # 第一段卷积，输出大小为112x112x64(省略了第一个batch_size参数)\n",
    "    conv1_1 = conv_op(input, name=\"conv1_1\", kernel_h=3, kernel_w=3, num_out=4,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv1_2 = conv_op(conv1_1, name=\"conv1_2\", kernel_h=3, kernel_w=3, num_out=64,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool1\")\n",
    "    print(pool1.op.name, ' ', pool1.get_shape().as_list())\n",
    "\n",
    "    # 第二段卷积，输出大小为56x56x128(省略了第一个batch_size参数)\n",
    "    conv2_1 = conv_op(pool1, name=\"conv2_1\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv2_2 = conv_op(conv2_1, name=\"conv2_2\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool2\")\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 第三段卷积，输出大小为28x28x256(省略了第一个batch_size参数)\n",
    "    conv3_1 = conv_op(pool2, name=\"conv3_1\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_2 = conv_op(conv3_1, name=\"conv3_2\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_3 = conv_op(conv3_2, name=\"conv3_3\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool3 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool3\")\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 第四段卷积，输出大小为14x14x512(省略了第一个batch_size参数)\n",
    "    conv4_1 = conv_op(pool3, name=\"conv4_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_2 = conv_op(conv4_1, name=\"conv4_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_3 = conv_op(conv4_2, name=\"conv4_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool4 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool4\")\n",
    "    print(pool4.op.name, ' ', pool4.get_shape().as_list())\n",
    "\n",
    "    # 第五段卷积，输出大小为7x7x512(省略了第一个batch_size参数)\n",
    "    conv5_1 = conv_op(pool4, name=\"conv5_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_2 = conv_op(conv5_1, name=\"conv5_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_3 = conv_op(conv5_2, name=\"conv5_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool5\")\n",
    "    print(pool5.op.name, ' ', pool5.get_shape().as_list())\n",
    "\n",
    "    # pool5的结果汇总为一个向量的形式\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    flattened_shape = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshped = tf.reshape(pool5, [-1, flattened_shape], name=\"reshped\")\n",
    "\n",
    "    # 第一个全连层\n",
    "    fc_6 = fc_op(reshped, name=\"fc6\", num_out=4096, para=parameters)\n",
    "    fc_6_drop = tf.nn.dropout(fc_6, keep_prob, name=\"fc6_drop\")\n",
    "\n",
    "    # 第二个全连层\n",
    "    fc_7 = fc_op(fc_6_drop, name=\"fc7\", num_out=4096, para=parameters)\n",
    "    fc_7_drop = tf.nn.dropout(fc_7, keep_prob, name=\"fc7_drop\")\n",
    "\n",
    "    # 第三个全连层及softmax层\n",
    "    fc_8 = fc_op(fc_7_drop, name=\"fc8\", num_out=1000, para=parameters)\n",
    "    softmax = tf.nn.softmax(fc_8)\n",
    "\n",
    "    # predictions模拟了通过argmax得到预测结果\n",
    "    predictions = tf.argmax(softmax, 1)\n",
    "    return predictions, softmax, fc_8, parameters\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # 创建模拟的图片数据\n",
    "    image_size = 224\n",
    "    images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                          dtype=tf.float32, stddev=1e-1))\n",
    "\n",
    "    # Dropout的keep_prob会根据前向传播或者反向传播而有所不同，在前向传播时，\n",
    "    # keep_prob=1.0，在反向传播时keep_prob=0.5\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # 为当前计算图添加前向传播过程\n",
    "    predictions, softmax, fc_8, parameters = inference_op(images, keep_prob)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 使用BFC算法确定GPU内存最佳分配策略\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = \"BFC\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        num_steps_burn_in = 10\n",
    "\n",
    "        total_dura = 0.0\n",
    "        total_dura_squared = 0.0\n",
    "\n",
    "        back_total_dura = 0.0\n",
    "        back_total_dura_squared = 0.0\n",
    "\n",
    "        # 运行前向传播的测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(predictions, feed_dict={keep_prob: 1.0})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_dura += duration\n",
    "                total_dura_squared += duration * duration\n",
    "        average_time = total_dura / num_batches\n",
    "\n",
    "        # 打印前向传播的运算时间信息\n",
    "        print(\"%s: Forward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, average_time,\n",
    "               math.sqrt(total_dura_squared / num_batches - average_time * average_time)))\n",
    "\n",
    "        # 定义求解梯度的操作\n",
    "        grad = tf.gradients(tf.nn.l2_loss(fc_8), parameters)\n",
    "\n",
    "        # 运行反向传播测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(grad, feed_dict={keep_prob: 0.5})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                back_total_dura += duration\n",
    "                back_total_dura_squared += duration * duration\n",
    "        back_avg_t = back_total_dura / num_batches\n",
    "\n",
    "        # 打印反向传播的运算时间信息\n",
    "        print(\"%s: Forward-backward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, back_avg_t,\n",
    "               math.sqrt(back_total_dura_squared / num_batches - back_avg_t * back_avg_t)))\n",
    "\n",
    "'''打印的内容\n",
    "pool1   [12, 112, 112, 64]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool4   [12, 14, 14, 512]\n",
    "pool5   [12, 7, 7, 512]\n",
    "2018-04-28 09:35:34.973581: step 0, duration = 0.353\n",
    "2018-04-28 09:35:38.553523: step 10, duration = 0.366\n",
    "2018-04-28 09:35:42.124513: step 20, duration = 0.351\n",
    "2018-04-28 09:35:45.691710: step 30, duration = 0.351\n",
    "2018-04-28 09:35:49.274942: step 40, duration = 0.366\n",
    "2018-04-28 09:35:52.828938: step 50, duration = 0.351\n",
    "2018-04-28 09:35:56.380751: step 60, duration = 0.351\n",
    "2018-04-28 09:35:59.951856: step 70, duration = 0.364\n",
    "2018-04-28 09:36:03.583875: step 80, duration = 0.360\n",
    "2018-04-28 09:36:07.214103: step 90, duration = 0.357\n",
    "2018-04-28 09:36:10.460588: Forward across 100 steps, 0.358 +/- 0.007 sec / batch\n",
    "2018-04-28 09:36:27.955719: step 0, duration = 1.364\n",
    "2018-04-28 09:36:41.584773: step 10, duration = 1.363\n",
    "2018-04-28 09:36:55.197996: step 20, duration = 1.355\n",
    "2018-04-28 09:37:08.822001: step 30, duration = 1.360\n",
    "2018-04-28 09:37:22.442811: step 40, duration = 1.364\n",
    "2018-04-28 09:37:36.091333: step 50, duration = 1.358\n",
    "2018-04-28 09:37:49.744742: step 60, duration = 1.362\n",
    "2018-04-28 09:38:03.358585: step 70, duration = 1.355\n",
    "2018-04-28 09:38:16.874355: step 80, duration = 1.331\n",
    "2018-04-28 09:38:30.432612: step 90, duration = 1.356\n",
    "2018-04-28 09:38:42.658764: Forward-backward across 100 steps, 1.361 +/- 0.009 sec / batch\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## 网络结构\n",
    "\n",
    "class Block(collections.namedtuple(\"block\", [\"name\", \"residual_unit\", \"args\"])):\n",
    "    \"A named tuple describing a ResNet Block\"\n",
    "    # namedtuple()函数原型为：\n",
    "    # collections.namedtuple(typename,field_names,verbose,rename)\n",
    "\n",
    "\n",
    "def conv2d_same(inputs, num_outputs, kernel_size, stride, scope=None):\n",
    "    # 如果步长为1，则直接使用padding=\"SAME\"的方式进行卷积操作\n",
    "    # 一般步长不为1的情况出现在残差学习块的最后一个卷积操作中\n",
    "    if stride == 1:\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1,\n",
    "                           padding=\"SAME\", scope=scope)\n",
    "    else:\n",
    "        pad_begin = (kernel_size - 1) // 2\n",
    "        pad_end = kernel_size - 1 - pad_begin\n",
    "\n",
    "        # pad()函数用于对矩阵进行定制填充\n",
    "        # 在这里用于对inputs进行向上填充pad_begin行0，向下填充pad_end行0，\n",
    "        # 向左填充pad_begin行0，向右填充pad_end行0\n",
    "        inputs = tf.pad(inputs,\n",
    "                        [[0, 0], [pad_begin, pad_end], [pad_begin, pad_end], [0, 0]])\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n",
    "                           padding=\"VALID\", scope=scope)\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def residual_unit(inputs, depth, depth_residual, stride, outputs_collections=None,\n",
    "                  scope=None):\n",
    "    with tf.variable_scope(scope, \"residual_v2\", [inputs]) as sc:\n",
    "\n",
    "        # 输入的通道数，取inputs的形状的最后一个元素\n",
    "        depth_input = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "\n",
    "        # 使用slim.batch_norm()函数进行BatchNormalization操作\n",
    "        preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\"preac\")\n",
    "\n",
    "        # 如果本块的depth值(depth参数)等于上一个块的depth(depth_input)，则考虑进行降采样操作，\n",
    "        # 如果depth值不等于depth_input，则使用conv2d()函数使输入通道数和输出通道数一致\n",
    "        if depth == depth_input:\n",
    "            # 如果stride等于1，则不进行降采样操作，如果stride不等于1，则使用max_pool2d\n",
    "            # 进行步长为stride且池化核为1x1的降采样操作\n",
    "            if stride == 1:\n",
    "                identity = inputs\n",
    "            else:\n",
    "                identity = slim.max_pool2d(inputs, [1, 1], stride=stride, scope=\"shortcut\")\n",
    "        else:\n",
    "            identity = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None,\n",
    "                                   activation_fn=None, scope=\"shortcut\")\n",
    "\n",
    "        # 在一个残差学习块中的3个卷积层\n",
    "        residual = slim.conv2d(preact, depth_residual, [1, 1], stride=1, scope=\"conv1\")\n",
    "        residual = conv2d_same(residual, depth_residual, 3, stride, scope=\"conv2\")\n",
    "        residual = slim.conv2d(residual, depth, [1, 1], stride=1, normalizer_fn=None,\n",
    "                               activation_fn=None, scope=\"conv3\")\n",
    "\n",
    "        # 将identity的结果和residual的结果相加\n",
    "        output = identity + residual\n",
    "\n",
    "        result = slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def resnet_v2_152(inputs, num_classes, reuse=None, scope=\"resnet_v2_152\"):\n",
    "    blocks = [\n",
    "        Block(\"block1\", residual_unit, [(256, 64, 1), (256, 64, 1), (256, 64, 2)]),\n",
    "        Block(\"block2\", residual_unit, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n",
    "        Block(\"block3\", residual_unit, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
    "        Block(\"block4\", residual_unit, [(2048, 512, 1)] * 3)]\n",
    "    return resnet_v2(inputs, blocks, num_classes, reuse=reuse, scope=scope)\n",
    "\n",
    "\n",
    "def resnet_v2(inputs, blocks, num_classes, reuse=None, scope=None):\n",
    "    with tf.variable_scope(scope, \"resnet_v2\", [inputs], reuse=reuse) as sc:\n",
    "        end_points_collection = sc.original_name_scope + \"_end_points\"\n",
    "\n",
    "        # 对函数residual_unit()的outputs_collections参数使用参数空间\n",
    "        with slim.arg_scope([residual_unit], outputs_collections=end_points_collection):\n",
    "\n",
    "            # 创建ResNet的第一个卷积层和池化层，卷积核大小7x7，深度64，池化核大侠3x3\n",
    "            with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None):\n",
    "                net = conv2d_same(inputs, 64, 7, stride=2, scope=\"conv1\")\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\"pool1\")\n",
    "\n",
    "            # 在两个嵌套的for循环内调用residual_unit()函数堆砌ResNet的结构\n",
    "            for block in blocks:\n",
    "                # block.name分别为block1、block2、block3和block4\n",
    "                with tf.variable_scope(block.name, \"block\", [net]) as sc:\n",
    "\n",
    "                    # tuple_value为Block类的args参数中的每一个元组值，\n",
    "                    # i是这些元组在每一个Block的args参数中的序号\n",
    "                    for i, tuple_value in enumerate(block.args):\n",
    "                        # i的值从0开始，对于第一个unit，i需要加1\n",
    "                        with tf.variable_scope(\"unit_%d\" % (i + 1), values=[net]):\n",
    "                            # 每一个元组都有3个数组成，将这三个数作为参数传递到Block类的\n",
    "                            # residual_unit参数中，在定义blockss时，这个参数就是函数residual_unit()\n",
    "                            depth, depth_bottleneck, stride = tuple_value\n",
    "                            net = block.residual_unit(net, depth=depth, depth_residual=depth_bottleneck,\n",
    "                                                      stride=stride)\n",
    "                    # net就是每一个块的结构\n",
    "                    net = slim.utils.collect_named_outputs(end_points_collection, sc.name, net)\n",
    "\n",
    "            # 对net使用slim.batch_norm()函数进行BatchNormalization操作\n",
    "            net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\"postnorm\")\n",
    "\n",
    "            # 创建全局平均池化层\n",
    "            net = tf.reduce_mean(net, [1, 2], name=\"pool5\", keep_dims=True)\n",
    "\n",
    "            # 如果定义了num_classes，则通过1x1池化的方式获得数目为num_classes的输出\n",
    "            if num_classes is not None:\n",
    "                net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                                  normalizer_fn=None, scope=\"logits\")\n",
    "\n",
    "            return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import ResNet_struct\n",
    "from tensorflow.contrib import slim\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "num_steps_burn_in = 10\n",
    "total_duration = 0.0\n",
    "total_duration_squared = 0.0\n",
    "inputs = tf.random_uniform((batch_size, 224, 224, 3))\n",
    "\n",
    "def arg_scope(is_training=True,weight_decay=0.0001,batch_norm_decay=0.997,\n",
    "                           batch_norm_epsilon=1e-5,batch_norm_scale=True):\n",
    "\n",
    "    batch_norm_params = {\"is_training\": is_training,\n",
    "                         \"decay\": batch_norm_decay,\n",
    "                         \"epsilon\": batch_norm_epsilon,\n",
    "                         \"scale\": batch_norm_scale,\n",
    "                         \"updates_collections\": tf.GraphKeys.UPDATE_OPS}\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                        #weights_initializer用于指定权重的初始化程序\n",
    "                        weights_initializer=slim.variance_scaling_initializer(),\n",
    "                        #weights_regularizer为权重可选的正则化程序\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                        #activation_fn用于激活函数的指定，默认的为ReLU函数\n",
    "                        #normalizer_params用于指定正则化函数的参数\n",
    "                        activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params=batch_norm_params):\n",
    "\n",
    "        #定义slim.batch_norm()函数的参数空间\n",
    "        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "            # slim.max_pool2d()函数的参数空间\n",
    "            with slim.arg_scope([slim.max_pool2d], padding=\"SAME\") as arg_scope:\n",
    "                return arg_scope\n",
    "\n",
    "# 定义模型的前向传播过程，这被限制在一个参数空间中\n",
    "with slim.arg_scope(arg_scope(is_training=False)):\n",
    "    net = ResNet_struct.resnet_v2_152(inputs, 1000)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    #运行前向传播测试过程\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = sess.run(net)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            if i % 10 == 0:\n",
    "                print('%s: step %d, duration = %.3f' %\n",
    "                      (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    average_time = total_duration / num_batches\n",
    "\n",
    "    #打印前向传播的运算时间信息\n",
    "    print('%s: Forward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "          (datetime.now(), num_batches, average_time,\n",
    "           math.sqrt(total_duration_squared / num_batches-average_time*average_time)))\n",
    "\n",
    "\n",
    "'''打印的内容\n",
    "2018-04-28 15:44:25.253434: step 0, duration = 1.039\n",
    "2018-04-28 15:44:35.616892: step 10, duration = 1.037\n",
    "2018-04-28 15:44:45.981536: step 20, duration = 1.035\n",
    "2018-04-28 15:44:56.349566: step 30, duration = 1.036\n",
    "2018-04-28 15:45:06.728368: step 40, duration = 1.035\n",
    "2018-04-28 15:45:17.089299: step 50, duration = 1.035\n",
    "2018-04-28 15:45:27.456285: step 60, duration = 1.037\n",
    "2018-04-28 15:45:37.822637: step 70, duration = 1.035\n",
    "2018-04-28 15:45:48.192688: step 80, duration = 1.035\n",
    "2018-04-28 15:45:58.555936: step 90, duration = 1.035\n",
    "2018-04-28 15:46:07.886232: Forward across 100 steps, 1.037 +/- 0.002 sec / batch\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
