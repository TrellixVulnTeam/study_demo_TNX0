{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN的基本模板（面向过程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "## 基本卷基层的数据结构\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# x为训练图像的占位符、y_为训练图像标签的占位符\n",
    "\n",
    "# 一般定义输入层数据和输出层数据用占位符\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "\n",
    "# 将单张图片从784维向量重新还原为28x28的矩阵图片，这个的意思就是输入的图片要是二维的结构\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "\n",
    "# 定义一层卷积层\n",
    "# 定义权重，满足正态分布，且要有一定的深度(32)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# 以为结构和权重的深度是一样的维度32\n",
    "b_conv1 = bias_variable([32])\n",
    "# 进行卷积运算，实际上是一元回归套一个relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# 池化作用，其实是一个选择最大特征的过程，维度是２＊２可以自己定义\n",
    " h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "# 全连接层，输出为1024维的向量\n",
    "# 第一层的全连接层，实质是最后一层卷积池化后的扁平化数据\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "# 依然需要线性变化和激活函数\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1\n",
    " keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# 把1024维的向量转换成10维，对应10个类别，对应的生成一个10维的数据\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "#　经过深度网络的最终目的是将原始的数据经过一些列的线性和为线性变化，把数据变成和标签一样维度的数据然后用梯度下降拟合\n",
    " y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " # 定义模型的梯度 并以一定的学习率进行计算梯度 ，这里的步骤基本上是死的！\n",
    "# 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算\n",
    "cross_entropy = tf.reduce_mean(\n",
    "     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "# 同样定义train_step\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 训练20000步\n",
    "for i in range(20000):\n",
    "     batch = mnist.train.next_batch(50)\n",
    "    # 每100步报告一次在验证集上的准确度\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    # sess的核心用法，随即梯度batch的循环的跑模型\n",
    "    sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})   \n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# 定义测试的准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#这是在测试集测试模型的精准度，其实也是一个死的过程。\n",
    "print(\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN基本模板（面向对象版本）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见的图像识别网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/jiangziyang/MNIST_data\", one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.99\n",
    "max_steps = 30000\n",
    "\n",
    "def hidden_layer(input_tensor,regularizer,avg_class,resuse):\n",
    "    #创建第一个卷积层，得到特征图大小为32@28x28\n",
    "    with tf.variable_scope(\"C1-conv\",reuse=resuse):\n",
    "        conv1_weights = tf.get_variable(\"weight\", [5, 5, 1, 32],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    #创建第一个池化层，池化后的结果为32@14x14\n",
    "    with tf.name_scope(\"S2-max_pool\",):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    # 创建第二个卷积层，得到特征图大小为64@14x14。注意，第一个池化层之后得到了32个\n",
    "    # 特征图，所以这里设输入的深度为32，我们在这一层选择的卷积核数量为64，所以输出\n",
    "    # 的深度是64，也就是说有64个特征图\n",
    "    with tf.variable_scope(\"C3-conv\",reuse=resuse):\n",
    "        conv2_weights = tf.get_variable(\"weight\", [5, 5, 32, 64],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    #创建第二个池化层，池化后结果为64@7x7\n",
    "    with tf.name_scope(\"S4-max_pool\",):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        #get_shape()函数可以得到这一层维度信息，由于每一层网络的输入输出都是一个batch的矩阵，\n",
    "        #所以通过get_shape()函数得到的维度信息会包含这个batch中数据的个数信息\n",
    "        #shape[1]是长度方向，shape[2]是宽度方向，shape[3]是深度方向\n",
    "        #shape[0]是一个batch中数据的个数，reshape()函数原型reshape(tensor,shape,name)\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        nodes = shape[1] * shape[2] * shape[3]    #nodes=3136\n",
    "        reshaped = tf.reshape(pool2, [shape[0], nodes])\n",
    "\n",
    "    #创建第一个全连层\n",
    "    with tf.variable_scope(\"layer5-full1\",reuse=resuse):\n",
    "        Full_connection1_weights = tf.get_variable(\"weight\", [nodes, 512],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection1_weights))\n",
    "        Full_connection1_biases = tf.get_variable(\"bias\", [512],\n",
    "                                                     initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class ==None:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, Full_connection1_weights) + \\\n",
    "                                                                   Full_connection1_biases)\n",
    "        else:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, avg_class.average(Full_connection1_weights))\n",
    "                                                   + avg_class.average(Full_connection1_biases))\n",
    "\n",
    "    #创建第二个全连层\n",
    "    with tf.variable_scope(\"layer6-full2\",reuse=resuse):\n",
    "        Full_connection2_weights = tf.get_variable(\"weight\", [512, 10],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection2_weights))\n",
    "        Full_connection2_biases = tf.get_variable(\"bias\", [10],\n",
    "                                                   initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class == None:\n",
    "            result = tf.matmul(Full_1, Full_connection2_weights) + Full_connection2_biases\n",
    "        else:\n",
    "            result = tf.matmul(Full_1, avg_class.average(Full_connection2_weights)) + \\\n",
    "                                                  avg_class.average(Full_connection2_biases)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size ,28,28,1],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "y = hidden_layer(x,regularizer,avg_class=None,resuse=False)\n",
    "\n",
    "training_step = tf.Variable(0, trainable=False)\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99, training_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "average_y = hidden_layer(x,regularizer,variable_averages,resuse=True)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(learning_rate,\n",
    "                                 training_step, mnist.train.num_examples /batch_size ,\n",
    "                                 learning_rate_decay, staircase=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate). \\\n",
    "    minimize(loss, global_step=training_step)\n",
    "\n",
    "with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "crorent_predicition = tf.equal(tf.arg_max(average_y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(max_steps):\n",
    "        if i %1000==0:\n",
    "            x_val, y_val = mnist.validation.next_batch(batch_size)\n",
    "            reshaped_x2 = np.reshape(x_val, (batch_size,28,28, 1))\n",
    "            validate_feed = {x: reshaped_x2, y_: y_val}\n",
    "\n",
    "            validate_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "            print(\"After %d trainging step(s) ,validation accuracy\"\n",
    "                  \"using average model is %g%%\" % (i, validate_accuracy * 100))\n",
    "\n",
    "        x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        reshaped_xs = np.reshape(x_train, (batch_size ,28,28,1))\n",
    "        sess.run(train_op, feed_dict={x: reshaped_xs, y_: y_train})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "# 在函数inference_op()内定义前向传播的过程\n",
    "def inference_op(images):\n",
    "    parameters = []\n",
    "\n",
    "    # 在命名空间conv1下实现第一个卷积层\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 96], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[96], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv1 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "\n",
    "        # 打印第一个卷积层的网络结构\n",
    "        print(conv1.op.name, ' ', conv1.get_shape().as_list())\n",
    "\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "    # 添加一个LRN层和最大池化层\n",
    "    lrn1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\"lrn1\")\n",
    "    pool1 = tf.nn.max_pool(lrn1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool1\")\n",
    "\n",
    "    # 打印池化层网络结构\n",
    "    print(pool1.op.name, ' ', pool1.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv2下实现第二个卷积层\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([5, 5, 96, 256], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv2 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第二个卷积层的网络结构\n",
    "        print(conv2.op.name, ' ', conv2.get_shape().as_list())\n",
    "\n",
    "    # 添加一个LRN层和最大池化层\n",
    "    lrn2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\"lrn2\")\n",
    "    pool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool2\")\n",
    "    # 打印池化层的网络结构\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv3下实现第三个卷积层\n",
    "    with tf.name_scope(\"conv3\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv3 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第三个卷积层的网络结构\n",
    "        print(conv3.op.name, ' ', conv3.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv4下实现第四个卷积层\n",
    "    with tf.name_scope(\"conv4\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        conv4 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第四个卷积层的网络结构\n",
    "        print(conv4.op.name, ' ', conv4.get_shape().as_list())\n",
    "\n",
    "    # 在命名空间conv5下实现第五个卷积层\n",
    "    with tf.name_scope(\"conv5\"):\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                                 dtype=tf.float32, stddev=1e-1),\n",
    "                             name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "\n",
    "        conv5 = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "        parameters += [kernel, biases]\n",
    "\n",
    "        # 打印第五个卷积层的网络结构\n",
    "        print(conv5.op.name, ' ', conv5.get_shape().as_list())\n",
    "\n",
    "    # 添加一个最大池化层\n",
    "    pool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\", name=\"pool5\")\n",
    "    # 打印最大池化层的网络结构\n",
    "    print(pool5.op.name, ' ', pool5.get_shape().as_list())\n",
    "\n",
    "    # 将pool5输出的矩阵汇总为向量的形式，为的是方便作为全连层的输入\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshaped = tf.reshape(pool5, [pool_shape[0], nodes])\n",
    "\n",
    "    # 创建第一个全连接层\n",
    "    with tf.name_scope(\"fc_1\"):\n",
    "        fc1_weights = tf.Variable(tf.truncated_normal([nodes, 4096], dtype=tf.float32,\n",
    "                                                      stddev=1e-1), name=\"weights\")\n",
    "        fc1_bias = tf.Variable(tf.constant(0.0, shape=[4096],\n",
    "                                           dtype=tf.float32), trainable=True, name=\"biases\")\n",
    "        fc_1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_bias)\n",
    "        parameters += [fc1_weights, fc1_bias]\n",
    "\n",
    "        # 打印第一个全连接层的网络结构信息\n",
    "        print(fc_1.op.name, ' ', fc_1.get_shape().as_list())\n",
    "\n",
    "    # 创建第二个全连接层\n",
    "    with tf.name_scope(\"fc_2\"):\n",
    "        fc2_weights = tf.Variable(tf.truncated_normal([4096, 4096], dtype=tf.float32,\n",
    "                                                      stddev=1e-1), name=\"weights\")\n",
    "        fc2_bias = tf.Variable(tf.constant(0.0, shape=[4096],\n",
    "                                           dtype=tf.float32), trainable=True, name=\"biases\")\n",
    "        fc_2 = tf.nn.relu(tf.matmul(fc_1, fc2_weights) + fc2_bias)\n",
    "        parameters += [fc2_weights, fc2_bias]\n",
    "\n",
    "        # 打印第二个全连接层的网络结构信息\n",
    "        print(fc_2.op.name, ' ', fc_2.get_shape().as_list())\n",
    "\n",
    "    # 返回全连接层处理的结果\n",
    "    return fc_2, parameters\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # 创建模拟的图片数据.\n",
    "    image_size = 224\n",
    "    images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                          dtype=tf.float32, stddev=1e-1))\n",
    "\n",
    "    # 在计算图中定义前向传播模型的运行，并得到不包括全连部分的参数\n",
    "    # 这些参数用于之后的梯度计算\n",
    "    fc_2, parameters = inference_op(images)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 配置会话，gpu_options.allocator_type 用于设置GPU的分配策略，值为\"BFC\"表示\n",
    "    # 采用最佳适配合并算法\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = \"BFC\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        num_steps_burn_in = 10\n",
    "        total_dura = 0.0\n",
    "        total_dura_squared = 0.0\n",
    "\n",
    "        back_total_dura = 0.0\n",
    "        back_total_dura_squared = 0.0\n",
    "\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(fc_2)\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print('%s: step %d, duration = %.3f' %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_dura += duration\n",
    "                total_dura_squared += duration * duration\n",
    "        average_time = total_dura / num_batches\n",
    "\n",
    "        # 打印前向传播的运算时间信息\n",
    "        print('%s: Forward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "              (datetime.now(), num_batches, average_time,\n",
    "               math.sqrt(total_dura_squared / num_batches - average_time * average_time)))\n",
    "\n",
    "        # 使用gradients()求相对于pool5的L2 loss的所有模型参数的梯度\n",
    "        # 函数原型gradients(ys,xs,grad_ys,name,colocate_gradients_with_ops,gate_gradients,\n",
    "        # aggregation_method=None)\n",
    "        # 一般情况下我们只需对参数ys、xs传递参数，他会计算ys相对于xs的偏导数，并将\n",
    "        # 结果作为一个长度为len(xs)的列表返回，其他参数在函数定义时都带有默认值，\n",
    "        # 比如grad_ys默认为None，name默认为gradients，colocate_gradients_with_ops默认\n",
    "        # 为False，gate_gradients默认为False\n",
    "        grad = tf.gradients(tf.nn.l2_loss(fc_2), parameters)\n",
    "\n",
    "        # 运行反向传播测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(grad)\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print('%s: step %d, duration = %.3f' %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                back_total_dura += duration\n",
    "                back_total_dura_squared += duration * duration\n",
    "        back_avg_t = back_total_dura / num_batches\n",
    "\n",
    "        # 打印反向传播的运算时间信息\n",
    "        print('%s: Forward-backward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "              (datetime.now(), num_batches, back_avg_t,\n",
    "               math.sqrt(back_total_dura_squared / num_batches - back_avg_t * back_avg_t)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
