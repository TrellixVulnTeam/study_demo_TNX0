{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "### 循环神经网络前向传播\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次 np.dot(init_state, W): [0.48 0.3 ]\n",
      "第 0 次 before_activation: [1.32 0.48]\n",
      "第 0 次 state: [0.86678393 0.44624361]\n",
      "第 1 次 np.dot(init_state, W): [0.48572731 0.48058665]\n",
      "第 1 次 before_activation: [0.76572731 0.59058665]\n",
      "第 1 次 state: [0.64443809 0.5303174 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义相关参数，init_state是输入到t1的t0时刻输出的状态\n",
    "x = [0.8,0.1]\n",
    "init_state = [0.3, 0.6]\n",
    "W = np.asarray([[0.2, 0.4], [0.7, 0.3]])\n",
    "U = np.asarray([0.8, 0.1])\n",
    "b_h = np.asarray([0.2, 0.1])\n",
    "V = np.asarray([[0.5], [0.5]])\n",
    "b_o = 0.1\n",
    "\n",
    "#执行两轮循环，模拟前向传播过程\n",
    "for i in range(len(x)):\n",
    "\n",
    "    #numpy的dot()函数用于矩阵相乘，函数原型为dot(a, b, out)\n",
    "    before_activation = np.dot(init_state, W) + x[i] * U + b_h\n",
    "    print('第 %d 次' %(i) ,'np.dot(init_state, W):', np.dot(init_state, W))\n",
    "    print('第 %d 次' %(i) ,'before_activation:', before_activation)\n",
    "\n",
    "\n",
    "    #numpy也提供了tanh()函数实现双曲正切函数的计算\n",
    "    state = np.tanh(before_activation)\n",
    "    print('第 %d 次' %(i) ,'state:', state)\n",
    "    #本时刻的状态作为下一时刻的初始状态\n",
    "    init_state=state\n",
    "\n",
    "    #计算本时刻的输出\n",
    "    final_output = np.dot(state, V) + b_o\n",
    "\n",
    "    # 打印t1和t2时刻的状态和输出信息\n",
    "#     print(\"t%s state: %s\" %(i+1,state))\n",
    "#     print(\"t%s output: %s\\n\" %(i+1,final_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of original word is 17005207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'打印的结果\\n    3082 originated -> 12 as\\n    3082 originated -> 5237 anarchism \\n    12 as -> 3082 originated \\n    12 as -> 6 a\\n    6 a -> 195 term\\n    6 a -> 12 as\\n    195 term -> 6 a\\n    195 term -> 2 of\\n    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "# 出现频率最高的50000词作为单词表\n",
    "vocabulary_size = 50000\n",
    "\n",
    "file = \"src/text8.zip\"\n",
    "\n",
    "\n",
    "def read_data(file):\n",
    "    # ZipFile类的构造函数原型__init__(self,file,mode,compression,allowZip64)\n",
    "    with zipfile.ZipFile(file=file) as f:\n",
    "        # ZipFile类namelist()函数原型namelist(self)\n",
    "        # ZipFile类read()函数原型read(self,name,pwd)\n",
    "        original_data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return original_data\n",
    "\n",
    "\n",
    "original_words = read_data(file)\n",
    "# len()函数是Python中的内容，用于测试列表中元素的数量\n",
    "print(\"len of original word is\", len(original_words))\n",
    "# 输出len of original words is 17005207\n",
    "\n",
    "\n",
    "def build_vocabulary(original_words):\n",
    "    # 创建一个名为count的列表，\n",
    "    count = [[\"unkown\", -1]]\n",
    "\n",
    "    # Counter类构造函数原型__init__(args,kwds)\n",
    "    # Counter类most_common()函数原型most_common(self,n)\n",
    "    # extend()函数会在列表末尾一次性追加另一个序列中的多个值(用于扩展原来的列表）\n",
    "    # 函数原型为extend(self,iterable)\n",
    "    count.extend(collections.Counter(original_words).most_common(vocabulary_size - 1))\n",
    "\n",
    "    # dict类构造函数原型__init__(self,iterable,kwargs)\n",
    "    dictionary = dict()\n",
    "\n",
    "    # 遍历count，并将count中按频率顺序排列好的单词装入dictionary，word为键\n",
    "    # len(dictionary)为键值，这样可以在dictionary中按0到49999的编号引用单词\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = list()\n",
    "\n",
    "    # unkown_count用于计数出现频率较低(属于未知)的单词\n",
    "    unkown_count = 0\n",
    "\n",
    "    # 遍历original_words原始单词列表，该列表并没有将单词按频率顺序排列好\n",
    "    for word in original_words:\n",
    "\n",
    "        if word in dictionary:  # original_words列表中的单词是否出现在dictionary中\n",
    "            index = dictionary[word]  # 取得该单词在dictionary中的编号赋值给index\n",
    "        else:\n",
    "            index = 0  # 没有出现在dictionary中的单词，index将被赋值0\n",
    "            unkown_count += 1  # 计数这些单词\n",
    "\n",
    "        # 列表的append()方法用于扩充列表的大小并在列表的尾部插入一项\n",
    "        # 如果用print(data)将data打印出来，会发现这里这里有很多0值\n",
    "        # 使用print(len(data))会发现data长度和original_words长度相等，都是17005207\n",
    "        data.append(index)\n",
    "\n",
    "    # 将unkown类型的单词数量赋值到count列表的第[0][1]个元素\n",
    "    count[0][1] = unkown_count\n",
    "\n",
    "    # 反转dictionary中的键值和键，并存入另一个字典中\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "# data, count, dictionary, reverse_dictionary = build_vocabulary(original_words)\n",
    "# count[:5]是列表的切片操作，获取列表的前5个元素并作为一个新列表返回\n",
    "# data[:10]在原理上是相同的\n",
    "# 打印unkown类的词汇量及top4的单词的数量\n",
    "# print(\"Most common words (+unkwon)\", count[:5])\n",
    "# 输出Most common words (+unkwon) [['unkown', 418391], ('the', 1061396),\n",
    "#                          ('of', 593677), ('and', 416629), ('one', 411764)]\n",
    "# 打印data中前十个单词及其编号\n",
    "# print(\"Sample data\", data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "# 输出Sample data [5235, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
    "# ['anarchism','originated','as','a','term','of','abuse','first','used','against']\n",
    "\n",
    "\n",
    "data_index = 0\n",
    "data, count, dictionary, reverse_dictionary = build_vocabulary(original_words)\n",
    "def generate_batch(batch_size, num_of_samples, skip_distance):\n",
    "    # 单词序号data_index定义为global变量，global是Python中的命名空间声明\n",
    "    # 因为之后会多次调用data_index，并在函数内对其进行修改\n",
    "    global data_index\n",
    "\n",
    "    # 创建放置产生的batch和labels的容器\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    num_of_sample_words = 2 * skip_distance + 1\n",
    "\n",
    "    #创建buffer队列，长度为num_of_sample_words，因为generate_batch()函数\n",
    "    #会被调用多次，所以这里使用buffer队列暂存来自data的编号\n",
    "    buffer = collections.deque(maxlen=num_of_sample_words)\n",
    "    for _ in range(num_of_sample_words):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1)\n",
    "\n",
    "\n",
    "    # Python中//运算符会对商结果取整\n",
    "    for i in range(batch_size // num_of_samples):\n",
    "        #target=1，它在一个三元素列表中位于中间的位置，所以下标为skip_distance值\n",
    "        #targets_to_avoid是生成样本时需要避免的单词列表\n",
    "        target = skip_distance\n",
    "        targets_to_avoid = [skip_distance]\n",
    "\n",
    "        for j in range(num_of_samples):\n",
    "            while target in targets_to_avoid:\n",
    "                # 使用randint()函数用于产生0到num_of_sample_words-1之间的随机整数，\n",
    "                # 使得target不在targets_to_avoid中\n",
    "                target = random.randint(0, num_of_sample_words - 1)\n",
    "            # 将需要避免的目标单词加入到列表targets_to_avoid中，在while后面使用append\n",
    "            # 的方式可以避免target是两个重复的值，比如两个0\n",
    "            targets_to_avoid.append(target)\n",
    "\n",
    "            # i*num_skips+j最终会等于batch_size-1\n",
    "            # 存入batch和labels的数据来源于buffer,而buffer中的数据来源于data\n",
    "            # 也就是说，数组batch存储了目标单词在data中的索引\n",
    "            # 而列表labels存储了语境单词(与目标单词相邻的单词)在data中的索引\n",
    "            batch[i * num_of_samples + j] = buffer[skip_distance]\n",
    "            labels[i * num_of_samples + j, 0] = buffer[target]\n",
    "\n",
    "        # 在最外层的for循环使用append()函数将一个新的目标单词入队，清空队列最前面的单词\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1)\n",
    "    return batch, labels\n",
    "'''\n",
    "batch, labels = generate_batch(batch_size=8, num_of_samples=2, skip_distance=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], \n",
    "          \"->\", labels[i, 0],reverse_dictionary[labels[i, 0]])\n",
    "'''\n",
    "'''打印的结果\n",
    "    3082 originated -> 12 as\n",
    "    3082 originated -> 5237 anarchism \n",
    "    12 as -> 3082 originated \n",
    "    12 as -> 6 a\n",
    "    6 a -> 195 term\n",
    "    6 a -> 12 as\n",
    "    195 term -> 6 a\n",
    "    195 term -> 2 of\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1023 20:39:47.016865 140389855897408 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1023 20:39:47.909790 140389855897408 deprecation.py:506] From <ipython-input-3-b75548b9eaef>:60: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b75548b9eaef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# 调用generate_batch()函数生成用于训练的batch及其labels，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             batch_inputs, batch_labels = vocabulary.generate_batch(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 batch_size, num_of_samples, skip_distance)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "#import vocabulary\n",
    "\n",
    "max_steps = 100000  # 训练最大迭代次数10w次\n",
    "batch_size = 128\n",
    "embedding_size = 128  # 嵌入向量的尺寸\n",
    "skip_distance = 1  # 相邻单词数\n",
    "num_of_samples = 2  # 对每个单词生成多少样本\n",
    "\n",
    "vocabulary_size = 50000  # 词汇量\n",
    "\n",
    "# numpy中choice()函数的函数原型为choice(a,size,replace,p)\n",
    "# choice()函数用于在a给出的范围内抽取size个大小的数组成一个一维数组\n",
    "# 当设置了replace=False则表示组成的这个一维数组中不能够有重复的数字\n",
    "valid_examples = np.random.choice(100, 16, replace=False)\n",
    "\n",
    "num_sampled = 64  # 训练时用来做负样本的噪声单词的数量\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # train_inputs和train_labels是训练数据及其label的placeholder\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    # embeddings是所有50000高频单词的词向量，向量的维度是128，数值是由\n",
    "    # random_uniform()函数生成的在-1.0到1.0之间平均分布的数值\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # embedding_lookup()函数用于选取一个张量里面索引对应的元素，函数原型是：\n",
    "    # embedding_lookup(params,ids,partition_strategy,name,validate_indices,max_norm)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # 用truncated_normal()函数产生标准差为1.0/math.sqrt(embedding_size)的正态分布数据\n",
    "    # 产生的nce_weights作为NCE loss中的权重参数\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "    # 产生的nce_biases作为NCE loss中的偏置参数\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # 计算词向量embedding在训练数据上的loss\n",
    "    # nce_loss()函数原型nce_loss(weights, biases, inputs, labels, num_sampled, num_classes,\n",
    "    #                                    num_true=1,sampled_values,remove_accidental_hits,\n",
    "    #                                                             partition_strategy,name)\n",
    "    nec_loss = tf.nn.nce_loss(weights=nce_weights, biases=nce_biases,\n",
    "                              labels=train_labels, inputs=embed,\n",
    "                              num_sampled=num_sampled,\n",
    "                              num_classes=vocabulary_size)\n",
    "\n",
    "    # 求nce_loss的均值\n",
    "    loss = tf.reduce_mean(nec_loss)\n",
    "\n",
    "    # 创建优化器，学习率为固定的1.0，最小化loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # square()函数用于求平方，之后使用reduce_sum()函数求和\n",
    "    # keep_dims=True表示求和之后维度不会发生改变\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    # 在标准化后的所有单词的词向量值中寻找随机抽取的16个单词对应的词向量值\n",
    "    # 在这之前，valid_inputs是由数组valid_examples进行constant操作转化为张量得来，\n",
    "    valid_inputs = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_inputs)\n",
    "\n",
    "    # 使用matmul()函数计算相似度\n",
    "    # 函数原型matmul(a, b, transpose_a, transpose_b, a_is_sparse, b_is_sparse, name)\n",
    "    # 在函数matmul()的定义中，name参数默认为None，除a和b外其他参数都有默认的\n",
    "    # False值，在这里我们设参数transpose_b设True，表示对参数b传入的矩阵进行转置\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # 开始训练\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # 总损失与平均损失\n",
    "        total_loss = 0\n",
    "        average_loss = 0\n",
    "\n",
    "        for step in range(max_steps + 1):\n",
    "\n",
    "            # 调用generate_batch()函数生成用于训练的batch及其labels，\n",
    "            batch_inputs, batch_labels = vocabulary.generate_batch(\n",
    "                batch_size, num_of_samples, skip_distance)\n",
    "\n",
    "            # 运行loss的计算及最小化loss的优化器\n",
    "            loss_val, _ = sess.run([loss, optimizer], feed_dict={train_inputs: batch_inputs,\n",
    "                                                                 train_labels: batch_labels})\n",
    "\n",
    "            # total_loss用于计算总损失，在每一轮迭代后都会与loos_val相加\n",
    "            total_loss += loss_val\n",
    "\n",
    "            # 每进行1000轮迭代就输出平均损失的值，并将average_loss和total_loss\n",
    "            # 重新归零，方便下一个1000轮的计算\n",
    "            if step > 0 and step % 1000 == 0:\n",
    "                average_loss = total_loss / 1000\n",
    "                print(\"Average loss at %d step is:%f \" % (step, average_loss))\n",
    "                average_loss = 0\n",
    "                total_loss = 0\n",
    "\n",
    "            # 每隔5000轮就打印一次与验证单词最相似的8个单词\n",
    "            if step > 0 and step % 5000 == 0:\n",
    "\n",
    "                # 执行计算相似性的操作\n",
    "                similar = similarity.eval()\n",
    "\n",
    "                # 外层循环16次，\n",
    "                for i in range(16):\n",
    "\n",
    "                    # 每执行一次最外层的循环，都会得到一个验证单词对应的nearest，\n",
    "                    # 这里有8个数据，是与验证单词最相近的单词的编号，通过\n",
    "                    # reverse_dictionary可以得到确切的单词\n",
    "                    nearest = (-similar[i, :]).argsort()[1:8 + 1]\n",
    "\n",
    "                    # 定义需要打印的字符串，其中valid_word是通过reverse_dictionary得到的验证单词\n",
    "                    valid_word = vocabulary.reverse_dictionary[valid_examples[i]]\n",
    "                    nearest_information = \"Nearest to %s is:\" % valid_word\n",
    "\n",
    "                    for j in range(8):\n",
    "                        # 在8个循环内通过reverse_dictionary得到与验证单词相近的8个单词的原型\n",
    "                        # 并改进需要打印的字符串\n",
    "                        close_word = vocabulary.reverse_dictionary[nearest[j]]\n",
    "                        nearest_information = \" %s %s\" % (nearest_information,close_word)\n",
    "\n",
    "                    # 打印出验证单词及与验证单词相近的8个单词\n",
    "                    print(\"valid_word is: %s\"% valid_word)\n",
    "                    print(nearest_information)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "'''打印的信息\n",
    "len of original word is 17005207\n",
    "Average loss at 1000 step is:145.009534 \n",
    "Average loss at 2000 step is:84.632241 \n",
    "Average loss at 3000 step is:58.798290 \n",
    "Average loss at 4000 step is:46.023421 \n",
    "Average loss at 5000 step is:35.622134 \n",
    "valid_word is: state\n",
    "        Nearest to state is: we noted reginae ants atlantic facts part asparagales\n",
    "valid_word is: states\n",
    "        Nearest to states is: cl acres aircraft victoriae expansion victim investment fullback\n",
    "valid_word is: it\n",
    "        Nearest to it is: preserve turned victoriae collation everyone gland opium lining\n",
    "valid_word is: up\n",
    "        Nearest to up is: columbia austin segments gland reginae metaphysics faith once\n",
    "valid_word is: between\n",
    "        Nearest to between is: populated mourned grow reginae deprived play ada afl\n",
    "valid_word is: with\n",
    "        Nearest to with is: and ada if programming acting aq carry suggested\n",
    "valid_word is: use\n",
    "        Nearest to use is: focuses austin reginae victoriae broadcasts influence out wire\n",
    "valid_word is: his\n",
    "        Nearest to his is: victoriae alignment perform cl byrd dynamics kick perennials\n",
    "valid_word is: two\n",
    "        Nearest to two is: victoriae cambodia one austin agave nine mouth outline\n",
    "valid_word is: three\n",
    "        Nearest to three is: cl agave zero victoriae reginae poets austin archaeologists\n",
    "valid_word is: but\n",
    "        Nearest to but is: built darius conference ayn geology cm victoriae gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: cl victoriae soap until york syllabary august agave\n",
    "valid_word is: the\n",
    "        Nearest to the is: a and victoriae one ptolemy unkown austin of\n",
    "valid_word is: in\n",
    "        Nearest to in is: and of to on emmy petroleum victoriae initial\n",
    "valid_word is: united\n",
    "        Nearest to united is: functions und reginae loyalists linebackers roman campaign austin\n",
    "valid_word is: during\n",
    "        Nearest to during is: results nozick cambodia gland linguistics victoriae captured russia\n",
    "Average loss at 6000 step is:30.947824 \n",
    "Average loss at 7000 step is:24.874575 \n",
    "Average loss at 8000 step is:22.010970 \n",
    "Average loss at 9000 step is:19.140614 \n",
    "Average loss at 10000 step is:16.452675 \n",
    "valid_word is: state\n",
    "        Nearest to state is: casement noted we ants flee atlantic reginae however\n",
    "valid_word is: states\n",
    "        Nearest to states is: cl aircraft expansion acres victoriae victim kept architectural\n",
    "valid_word is: it\n",
    "        Nearest to it is: he victoriae preserve hustler bang drawings gland cl\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada sarcoma columbia segments enlightened fervent austin metaphysics\n",
    "valid_word is: between\n",
    "        Nearest to between is: deprived mourned populated one grow in afl play\n",
    "valid_word is: with\n",
    "        Nearest to with is: and in of easy if motorcycle markov from\n",
    "valid_word is: use\n",
    "        Nearest to use is: focuses austin broadcasts reginae victoriae markov amo influence\n",
    "valid_word is: his\n",
    "        Nearest to his is: the their victoriae a alignment longest perform dose\n",
    "valid_word is: two\n",
    "        Nearest to two is: one victoriae unkown cambodia nine austin reginae markov\n",
    "valid_word is: three\n",
    "        Nearest to three is: nine victoriae zero four reginae agave one five\n",
    "valid_word is: but\n",
    "        Nearest to but is: built and conference ayn darius scenario geology posters\n",
    "valid_word is: more\n",
    "        Nearest to more is: cl roper modes victoriae soap nextstep lords until\n",
    "valid_word is: the\n",
    "        Nearest to the is: a austin unkown his victoriae hydrate one ptolemy\n",
    "valid_word is: in\n",
    "        Nearest to in is: of and on at with reginae by unkown\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro functions und car amo conversely reginae perimeter\n",
    "valid_word is: during\n",
    "        Nearest to during is: results nozick architect at receivers cambodia stands kicking\n",
    "Average loss at 11000 step is:14.630424 \n",
    "Average loss at 12000 step is:13.392861 \n",
    "Average loss at 13000 step is:12.213534 \n",
    "Average loss at 14000 step is:10.764199 \n",
    "Average loss at 15000 step is:10.540506 \n",
    "valid_word is: state\n",
    "        Nearest to state is: meridians flee noted ants casement tats we atlantic\n",
    "valid_word is: states\n",
    "        Nearest to states is: pains expansion acres cl hello siena aircraft victoriae\n",
    "valid_word is: it\n",
    "        Nearest to it is: he victoriae they bang cl hustler this preserve\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada sarcoma columbia dasyprocta lignite fervent enlightened trumps\n",
    "valid_word is: between\n",
    "        Nearest to between is: deprived in mourned grow populated at meridians practised\n",
    "valid_word is: with\n",
    "        Nearest to with is: in and from dasyprocta of easy to as\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti broadcasts focuses dasyprocta austin amo markov convicted\n",
    "valid_word is: his\n",
    "        Nearest to his is: the their its a victoriae s longest dose\n",
    "valid_word is: two\n",
    "        Nearest to two is: one nine three eight five four zero dasyprocta\n",
    "valid_word is: three\n",
    "        Nearest to three is: two zero nine four five eight one six\n",
    "valid_word is: but\n",
    "        Nearest to but is: agouti and dasyprocta built epistle geology meridians ayn\n",
    "valid_word is: more\n",
    "        Nearest to more is: roper cl modes quarried lords aim dasyprocta drivers\n",
    "valid_word is: the\n",
    "        Nearest to the is: a his and dasyprocta austin its agouti some\n",
    "valid_word is: in\n",
    "        Nearest to in is: of and at on with by for nine\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro functions und octavia weaver clement oblique calvinism\n",
    "valid_word is: during\n",
    "        Nearest to during is: results nozick at architect receivers kicking expounded stands\n",
    "Average loss at 16000 step is:9.183173 \n",
    "Average loss at 17000 step is:8.530489 \n",
    "Average loss at 18000 step is:8.715831 \n",
    "Average loss at 19000 step is:8.228534 \n",
    "Average loss at 20000 step is:7.280190 \n",
    "valid_word is: state\n",
    "        Nearest to state is: tats flee ants meridians noted casement however atlantic\n",
    "valid_word is: states\n",
    "        Nearest to states is: imran brine pains stabbed acres cl hello siena\n",
    "valid_word is: it\n",
    "        Nearest to it is: he they this victoriae antipope bang cl not\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada sarcoma columbia dasyprocta trumps lignite atop segments\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned and by of at populated\n",
    "valid_word is: with\n",
    "        Nearest to with is: in and from of by for dasyprocta as\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti broadcasts dasyprocta focuses austin convicted out influence\n",
    "valid_word is: his\n",
    "        Nearest to his is: the their its s a victoriae longest dose\n",
    "valid_word is: two\n",
    "        Nearest to two is: one three five four eight zero nine six\n",
    "valid_word is: three\n",
    "        Nearest to three is: two four zero eight five six nine dasyprocta\n",
    "valid_word is: but\n",
    "        Nearest to but is: agouti and dasyprocta built meridians victoriae that gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried roper three modes cl lords aim\n",
    "valid_word is: the\n",
    "        Nearest to the is: a his its dasyprocta agouti victoriae their some\n",
    "valid_word is: in\n",
    "        Nearest to in is: at with of and on from by for\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro functions octavia und weaver clement nlm conversely\n",
    "valid_word is: during\n",
    "        Nearest to during is: at receivers for results architect in nozick clergyman\n",
    "Average loss at 21000 step is:7.174989 \n",
    "Average loss at 22000 step is:7.416212 \n",
    "Average loss at 23000 step is:6.940413 \n",
    "Average loss at 24000 step is:6.959669 \n",
    "Average loss at 25000 step is:6.733032 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps meridians flee tats noted ants casement predates\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed pains acres imran brine siena aircraft expansion\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this they victoriae there not antipope aediles\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada sarcoma columbia trumps dasyprocta atop integrity lignite\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned at by afl populated five\n",
    "valid_word is: with\n",
    "        Nearest to with is: in and from by for or as dasyprocta\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti focuses broadcasts dasyprocta dek convicted austin out\n",
    "valid_word is: his\n",
    "        Nearest to his is: their the its s her dose alignment a\n",
    "valid_word is: two\n",
    "        Nearest to two is: one three four five six eight nine agouti\n",
    "valid_word is: three\n",
    "        Nearest to three is: eight four six five two nine seven zero\n",
    "valid_word is: but\n",
    "        Nearest to but is: agouti and dasyprocta built meridians gland victoriae reginae\n",
    "valid_word is: more\n",
    "        Nearest to more is: most three quarried modes roper cl hamburger ramps\n",
    "valid_word is: the\n",
    "        Nearest to the is: a his their dasyprocta its agouti victoriae ramps\n",
    "valid_word is: in\n",
    "        Nearest to in is: at from on nine and of with initial\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro octavia of und functions perimeter weaver clement\n",
    "valid_word is: during\n",
    "        Nearest to during is: at for in receivers clergyman results architect chesapeake\n",
    "Average loss at 26000 step is:6.602586 \n",
    "Average loss at 27000 step is:6.366769 \n",
    "Average loss at 28000 step is:5.879195 \n",
    "Average loss at 29000 step is:6.114192 \n",
    "Average loss at 30000 step is:6.131620 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps meridians ants tats flee casement noted stadium\n",
    "valid_word is: states\n",
    "        Nearest to states is: pains stabbed acres brine imran siena cl aircraft\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this they there akita not victoriae antipope\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada trumps sarcoma columbia dasyprocta atop segments abkhazians\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned by davids at practised of\n",
    "valid_word is: with\n",
    "        Nearest to with is: in from by for and as or dasyprocta\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti broadcasts focuses dek dasyprocta convicted out austin\n",
    "valid_word is: his\n",
    "        Nearest to his is: their the its s a her alignment victoriae\n",
    "valid_word is: two\n",
    "        Nearest to two is: four three one five six eight seven zero\n",
    "valid_word is: three\n",
    "        Nearest to three is: four eight six five two seven nine zero\n",
    "valid_word is: but\n",
    "        Nearest to but is: agouti and dasyprocta that meridians victoriae built gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried modes hamburger cl roper ramps lords\n",
    "valid_word is: the\n",
    "        Nearest to the is: its a his their akita ramps dasyprocta this\n",
    "valid_word is: in\n",
    "        Nearest to in is: at on of with and from nine by\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro primigenius octavia lemonade of weaver und functions\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for by receivers clergyman architect sdi\n",
    "Average loss at 31000 step is:5.967377 \n",
    "Average loss at 32000 step is:5.830637 \n",
    "Average loss at 33000 step is:5.924866 \n",
    "Average loss at 34000 step is:5.862029 \n",
    "Average loss at 35000 step is:5.765287 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps tats ants predates casement meridians flee stadium\n",
    "valid_word is: states\n",
    "        Nearest to states is: pains stabbed acres aircraft siena zubaydah brine imran\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there they akita not victoriae gland\n",
    "valid_word is: up\n",
    "        Nearest to up is: granada trumps columbia sarcoma dasyprocta once atop segments\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned by of at davids with\n",
    "valid_word is: with\n",
    "        Nearest to with is: in and from by for or as dasyprocta\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti broadcasts focuses dasyprocta out dek convicted austin\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its the her s alignment dose a\n",
    "valid_word is: two\n",
    "        Nearest to two is: four three one five six seven eight zero\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six eight two seven nine zero\n",
    "valid_word is: but\n",
    "        Nearest to but is: agouti and dasyprocta that victoriae meridians gland built\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried modes hamburger cl three roper ramps\n",
    "valid_word is: the\n",
    "        Nearest to the is: akita its a this dasyprocta their his ramps\n",
    "valid_word is: in\n",
    "        Nearest to in is: at and on from with of by nine\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro lemonade perimeter octavia primigenius of weaver clement\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for receivers clergyman by liqueur chesapeake\n",
    "Average loss at 36000 step is:5.624653 \n",
    "Average loss at 37000 step is:4.989881 \n",
    "Average loss at 38000 step is:5.553697 \n",
    "Average loss at 39000 step is:5.567795 \n",
    "Average loss at 40000 step is:5.324156 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps meridians tats ants casement flee predates stadium\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres pains aircraft imran guderian zubaydah cl\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there they akita not victoriae and\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada dasyprocta sarcoma columbia extract atop once\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned by at with davids and\n",
    "valid_word is: with\n",
    "        Nearest to with is: in and from or for by dasyprocta as\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta broadcasts focuses convicted dek austin abet\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its the her s alignment dose a\n",
    "valid_word is: two\n",
    "        Nearest to two is: four three five six one seven eight zero\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six eight seven two zero nine\n",
    "valid_word is: but\n",
    "        Nearest to but is: and agouti dasyprocta however that victoriae meridians gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried modes hamburger six cl three sleepy\n",
    "valid_word is: the\n",
    "        Nearest to the is: a its his their akita dasyprocta ramps this\n",
    "valid_word is: in\n",
    "        Nearest to in is: at on and from with of nine reginae\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro of primigenius lemonade perimeter octavia abitibi clement\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for receivers clergyman by liqueur sdi\n",
    "Average loss at 41000 step is:5.274549 \n",
    "Average loss at 42000 step is:5.295857 \n",
    "Average loss at 43000 step is:5.367129 \n",
    "Average loss at 44000 step is:5.269009 \n",
    "Average loss at 45000 step is:5.295506 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps carbohydrates meridians predates stadium flee casement tats\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed pains acres aircraft imran siena zubaydah cl\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there they akita she victoriae that\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada columbia sarcoma dasyprocta extract atop integrity\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived with mourned by at davids of\n",
    "valid_word is: with\n",
    "        Nearest to with is: in from or and for dasyprocta by markov\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta focuses broadcasts dek out convicted abet\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its the her s renminbi alignment prism\n",
    "valid_word is: two\n",
    "        Nearest to two is: four three six five one eight seven zero\n",
    "valid_word is: three\n",
    "        Nearest to three is: four six five eight two seven nine zero\n",
    "valid_word is: but\n",
    "        Nearest to but is: and agouti however dasyprocta that victoriae meridians gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried hamburger modes cl three asatru roper\n",
    "valid_word is: the\n",
    "        Nearest to the is: its a their his akita dasyprocta this victoriae\n",
    "valid_word is: in\n",
    "        Nearest to in is: at on from with for of and nine\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro perimeter lemonade octavia of functions weaver roman\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for receivers clergyman by chesapeake sdi\n",
    "Average loss at 46000 step is:5.276804 \n",
    "Average loss at 47000 step is:4.984429 \n",
    "Average loss at 48000 step is:5.082731 \n",
    "Average loss at 49000 step is:5.238467 \n",
    "Average loss at 50000 step is:5.081029 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps kapoor carbohydrates predates stadium flee meridians casement\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed pains acres zubaydah siena imran aircraft brine\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there they she akita which victoriae\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada extract dasyprocta sarcoma columbia lignite integrity\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived mourned with by original at seven\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or and from dasyprocta by markov thibetanus\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta out dek focuses broadcasts austin abet\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its the her s dose altenberg prism\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four one five six seven eight zero\n",
    "valid_word is: three\n",
    "        Nearest to three is: four six five two eight seven zero nine\n",
    "valid_word is: but\n",
    "        Nearest to but is: and agouti however thibetanus dasyprocta meridians victoriae gland\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried hamburger clearly asatru cl tied modes\n",
    "valid_word is: the\n",
    "        Nearest to the is: its their akita his a this agouti dasyprocta\n",
    "valid_word is: in\n",
    "        Nearest to in is: at on and from nine kapoor of reginae\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro perimeter lemonade primigenius octavia functions abitibi roman\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for receivers is clergyman by answered\n",
    "Average loss at 51000 step is:5.210266 \n",
    "Average loss at 52000 step is:5.162591 \n",
    "Average loss at 53000 step is:5.127516 \n",
    "Average loss at 54000 step is:5.150660 \n",
    "Average loss at 55000 step is:5.055330 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps kapoor predates stadium meridians carbohydrates casement michelob\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres pains zubaydah aircraft scholarships siena imran\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there they which she akita victoriae\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada dasyprocta tours extract columbia integrity sarcoma\n",
    "valid_word is: between\n",
    "        Nearest to between is: in deprived with mourned original by afl davids\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or by from and dasyprocta markov for\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta broadcasts out abet abitibi dek focuses\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its the her s megabats dose altenberg\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four five six one eight seven michelob\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six eight two seven nine zero\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti michelob thibetanus dasyprocta meridians victoriae\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried michelob hamburger terribly very tied cl\n",
    "valid_word is: the\n",
    "        Nearest to the is: its microbats their akita his this dasyprocta ramps\n",
    "valid_word is: in\n",
    "        Nearest to in is: at and from on kapoor of during with\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro perimeter primigenius functions lemonade octavia abitibi roman\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at for clergyman receivers by chesapeake thibetanus\n",
    "Average loss at 56000 step is:5.067634 \n",
    "Average loss at 57000 step is:5.058398 \n",
    "Average loss at 58000 step is:5.156872 \n",
    "Average loss at 59000 step is:4.971608 \n",
    "Average loss at 60000 step is:4.902193 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps kapoor meridians michelob predates arms synod casement\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres pains zubaydah imran michelob cl scholarships\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they akita callithrix which\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada tours extract dasyprocta integrity columbia sarcoma\n",
    "valid_word is: between\n",
    "        Nearest to between is: in with deprived mourned holographic original davids by\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or fets by and dasyprocta thibetanus markov\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta cebus microcebus dek abitibi broadcasts out\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s recordings alignment dose\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four five six one seven eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six two eight seven nine one\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti thibetanus dasyprocta michelob meridians victoriae\n",
    "valid_word is: more\n",
    "        Nearest to more is: most quarried michelob very microcebus hamburger cl plaintext\n",
    "valid_word is: the\n",
    "        Nearest to the is: their its microbats akita dasyprocta a this his\n",
    "valid_word is: in\n",
    "        Nearest to in is: at on and from of kapoor thibetanus with\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro roman perimeter primigenius abitibi lemonade functions octavia\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at clergyman receivers after for answered chesapeake\n",
    "Average loss at 61000 step is:4.859219 \n",
    "Average loss at 62000 step is:4.740433 \n",
    "Average loss at 63000 step is:4.596152 \n",
    "Average loss at 64000 step is:4.966679 \n",
    "Average loss at 65000 step is:5.010026 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps kapoor meridians michelob predates stadium casement nils\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres scholarships pains expansion imran zubaydah twh\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita callithrix\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps granada clo tours extract dasyprocta columbia integrity\n",
    "valid_word is: between\n",
    "        Nearest to between is: in with deprived mourned holographic original davids hometown\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or fets by dasyprocta between six markov\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta thaler cebus microcebus dek abitibi out\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s alignment recordings dose\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four six five one seven eight michelob\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six two seven eight nine callithrix\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti dasyprocta thibetanus michelob which meridians\n",
    "valid_word is: more\n",
    "        Nearest to more is: most very quarried less michelob microcebus plaintext hamburger\n",
    "valid_word is: the\n",
    "        Nearest to the is: microbats its their this akita his dasyprocta a\n",
    "valid_word is: in\n",
    "        Nearest to in is: at kapoor from during thaler on callithrix initial\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro perimeter roman abitibi primigenius lemonade clement nlm\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at clergyman after receivers from chesapeake thibetanus\n",
    "Average loss at 66000 step is:4.913359 \n",
    "Average loss at 67000 step is:4.897437 \n",
    "Average loss at 68000 step is:4.929057 \n",
    "Average loss at 69000 step is:4.698001 \n",
    "Average loss at 70000 step is:4.857533 \n",
    "valid_word is: state\n",
    "        Nearest to state is: compost ramps stadium meridians states kapoor predates interludes\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres pains michelob scholarships imran expansion cl\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they akita callithrix which\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps dasyprocta granada clo tours extract lignite selma\n",
    "valid_word is: between\n",
    "        Nearest to between is: in with deprived holographic mourned original davids hometown\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or fets by dasyprocta between and purchases\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta thaler cebus microcebus abitibi abet convicted\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s recordings megabats alignment\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four six one five seven eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six two seven eight callithrix nine\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti dasyprocta michelob which thibetanus meridians\n",
    "valid_word is: more\n",
    "        Nearest to more is: most very cira less quarried plaintext michelob microcebus\n",
    "valid_word is: the\n",
    "        Nearest to the is: microbats their its this akita dasyprocta a cebus\n",
    "valid_word is: in\n",
    "        Nearest to in is: at from on kapoor during thaler thibetanus with\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro of perimeter abitibi roman primigenius clement lemonade\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman receivers chesapeake thibetanus from\n",
    "Average loss at 71000 step is:4.843493 \n",
    "Average loss at 72000 step is:4.764801 \n",
    "Average loss at 73000 step is:4.790531 \n",
    "Average loss at 74000 step is:4.783862 \n",
    "Average loss at 75000 step is:4.904189 \n",
    "valid_word is: state\n",
    "        Nearest to state is: compost ramps stadium kapoor meridians predates interludes antinomies\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed acres scholarships pains michelob expansion imran zubaydah\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita callithrix\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps tours granada clo lignite extract dasyprocta selma\n",
    "valid_word is: between\n",
    "        Nearest to between is: in with holographic deprived mourned original davids hometown\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or fets dasyprocta between markov sprague purchases\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta thaler cebus microcebus abitibi dek sprint\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s recordings megabats alignment\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four six five seven one eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: four six five two seven eight nine callithrix\n",
    "valid_word is: but\n",
    "        Nearest to but is: however agouti dasyprocta and michelob meridians thibetanus which\n",
    "valid_word is: more\n",
    "        Nearest to more is: most very less quarried cira michelob plaintext microcebus\n",
    "valid_word is: the\n",
    "        Nearest to the is: microbats their its akita his ramps dasyprocta agouti\n",
    "valid_word is: in\n",
    "        Nearest to in is: at kapoor from on of during thaler and\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro perimeter of roman clement lemonade abitibi antiparticle\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman from receivers is chesapeake\n",
    "Average loss at 76000 step is:4.813737 \n",
    "Average loss at 77000 step is:4.847285 \n",
    "Average loss at 78000 step is:4.753476 \n",
    "Average loss at 79000 step is:4.813591 \n",
    "Average loss at 80000 step is:4.790038 \n",
    "valid_word is: state\n",
    "        Nearest to state is: compost ramps kapoor stadium meridians michelob states antinomies\n",
    "valid_word is: states\n",
    "        Nearest to states is: stabbed scholarships acres michelob state pains expansion imran\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita victoriae\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps tours vec granada kanem dasyprocta clo lignite\n",
    "valid_word is: between\n",
    "        Nearest to between is: with in holographic deprived mourned original hometown afl\n",
    "valid_word is: with\n",
    "        Nearest to with is: in or between by fets dasyprocta thibetanus callithrix\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta cebus thaler abitibi microcebus dek abet\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s recordings megabats prism\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four six five seven one eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: four five six two seven eight callithrix microcebus\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti dasyprocta michelob meridians thibetanus thaler\n",
    "valid_word is: more\n",
    "        Nearest to more is: most very less quarried cira michelob microcebus plaintext\n",
    "valid_word is: the\n",
    "        Nearest to the is: its their microbats akita this dasyprocta a his\n",
    "valid_word is: in\n",
    "        Nearest to in is: at during kapoor on from thaler nine with\n",
    "valid_word is: united\n",
    "        Nearest to united is: pedro of perimeter roman clement abitibi lemonade antiparticle\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman from receivers was thibetanus\n",
    "Average loss at 81000 step is:4.822560 \n",
    "Average loss at 82000 step is:4.770881 \n",
    "Average loss at 83000 step is:4.769358 \n",
    "Average loss at 84000 step is:4.790236 \n",
    "Average loss at 85000 step is:4.782678 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps compost kapoor stadium states meridians michelob antinomies\n",
    "valid_word is: states\n",
    "        Nearest to states is: state stabbed scholarships michelob acres expansion imran viewer\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita callithrix\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps tours vec out lignite granada kanem extract\n",
    "valid_word is: between\n",
    "        Nearest to between is: with in holographic deprived mourned original hometown afl\n",
    "valid_word is: with\n",
    "        Nearest to with is: in tamias between or fets by and dasyprocta\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta cebus thaler abitibi microcebus tamias dek\n",
    "valid_word is: his\n",
    "        Nearest to his is: their its her the s recordings megabats altenberg\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four one six five seven eight microcebus\n",
    "valid_word is: three\n",
    "        Nearest to three is: five four seven two six eight one nine\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti dasyprocta michelob meridians which although\n",
    "valid_word is: more\n",
    "        Nearest to more is: most very less quarried cotswold michelob cira plaintext\n",
    "valid_word is: the\n",
    "        Nearest to the is: its their akita microbats a this dasyprocta cebus\n",
    "valid_word is: in\n",
    "        Nearest to in is: at during kapoor on from of and thibetanus\n",
    "valid_word is: united\n",
    "        Nearest to united is: of pedro roman perimeter abitibi primigenius antiparticle lemonade\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman for from receivers chesapeake\n",
    "Average loss at 86000 step is:4.719234 \n",
    "Average loss at 87000 step is:4.690412 \n",
    "Average loss at 88000 step is:4.693576 \n",
    "Average loss at 89000 step is:4.759110 \n",
    "Average loss at 90000 step is:4.723030 \n",
    "valid_word is: state\n",
    "        Nearest to state is: compost stadium ramps kapoor states meridians antinomies interludes\n",
    "valid_word is: states\n",
    "        Nearest to states is: michelob scholarships state stabbed acres imran expansion tamarin\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita callithrix\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps tours out lignite granada kanem selma extract\n",
    "valid_word is: between\n",
    "        Nearest to between is: with in holographic deprived mourned original hometown afl\n",
    "valid_word is: with\n",
    "        Nearest to with is: tamias between in or dasyprocta fets for markov\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta cebus fsm abitibi thaler microcebus methane\n",
    "valid_word is: his\n",
    "        Nearest to his is: their her its the s recordings megabats altenberg\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four five six seven one eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: five four two six seven eight callithrix microcebus\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and agouti which dasyprocta although michelob meridians\n",
    "valid_word is: more\n",
    "        Nearest to more is: most less very cotswold cira quarried terribly microcebus\n",
    "valid_word is: the\n",
    "        Nearest to the is: akita its microbats dasyprocta ramps their callithrix cebus\n",
    "valid_word is: in\n",
    "        Nearest to in is: at during kapoor on and thaler of from\n",
    "valid_word is: united\n",
    "        Nearest to united is: of roman pedro perimeter abitibi antiparticle clement nlm\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman for of from chesapeake\n",
    "Average loss at 91000 step is:4.704591 \n",
    "Average loss at 92000 step is:4.708268 \n",
    "Average loss at 93000 step is:4.578086 \n",
    "Average loss at 94000 step is:4.664870 \n",
    "Average loss at 95000 step is:4.694557 \n",
    "valid_word is: state\n",
    "        Nearest to state is: ramps compost kapoor stadium meridians states michelob antinomies\n",
    "valid_word is: states\n",
    "        Nearest to states is: state scholarships michelob stabbed expansion acres imran viewer\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which but akita\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps out tours him them selma lignite kanem\n",
    "valid_word is: between\n",
    "        Nearest to between is: with in holographic deprived mourned original hometown afl\n",
    "valid_word is: with\n",
    "        Nearest to with is: in tamias between eight or for fets six\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta fsm cebus abitibi thaler microcebus sprint\n",
    "valid_word is: his\n",
    "        Nearest to his is: their her its the s megabats prism recordings\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four five six seven one eight callithrix\n",
    "valid_word is: three\n",
    "        Nearest to three is: five four two seven six eight callithrix microcebus\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and which agouti dasyprocta although though meridians\n",
    "valid_word is: more\n",
    "        Nearest to more is: most less very quarried widening cira cotswold terribly\n",
    "valid_word is: the\n",
    "        Nearest to the is: its their his a akita microbats this dasyprocta\n",
    "valid_word is: in\n",
    "        Nearest to in is: at during kapoor on from thaler within thibetanus\n",
    "valid_word is: united\n",
    "        Nearest to united is: roman perimeter pedro antiparticle abitibi clement plum nlm\n",
    "valid_word is: during\n",
    "        Nearest to during is: in after at clergyman of from for chesapeake\n",
    "Average loss at 96000 step is:4.768890 \n",
    "Average loss at 97000 step is:4.600439 \n",
    "Average loss at 98000 step is:4.638365 \n",
    "Average loss at 99000 step is:4.677196 \n",
    "Average loss at 100000 step is:4.664153 \n",
    "valid_word is: state\n",
    "        Nearest to state is: stadium compost ramps kapoor states meridians michelob interludes\n",
    "valid_word is: states\n",
    "        Nearest to states is: scholarships state michelob stabbed expansion acres imran calif\n",
    "valid_word is: it\n",
    "        Nearest to it is: he this there she they which akita callithrix\n",
    "valid_word is: up\n",
    "        Nearest to up is: trumps out tours them him selma lignite kanem\n",
    "valid_word is: between\n",
    "        Nearest to between is: with in holographic mourned deprived original hometown davids\n",
    "valid_word is: with\n",
    "        Nearest to with is: in tamias between or including and fets markov\n",
    "valid_word is: use\n",
    "        Nearest to use is: agouti dasyprocta cebus fsm abitibi thaler microcebus abet\n",
    "valid_word is: his\n",
    "        Nearest to his is: their her its the s megabats prism recordings\n",
    "valid_word is: two\n",
    "        Nearest to two is: three four six five seven one eight microcebus\n",
    "valid_word is: three\n",
    "        Nearest to three is: five four two six seven eight callithrix microcebus\n",
    "valid_word is: but\n",
    "        Nearest to but is: however and although agouti though dasyprocta meridians or\n",
    "valid_word is: more\n",
    "        Nearest to more is: most less very widening quarried microcebus michelob plaintext\n",
    "valid_word is: the\n",
    "        Nearest to the is: its their akita microbats his dasyprocta this agouti\n",
    "valid_word is: in\n",
    "        Nearest to in is: during at kapoor from within on thaler thibetanus\n",
    "valid_word is: united\n",
    "        Nearest to united is: of perimeter roman pedro abitibi antiparticle clement plum\n",
    "valid_word is: during\n",
    "        Nearest to during is: in at after clergyman from following thibetanus within\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import vocabulary\n",
    "import Word2Vec_skip\n",
    "\n",
    "# 初始化TSNE类\n",
    "# 构造函数原型__init__(self,n_components,perplexity,early_exaggeration,learning_rate,n_iter,\n",
    "#                      n_iter_without_progress,min_grad_norm,metric,init,verbose,random_state,\n",
    "#                       method,angle)\n",
    "tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "plot_only = 100\n",
    "\n",
    "# 执行降维操作\n",
    "# 函数原型TSNE.fit_transform(Self,X,y)\n",
    "low_dim_embs = tsne.fit_transform(Word2Vec_skip.final_embeddings[:plot_only, :])\n",
    "\n",
    "labels = list()\n",
    "for i in range(plot_only):\n",
    "    labels.append(vocabulary.reverse_dictionary[i])\n",
    "\n",
    "# pyplot的figure()函数用于定义画布的大小，这里设为20x20，你也可以尝试其他大小\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for j, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[j, :]\n",
    "\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\",\n",
    "                 ha=\"right\", va=\"bottom\")\n",
    "\n",
    "# 以png格式保存图片\n",
    "plt.savefig(filename=\"after_tsne.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
