{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from config import resnet_config\n",
    "from data_loader import DataLoader\n",
    "from eval.evaluate import accuracy\n",
    "\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self,\n",
    "                 depth=resnet_config.depth,\n",
    "                 height=config.height,\n",
    "                 width=config.width,\n",
    "                 channel=config.channel,\n",
    "                 num_classes=config.num_classes,\n",
    "                 learning_rate=resnet_config.learning_rate,\n",
    "                 learning_decay_rate=resnet_config.learning_decay_rate,\n",
    "                 learning_decay_steps=resnet_config.learning_decay_steps,\n",
    "                 epoch=resnet_config.epoch,\n",
    "                 batch_size=resnet_config.batch_size,\n",
    "                 model_path=resnet_config.model_path,\n",
    "                 summary_path=resnet_config.summary_path):\n",
    "        \"\"\"\n",
    "\n",
    "        :param depth:\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_decay_rate = learning_decay_rate\n",
    "        self.learning_decay_steps = learning_decay_steps\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.model_path = model_path\n",
    "        self.summary_path = summary_path\n",
    "        self.num_block_dict = {18: [2, 2, 2, 2],\n",
    "                               34: [3, 4, 6, 3],\n",
    "                               50: [3, 4, 6, 3],\n",
    "                               101: [3, 4, 23, 3]}\n",
    "        self.bottleneck_dict = {18: False,\n",
    "                                34: False,\n",
    "                                50: True,\n",
    "                                101: True}\n",
    "        self.filter_out = [64, 128, 256, 512]\n",
    "        self.filter_out_last_layer = [256, 512, 1024, 2048]\n",
    "        self.conv_out_depth = self.filter_out[-1] if self.depth < 50 else self.filter_out_last_layer[-1]\n",
    "        assert self.depth in self.num_block_dict, 'depth should be in [18,34,50,101]'\n",
    "        self.num_block = self.num_block_dict[self.depth]\n",
    "        self.bottleneck = self.bottleneck_dict[self.depth]\n",
    "        self.input_x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='input_y')\n",
    "        self.prediction = None\n",
    "        self.loss = None\n",
    "        self.acc = None\n",
    "        self.global_step = None\n",
    "        self.data_loader = DataLoader()\n",
    "        self.model()\n",
    "\n",
    "    def model(self):\n",
    "        # first convolution layers\n",
    "        x = self.conv(x=self.input_x, k_size=7, filters_out=64, strides=2, activation=True, name='First_Conv')\n",
    "        x = tf.layers.max_pooling2d(x, pool_size=[3, 3], strides=2, padding='same', name='max_pool')\n",
    "        x = self.stack_block(x)\n",
    "        x = tf.layers.average_pooling2d(x, pool_size=x.get_shape()[1:3], strides=1, name='average_pool')\n",
    "        x = tf.reshape(x, [-1, 1 * 1 * self.conv_out_depth])\n",
    "        fc_W = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        logits = tf.layers.dense(inputs=x, units=self.num_classes,kernel_initializer=fc_W)\n",
    "\n",
    "        # 预测值\n",
    "        self.prediction = tf.argmax(logits,axis=-1)\n",
    "        # 计算准确率\n",
    "        self.acc = accuracy(logits, self.input_y)\n",
    "        # 损失值\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y))\n",
    "        # 全局步数\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        # 递减学习率\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=self.learning_rate,\n",
    "                                                   global_step=self.global_step,\n",
    "                                                   decay_rate=self.learning_decay_rate,\n",
    "                                                   decay_steps=self.learning_decay_steps,\n",
    "                                                   staircase=True)\n",
    "        self.optimize = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "    def stack_block(self, input_x):\n",
    "        for stack in range(4):\n",
    "            stack_strides = 1 if stack == 0 else 2\n",
    "            stack_name = 'stack_%s' % stack\n",
    "            with tf.name_scope(stack_name):\n",
    "                for block in range(self.num_block[stack]):\n",
    "                    shortcut = input_x\n",
    "                    block_strides = stack_strides if block == 0 else 1\n",
    "                    block_name = stack_name + '_block_%s' % block\n",
    "                    with tf.name_scope(block_name):\n",
    "                        if self.bottleneck:\n",
    "                            for layer in range(3):\n",
    "                                with tf.name_scope(block_name + '_layer_%s' % layer):\n",
    "                                    filters = self.filter_out[stack] if layer < 2 else self.filter_out_last_layer[stack]\n",
    "                                    k_size = 3 if layer == 1 else 1\n",
    "                                    layer_strides = block_strides if layer < 1 else 1\n",
    "                                    activation = True if layer < 2 else False\n",
    "                                    layer_name = block_name + '_conv_%s' % layer\n",
    "                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,\n",
    "                                                        strides=layer_strides, activation=activation, name=layer_name)\n",
    "                        else:\n",
    "                            for layer in range(2):\n",
    "                                with tf.name_scope(block_name + '_layer_%s' % layer):\n",
    "                                    filters = self.filter_out[stack]\n",
    "                                    k_size = 3\n",
    "                                    layer_strides = block_strides if layer < 1 else 1\n",
    "                                    activation = True if layer < 1 else False\n",
    "                                    layer_name = block_name + '_conv_%s' % layer\n",
    "                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,\n",
    "                                                        strides=layer_strides, activation=activation, name=layer_name)\n",
    "                    shortcut_depth = shortcut.get_shape()[-1]\n",
    "                    input_x_depth = input_x.get_shape()[-1]\n",
    "                    with tf.name_scope('shortcut_connect'):\n",
    "                        if shortcut_depth != input_x_depth:\n",
    "                            connect_k_size = 1\n",
    "                            connect_strides = block_strides\n",
    "                            connect_filter = filters\n",
    "                            shortcut_name = block_name + '_shortcut'\n",
    "                            shortcut = self.conv(x=shortcut, filters_out=connect_filter, k_size=connect_k_size,\n",
    "                                                 strides=connect_strides, activation=False, name=shortcut_name)\n",
    "                        input_x = tf.nn.relu(shortcut + input_x)\n",
    "\n",
    "        return input_x\n",
    "\n",
    "    def conv(self, x, k_size, filters_out, strides, activation, name):\n",
    "        x = tf.layers.conv2d(x, filters=filters_out, kernel_size=k_size, strides=strides, padding='same', name=name)\n",
    "        x = tf.layers.batch_normalization(x, name=name + '_BN')\n",
    "        if activation:\n",
    "            x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_id_list, valid_img, valid_label):\n",
    "        \"\"\"\n",
    "        training model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 模型存储路径初始化\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        if not os.path.exists(self.summary_path):\n",
    "            os.makedirs(self.summary_path)\n",
    "\n",
    "        # train_steps初始化\n",
    "        train_steps = 0\n",
    "        best_valid_acc = 0.0\n",
    "\n",
    "        # summary初始化\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # session初始化\n",
    "        sess = tf.Session()\n",
    "        writer = tf.summary.FileWriter(self.summary_path, sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(self.epoch):\n",
    "            shuffle_id_list = random.sample(train_id_list.tolist(), len(train_id_list))\n",
    "            batch_num = int(np.ceil(len(shuffle_id_list) / self.batch_size))\n",
    "            train_id_batch = np.array_split(shuffle_id_list, batch_num)\n",
    "            for i in range(batch_num):\n",
    "                this_batch = train_id_batch[i]\n",
    "                batch_img, batch_label = self.data_loader.get_batch_data(this_batch)\n",
    "                train_steps += 1\n",
    "                feed_dict = {self.input_x: batch_img, self.input_y: batch_label}\n",
    "                _, train_loss, train_acc = sess.run([self.optimize, self.loss, self.acc], feed_dict=feed_dict)\n",
    "                if train_steps % 1 == 0:\n",
    "                    val_loss, val_acc = sess.run([self.loss, self.acc],\n",
    "                                                 feed_dict={self.input_x: valid_img, self.input_y: valid_label})\n",
    "                    msg = 'epoch:%s | steps:%s | train_loss:%.4f | val_loss:%.4f | train_acc:%.4f | val_acc:%.4f' % (\n",
    "                        epoch, train_steps, train_loss, val_loss, train_acc, val_acc)\n",
    "                    print(msg)\n",
    "                    summary = sess.run(merged, feed_dict={self.input_x: valid_img, self.input_y: valid_label})\n",
    "                    writer.add_summary(summary, global_step=train_steps)\n",
    "                    if val_acc >= best_valid_acc:\n",
    "                        best_valid_acc = val_acc\n",
    "                        saver.save(sess, save_path=self.model_path, global_step=train_steps)\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        predicting\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        prediction = sess.run(self.prediction, feed_dict={self.input_x: x})\n",
    "        return prediction\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
