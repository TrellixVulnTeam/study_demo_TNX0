{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1221 09:34:24.090019 140139828012864 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1221 09:34:24.296502 140139828012864 deprecation.py:506] From <ipython-input-1-67d8316f3589>:109: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1   [12, 112, 112, 64]\n",
      "pool2   [12, 56, 56, 128]\n",
      "pool3   [12, 28, 28, 256]\n",
      "pool4   [12, 14, 14, 512]\n",
      "pool5   [12, 7, 7, 512]\n",
      "###########################################################\n",
      "prediction (12,)\n",
      "2019-12-21 09:35:44.509545: step 0, duration = 7.190\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "batch_size = 12\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "# 定义卷积操作\n",
    "def conv_op(input, name, kernel_h, kernel_w, num_out, step_h, step_w, para):\n",
    "    # num_in是输入的深度，这个参数被用来确定过滤器的输入通道数\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(scope + \"w\", shape=[kernel_h, kernel_w, num_in, num_out],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        conv = tf.nn.conv2d(input, kernel, (1, step_h, step_w, 1), padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[num_out], dtype=tf.float32),\n",
    "                             trainable=True, name=\"b\")\n",
    "        # 计算relu后的激活值\n",
    "        activation = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        para += [kernel, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义全连操作\n",
    "def fc_op(input, name, num_out, para):\n",
    "    # num_in为输入单元的数量\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        weights = tf.get_variable(scope + \"w\", shape=[num_in, num_out], dtype=tf.float32,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_out], dtype=tf.float32), name=\"b\")\n",
    "\n",
    "        # tf.nn.relu_layer()函数会同时完成矩阵乘法以加和偏置项并计算relu激活值\n",
    "        # 这是分步编程的良好替代\n",
    "        activation = tf.nn.relu_layer(input, weights, biases)\n",
    "\n",
    "        para += [weights, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义前向传播的计算过程，input参数的大小为224x224x3，也就是输入的模拟图片数据\n",
    "def inference_op(input, keep_prob):\n",
    "    parameters = []\n",
    "\n",
    "    # 第一段卷积，输出大小为112x112x64(省略了第一个batch_size参数)\n",
    "    conv1_1 = conv_op(input, name=\"conv1_1\", kernel_h=3, kernel_w=3, num_out=4,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv1_2 = conv_op(conv1_1, name=\"conv1_2\", kernel_h=3, kernel_w=3, num_out=64,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool1\")\n",
    "    print(pool1.op.name, ' ', pool1.get_shape().as_list())\n",
    "\n",
    "    # 第二段卷积，输出大小为56x56x128(省略了第一个batch_size参数)\n",
    "    conv2_1 = conv_op(pool1, name=\"conv2_1\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv2_2 = conv_op(conv2_1, name=\"conv2_2\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool2\")\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 第三段卷积，输出大小为28x28x256(省略了第一个batch_size参数)\n",
    "    conv3_1 = conv_op(pool2, name=\"conv3_1\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_2 = conv_op(conv3_1, name=\"conv3_2\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_3 = conv_op(conv3_2, name=\"conv3_3\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool3 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool3\")\n",
    "    print(pool3.op.name, ' ', pool3.get_shape().as_list())\n",
    "\n",
    "    # 第四段卷积，输出大小为14x14x512(省略了第一个batch_size参数)\n",
    "    conv4_1 = conv_op(pool3, name=\"conv4_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_2 = conv_op(conv4_1, name=\"conv4_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_3 = conv_op(conv4_2, name=\"conv4_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool4 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool4\")\n",
    "    print(pool4.op.name, ' ', pool4.get_shape().as_list())\n",
    "\n",
    "    # 第五段卷积，输出大小为7x7x512(省略了第一个batch_size参数)\n",
    "    conv5_1 = conv_op(pool4, name=\"conv5_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_2 = conv_op(conv5_1, name=\"conv5_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_3 = conv_op(conv5_2, name=\"conv5_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool5\")\n",
    "    print(pool5.op.name, ' ', pool5.get_shape().as_list())\n",
    "\n",
    "    # pool5的结果汇总为一个向量的形式\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    flattened_shape = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshped = tf.reshape(pool5, [-1, flattened_shape], name=\"reshped\")\n",
    "\n",
    "    # 第一个全连层\n",
    "    fc_6 = fc_op(reshped, name=\"fc6\", num_out=4096, para=parameters)\n",
    "    fc_6_drop = tf.nn.dropout(fc_6, keep_prob, name=\"fc6_drop\")\n",
    "\n",
    "    # 第二个全连层\n",
    "    fc_7 = fc_op(fc_6_drop, name=\"fc7\", num_out=4096, para=parameters)\n",
    "    fc_7_drop = tf.nn.dropout(fc_7, keep_prob, name=\"fc7_drop\")\n",
    "\n",
    "    # 第三个全连层及softmax层\n",
    "    fc_8 = fc_op(fc_7_drop, name=\"fc8\", num_out=1000, para=parameters)\n",
    "    softmax = tf.nn.softmax(fc_8)\n",
    "\n",
    "    # predictions模拟了通过argmax得到预测结果\n",
    "    predictions = tf.argmax(softmax, 1,name='test')\n",
    "    print(\"###########################################################\")\n",
    "    return predictions, softmax, fc_8, parameters\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # 创建模拟的图片数据\n",
    "    image_size = 224\n",
    "    images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                          dtype=tf.float32, stddev=1e-1))\n",
    "\n",
    "    # Dropout的keep_prob会根据前向传播或者反向传播而有所不同，在前向传播时，\n",
    "    # keep_prob=1.0，在反向传播时keep_prob=0.5\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # 为当前计算图添加前向传播过程\n",
    "    predictions, softmax, fc_8, parameters = inference_op(images, keep_prob)\n",
    "    print('prediction',predictions.shape)\n",
    "    print('softmax',softmax)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 使用BFC算法确定GPU内存最佳分配策略\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = \"BFC\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        num_steps_burn_in = 10\n",
    "\n",
    "        total_dura = 0.0\n",
    "        total_dura_squared = 0.0\n",
    "\n",
    "        back_total_dura = 0.0\n",
    "        back_total_dura_squared = 0.0\n",
    "\n",
    "        # 运行前向传播的测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(predictions, feed_dict={keep_prob: 1.0})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_dura += duration\n",
    "                total_dura_squared += duration * duration\n",
    "        average_time = total_dura / num_batches\n",
    "\n",
    "        # 打印前向传播的运算时间信息\n",
    "        print(\"%s: Forward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, average_time,\n",
    "               math.sqrt(total_dura_squared / num_batches - average_time * average_time)))\n",
    "\n",
    "        # 定义求解梯度的操作\n",
    "        grad = tf.gradients(tf.nn.l2_loss(fc_8), parameters)\n",
    "\n",
    "        # 运行反向传播测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(grad, feed_dict={keep_prob: 0.5})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                back_total_dura += duration\n",
    "                back_total_dura_squared += duration * duration\n",
    "        back_avg_t = back_total_dura / num_batches\n",
    "\n",
    "        # 打印反向传播的运算时间信息\n",
    "        print(\"%s: Forward-backward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, back_avg_t,\n",
    "               math.sqrt(back_total_dura_squared / num_batches - back_avg_t * back_avg_t)))\n",
    "\n",
    "'''打印的内容\n",
    "pool1   [12, 112, 112, 64]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool4   [12, 14, 14, 512]\n",
    "pool5   [12, 7, 7, 512]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改图片尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1205 11:57:49.432171 140526890616640 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1205 11:57:49.639084 140526890616640 deprecation.py:506] From <ipython-input-1-9074be04819a>:109: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1   [12, 40, 40, 64]\n",
      "pool2   [12, 20, 20, 128]\n",
      "pool2   [12, 20, 20, 128]\n",
      "pool4   [12, 5, 5, 512]\n",
      "pool5   [12, 3, 3, 512]\n",
      "###########################################################\n",
      "2019-12-05 11:58:00.493900: step 0, duration = 0.945\n",
      "2019-12-05 11:58:09.933445: step 10, duration = 0.945\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "batch_size = 12\n",
    "num_batches = 100\n",
    "\n",
    "\n",
    "# 定义卷积操作\n",
    "def conv_op(input, name, kernel_h, kernel_w, num_out, step_h, step_w, para):\n",
    "    # num_in是输入的深度，这个参数被用来确定过滤器的输入通道数\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(scope + \"w\", shape=[kernel_h, kernel_w, num_in, num_out],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        conv = tf.nn.conv2d(input, kernel, (1, step_h, step_w, 1), padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[num_out], dtype=tf.float32),\n",
    "                             trainable=True, name=\"b\")\n",
    "        # 计算relu后的激活值\n",
    "        activation = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        para += [kernel, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义全连操作\n",
    "def fc_op(input, name, num_out, para):\n",
    "    # num_in为输入单元的数量\n",
    "    num_in = input.get_shape()[-1].value\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        weights = tf.get_variable(scope + \"w\", shape=[num_in, num_out], dtype=tf.float32,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_out], dtype=tf.float32), name=\"b\")\n",
    "\n",
    "        # tf.nn.relu_layer()函数会同时完成矩阵乘法以加和偏置项并计算relu激活值\n",
    "        # 这是分步编程的良好替代\n",
    "        activation = tf.nn.relu_layer(input, weights, biases)\n",
    "\n",
    "        para += [weights, biases]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# 定义前向传播的计算过程，input参数的大小为224x224x3，也就是输入的模拟图片数据\n",
    "def inference_op(input, keep_prob):\n",
    "    parameters = []\n",
    "\n",
    "    # 第一段卷积，输出大小为112x112x64(省略了第一个batch_size参数)\n",
    "    conv1_1 = conv_op(input, name=\"conv1_1\", kernel_h=3, kernel_w=3, num_out=4,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv1_2 = conv_op(conv1_1, name=\"conv1_2\", kernel_h=3, kernel_w=3, num_out=64,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool1\")\n",
    "    print(pool1.op.name, ' ', pool1.get_shape().as_list())\n",
    "\n",
    "    # 第二段卷积，输出大小为56x56x128(省略了第一个batch_size参数)\n",
    "    conv2_1 = conv_op(pool1, name=\"conv2_1\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv2_2 = conv_op(conv2_1, name=\"conv2_2\", kernel_h=3, kernel_w=3, num_out=128,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool2\")\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 第三段卷积，输出大小为28x28x256(省略了第一个batch_size参数)\n",
    "    conv3_1 = conv_op(pool2, name=\"conv3_1\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_2 = conv_op(conv3_1, name=\"conv3_2\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv3_3 = conv_op(conv3_2, name=\"conv3_3\", kernel_h=3, kernel_w=3, num_out=256,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool3 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool3\")\n",
    "    print(pool2.op.name, ' ', pool2.get_shape().as_list())\n",
    "\n",
    "    # 第四段卷积，输出大小为14x14x512(省略了第一个batch_size参数)\n",
    "    conv4_1 = conv_op(pool3, name=\"conv4_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_2 = conv_op(conv4_1, name=\"conv4_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv4_3 = conv_op(conv4_2, name=\"conv4_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool4 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool4\")\n",
    "    print(pool4.op.name, ' ', pool4.get_shape().as_list())\n",
    "\n",
    "    # 第五段卷积，输出大小为7x7x512(省略了第一个batch_size参数)\n",
    "    conv5_1 = conv_op(pool4, name=\"conv5_1\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_2 = conv_op(conv5_1, name=\"conv5_2\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    conv5_3 = conv_op(conv5_2, name=\"conv5_3\", kernel_h=3, kernel_w=3, num_out=512,\n",
    "                      step_h=1, step_w=1, para=parameters)\n",
    "    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding=\"SAME\", name=\"pool5\")\n",
    "    print(pool5.op.name, ' ', pool5.get_shape().as_list())\n",
    "\n",
    "    # pool5的结果汇总为一个向量的形式\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    flattened_shape = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshped = tf.reshape(pool5, [-1, flattened_shape], name=\"reshped\")\n",
    "\n",
    "    # 第一个全连层\n",
    "    fc_6 = fc_op(reshped, name=\"fc6\", num_out=4096, para=parameters)\n",
    "    fc_6_drop = tf.nn.dropout(fc_6, keep_prob, name=\"fc6_drop\")\n",
    "\n",
    "    # 第二个全连层\n",
    "    fc_7 = fc_op(fc_6_drop, name=\"fc7\", num_out=4096, para=parameters)\n",
    "    fc_7_drop = tf.nn.dropout(fc_7, keep_prob, name=\"fc7_drop\")\n",
    "\n",
    "    # 第三个全连层及softmax层\n",
    "    fc_8 = fc_op(fc_7_drop, name=\"fc8\", num_out=1000, para=parameters)\n",
    "    softmax = tf.nn.softmax(fc_8)\n",
    "\n",
    "    # predictions模拟了通过argmax得到预测结果\n",
    "    predictions = tf.argmax(softmax, 1,name='test')\n",
    "    print(\"###########################################################\")\n",
    "    return predictions, softmax, fc_8, parameters\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # 创建模拟的图片数据\n",
    "    image_size = 80\n",
    "    images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                          dtype=tf.float32, stddev=1e-1))\n",
    "\n",
    "    # Dropout的keep_prob会根据前向传播或者反向传播而有所不同，在前向传播时，\n",
    "    # keep_prob=1.0，在反向传播时keep_prob=0.5\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # 为当前计算图添加前向传播过程\n",
    "    predictions, softmax, fc_8, parameters = inference_op(images, keep_prob)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 使用BFC算法确定GPU内存最佳分配策略\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = \"BFC\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        num_steps_burn_in = 10\n",
    "\n",
    "        total_dura = 0.0\n",
    "        total_dura_squared = 0.0\n",
    "\n",
    "        back_total_dura = 0.0\n",
    "        back_total_dura_squared = 0.0\n",
    "\n",
    "        # 运行前向传播的测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(predictions, feed_dict={keep_prob: 1.0})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_dura += duration\n",
    "                total_dura_squared += duration * duration\n",
    "        average_time = total_dura / num_batches\n",
    "\n",
    "        # 打印前向传播的运算时间信息\n",
    "        print(\"%s: Forward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, average_time,\n",
    "               math.sqrt(total_dura_squared / num_batches - average_time * average_time)))\n",
    "\n",
    "        # 定义求解梯度的操作\n",
    "        grad = tf.gradients(tf.nn.l2_loss(fc_8), parameters)\n",
    "\n",
    "        # 运行反向传播测试过程\n",
    "        for i in range(num_batches + num_steps_burn_in):\n",
    "            start_time = time.time()\n",
    "            _ = sess.run(grad, feed_dict={keep_prob: 0.5})\n",
    "            duration = time.time() - start_time\n",
    "            if i >= num_steps_burn_in:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"%s: step %d, duration = %.3f\" %\n",
    "                          (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                back_total_dura += duration\n",
    "                back_total_dura_squared += duration * duration\n",
    "        back_avg_t = back_total_dura / num_batches\n",
    "\n",
    "        # 打印反向传播的运算时间信息\n",
    "        print(\"%s: Forward-backward across %d steps, %.3f +/- %.3f sec / batch\" %\n",
    "              (datetime.now(), num_batches, back_avg_t,\n",
    "               math.sqrt(back_total_dura_squared / num_batches - back_avg_t * back_avg_t)))\n",
    "\n",
    "'''打印的内容\n",
    "pool1   [12, 112, 112, 64]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool2   [12, 56, 56, 128]\n",
    "pool4   [12, 14, 14, 512]\n",
    "pool5   [12, 7, 7, 512]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
