{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist数据集的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "训练数据的大小 (55000, 784)\n",
      "训练数据标签的大小 (55000, 10)\n",
      "验证数据的大小 (5000, 784)\n",
      "验证数据标签的大小 (5000, 10)\n",
      "测试数据的大小 (10000, 784)\n",
      "测试标签数据的大小 (10000, 10)\n",
      "打印第一张照片 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "打印第一张照片的标签 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 从tensorflow.examples.tutorials.mnist引入模块。这是TensorFlow为了教学MNIST而提前编制的程序\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# 从MNIST_data/中读取MNIST数据。这条语句在数据不存在时，会自动执行下载\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 查看训练数据的大小\n",
    "print('训练数据的大小',mnist.train.images.shape)  # (55000, 784)\n",
    "print('训练数据标签的大小',mnist.train.labels.shape)  # (55000, 10)\n",
    "\n",
    "# 查看验证数据的大小\n",
    "print('验证数据的大小',mnist.validation.images.shape)  # (5000, 784)\n",
    "print('验证数据标签的大小',mnist.validation.labels.shape)  # (5000, 10)\n",
    "\n",
    "# 查看测试数据的大小\n",
    "print('测试数据的大小',mnist.test.images.shape)  # (10000, 784)\n",
    "print('测试标签数据的大小',mnist.test.labels.shape)  # (10000, 10)\n",
    "\n",
    "# 打印出第0幅图片的向量表示\n",
    "print('打印第一张照片',mnist.train.images[0, :])\n",
    "\n",
    "# 打印出第0幅图片的标签\n",
    "print('打印第一张照片的标签',mnist.train.labels[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存前20张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Please check: MNIST_data/raw/ \n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import scipy.misc\n",
    "import os\n",
    "\n",
    "# 读取MNIST数据集。如果不存在会事先下载。\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 我们把原始图片保存在MNIST_data/raw/文件夹下\n",
    "# 如果没有这个文件夹会自动创建\n",
    "save_dir = 'MNIST_data/raw/'\n",
    "if os.path.exists(save_dir) is False:\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 保存前20张图片\n",
    "for i in range(20):\n",
    "    # 请注意，mnist.train.images[i, :]就表示第i张图片（序号从0开始）\n",
    "    image_array = mnist.train.images[i, :]\n",
    "    # TensorFlow中的MNIST图片是一个784维的向量，我们重新把它还原为28x28维的图像。\n",
    "    image_array = image_array.reshape(28, 28)\n",
    "    # 保存文件的格式为 mnist_train_0.jpg, mnist_train_1.jpg, ... ,mnist_train_19.jpg\n",
    "    filename = save_dir + 'mnist_train_%d.jpg' % i\n",
    "    # 将image_array保存为图片\n",
    "    # 先用scipy.misc.toimage转换为图像，再调用save直接保存。\n",
    "    \n",
    "    #scipy.misc.toimage(image_array, cmin=0.0, cmax=1.0).save(filename)\n",
    "\n",
    "print('Please check: %s ' % save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的softmax分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "0.9128\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 导入tensorflow。\n",
    "# 这句import tensorflow as tf是导入TensorFlow约定俗成的做法，请大家记住。\n",
    "import tensorflow as tf\n",
    "# 导入MNIST教学的模块\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# 与之前一样，读入MNIST数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 创建x，x是一个占位符（placeholder），代表待识别的图片\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# W是Softmax模型的参数，将一个784维的输入转换为一个10维的输出\n",
    "# 在TensorFlow中，变量的参数用tf.Variable表示\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# y=softmax(Wx + b)，y表示模型的输出\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# y_是实际的图像标签，同样以占位符表示。\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 至此，我们得到了两个重要的Tensor：y和y_。\n",
    "# y是模型的输出，y_是实际的图像标签，不要忘了y_是独热表示的\n",
    "# 下面我们就会根据y和y_构造损失\n",
    "\n",
    "# 根据y, y_构造交叉熵损失\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))\n",
    "\n",
    "# 有了损失，我们就可以用随机梯度下降针对模型的参数（W和b）进行优化\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# 创建一个Session。只有在Session中才能运行优化步骤train_step。\n",
    "sess = tf.InteractiveSession()\n",
    "# 运行之前必须要初始化所有变量，分配内存。\n",
    "tf.global_variables_initializer().run()\n",
    "print('start training...')\n",
    "\n",
    "# 进行1000步梯度下降\n",
    "for _ in range(1000):\n",
    "    # 在mnist.train中取100个训练数据\n",
    "    # batch_xs是形状为(100, 784)的图像数据，batch_ys是形如(100, 10)的实际标签\n",
    "    # batch_xs, batch_ys对应着两个占位符x和y_\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # 在Session中运行train_step，运行时要传入占位符的值\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# 正确的预测结果\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# 计算预测准确率，它们都是Tensor\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# 在Session中运行Tensor可以得到Tensor的值\n",
    "# 这里是获取最终模型的正确率\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))  # 0.9185\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1004 22:31:00.366731 140168492443456 deprecation.py:323] From <ipython-input-1-132123ad7681>:27: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1004 22:31:00.369883 140168492443456 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1004 22:31:00.372128 140168492443456 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1004 22:31:00.715982 140168492443456 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1004 22:31:00.737642 140168492443456 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1004 22:31:00.845816 140168492443456 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1004 22:31:01.333488 140168492443456 deprecation.py:506] From <ipython-input-1-132123ad7681>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1004 22:31:01.363921 140168492443456 deprecation.py:323] From <ipython-input-1-132123ad7681>:63: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.86\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 读入数据\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    # x为训练图像的占位符、y_为训练图像标签的占位符\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # 将单张图片从784维向量重新还原为28x28的矩阵图片\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # 第一层卷积层\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # 第二层卷积层\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # 全连接层，输出为1024维的向量\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    # 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # 把1024维的向量转换成10维，对应10个类别，对应的生成一个10维的数据\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    # 同样定义train_step\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # 定义测试的准确率\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 创建Session和变量初始化\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 训练20000步\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        # 每100步报告一次在验证集上的准确度\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})   \n",
    "       # train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    # 训练结束后报告在测试集上的准确度\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对交叉熵的理解\n",
    "\n",
    "熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 读入数据\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    # x为训练图像的占位符、y_为训练图像标签的占位符\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # 将单张图片从784维向量重新还原为28x28的矩阵图片\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # 第一层卷积层\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # 第二层卷积层\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # 全连接层，输出为1024维的向量\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    # 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # 把1024维的向量转换成10维，对应10个类别，对应的生成一个10维的数据\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    # 同样定义train_step\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # 定义测试的准确率\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 创建Session和变量初始化\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "            #sess = tf.InteractiveSession()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(20000):\n",
    "                batch = mnist.train.next_batch(50)\n",
    "#                 if i % 100 == 0:\n",
    "#                     train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "#                     print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "                sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})   \n",
    "                \n",
    "        \n",
    "\n",
    "    # 训练20000步\n",
    "#     for i in range(20000):\n",
    "#         batch = mnist.train.next_batch(50)\n",
    "#         # 每100步报告一次在验证集上的准确度\n",
    "#         if i % 100 == 0:\n",
    "#             train_accuracy = accuracy.eval(feed_dict={\n",
    "#                 x: batch[0], y_: batch[1], keep_prob: 1.0}) \n",
    "#             print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "#         sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})   \n",
    "       # train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "#     # 训练结束后报告在测试集上的准确度\n",
    "#     print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "#         x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 11:11:23.389134 139800070674240 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 11:11:23.988765 139800070674240 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1010 11:11:24.017867 139800070674240 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1010 11:11:24.139470 139800070674240 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 11:11:24.888353 139800070674240 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 trainging step(s) ,validation accuracyusing average model is 12.02%\n",
      "After 1000 trainging step(s) ,validation accuracyusing average model is 97.86%\n",
      "After 2000 trainging step(s) ,validation accuracyusing average model is 98.28%\n",
      "After 3000 trainging step(s) ,validation accuracyusing average model is 98.4%\n",
      "After 4000 trainging step(s) ,validation accuracyusing average model is 98.5%\n",
      "After 5000 trainging step(s) ,validation accuracyusing average model is 98.44%\n",
      "After 6000 trainging step(s) ,validation accuracyusing average model is 98.42%\n",
      "After 7000 trainging step(s) ,validation accuracyusing average model is 98.48%\n",
      "After 8000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 9000 trainging step(s) ,validation accuracyusing average model is 98.54%\n",
      "After 10000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 11000 trainging step(s) ,validation accuracyusing average model is 98.44%\n",
      "After 12000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 13000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 14000 trainging step(s) ,validation accuracyusing average model is 98.6%\n",
      "After 15000 trainging step(s) ,validation accuracyusing average model is 98.56%\n",
      "After 16000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 17000 trainging step(s) ,validation accuracyusing average model is 98.58%\n",
      "After 18000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 19000 trainging step(s) ,validation accuracyusing average model is 98.48%\n",
      "After 20000 trainging step(s) ,validation accuracyusing average model is 98.68%\n",
      "After 21000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 22000 trainging step(s) ,validation accuracyusing average model is 98.6%\n",
      "After 23000 trainging step(s) ,validation accuracyusing average model is 98.62%\n",
      "After 24000 trainging step(s) ,validation accuracyusing average model is 98.52%\n",
      "After 25000 trainging step(s) ,validation accuracyusing average model is 98.62%\n",
      "After 26000 trainging step(s) ,validation accuracyusing average model is 98.66%\n",
      "After 27000 trainging step(s) ,validation accuracyusing average model is 98.46%\n",
      "After 28000 trainging step(s) ,validation accuracyusing average model is 98.58%\n",
      "After 29000 trainging step(s) ,validation accuracyusing average model is 98.56%\n",
      "After 30000 trainging step(s) ,test accuracy using average model is 98.42%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "batch_size = 100                #设置每一轮训练的batch大小\n",
    "learning_rate = 0.8             #学习率\n",
    "learning_rate_decay = 0.999     #学习率的衰减\n",
    "max_steps = 30000               #最大训练步数\n",
    "\n",
    "#定义存储训练轮数的变量，在使用Tensorflow训练神经网络时，\n",
    "#一般会将代表训练轮数的变量通过trainable参数设置为不可训练的\n",
    "training_step = tf.Variable(0,trainable=False)\n",
    "\n",
    "#定义得到隐藏层和输出层的前向传播计算方式，激活函数使用relu()\n",
    "def hidden_layer(input_tensor,weights1,biases1,weights2,biases2,layer_name):\n",
    "    layer1=tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "    return tf.matmul(layer1,weights2)+biases2\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,784],name=\"x-input\")   #INPUT_NODE=784\n",
    "y_ = tf.placeholder(tf.float32,[None,10],name=\"y-output\")   #OUT_PUT=10\n",
    "#生成隐藏层参数，其中weights包含784x500=392000个参数\n",
    "weights1=tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\n",
    "biases1 = tf.Variable(tf.constant(0.1,shape=[500]))\n",
    "#生成输出层参数，其中weights2包含500x10=5000个参数\n",
    "weights2 = tf.Variable(tf.truncated_normal([500, 10], stddev=0.1))\n",
    "biases2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "#计算经过神经网络前向传播后得到的y的值，这里没有使用滑动平均\n",
    "y = hidden_layer(x,weights1,biases1,weights2,biases2,'y')\n",
    "\n",
    "\n",
    "#初始化一个滑动平均类，衰减率为0.99\n",
    "#为了使模型在训练前期可以更新地更快，这里提供了num_updates参数\n",
    "#并设置为当前网络的训练轮数\n",
    "averages_class = tf.train.ExponentialMovingAverage(0.99,training_step)\n",
    "#定义一个更新变量滑动平均值的操作需要向滑动平均类的apply()函数提供一个参数列表\n",
    "#train_variables()函数返回集合图上Graph.TRAINABLE_VARIABLES中的元素，\n",
    "#这个集合的元素就是所有没有指定trainable_variables=False的参数\n",
    "averages_op = averages_class.apply(tf.trainable_variables())\n",
    "#再次计算经过神经网络前向传播后得到的y的值，这里使用了滑动平均，但要牢记滑动平均值只是一个影子变量\n",
    "average_y = hidden_layer(x,averages_class.average(weights1),averages_class.average(biases1),\n",
    "                averages_class.average(weights2),averages_class.average(biases2),'average_y')\n",
    "\n",
    "#计算交叉熵损失的函数原型为sparse_softmax_cross_entropy_with_logits(_sential, labels, logdits, name)\n",
    "#它与softmax_cross_entropy_with_logits()函数的计算方式相同，适用于每个类别相互独立且排斥的情况，\n",
    "#即一幅图只能属于一类。在1.0.0版本的TensorFlow中，这个函数只能通过命名参数的方式来使用，在这里\n",
    "#logits参数是神经网络不包括softmax层的前向传播结果，lables参数给出了训练数据的正确答案\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_, 1))\n",
    "#argmax()函数原型为argmax(input, axis, name, dimension),用于计算每一个样例的预测答案，\n",
    "#其中input参数y是一个batch_size * 10(batch_size行，10列)的二维数组，每一行表示一个样例前向传播的结果，\n",
    "#axis参数“1”表示选取最大值的操作仅在第一个维度中进行，即只在每一行选取最大值对应的下标。\n",
    "#于是得到的结果是一个长度为batch_size的一维数组，这个一维数组中的值就表示了每一个样例对应的\n",
    "#数字识别结果。\n",
    "\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)       #计算L2正则化损失函数\n",
    "regularization = regularizer(weights1)+regularizer(weights2) #计算模型的正则化损失\n",
    "loss = tf.reduce_mean(cross_entropy)+regularization          #总损失\n",
    "\n",
    "\n",
    "#用指数衰减法设置学习率，这里staircase参数采用默认的False，即学习率连续衰减\n",
    "laerning_rate = tf.train.exponential_decay(learning_rate,training_step,mnist.train.num_examples/batch_size,\n",
    "                                                                                       learning_rate_decay)\n",
    "#使用GradientDescentOptimizer优化算法来优化交叉熵损失和正则化损失\n",
    "train_step= tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=training_step)\n",
    "\n",
    "\n",
    "#在训练这个模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数又需要\n",
    "#更新每一个参数的滑动平均值，control_dependencies()用于完成这样的一次性多次操作，\n",
    "# 同样的操作也可以使用下面这行代码完成：\n",
    "# train_op = tf.group(train_step,averages_op)\n",
    "with tf.control_dependencies([train_step,averages_op]):\n",
    "     train_op = tf.no_op(name=\"train\")\n",
    "\n",
    "\n",
    "#检查使用了滑动平均值模型的神经网络前向传播结果是否正确。\n",
    "#equal()函数原型为equal(x, y, name)，用于判断两个张量的每一维是否相等，如果相等返回True,否则返回False。\n",
    "crorent_predicition = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "\n",
    "#cast()函数原型为cast(x, DstT, name)。在这里用于将一个bool型的数据转为float32类型\n",
    "#之后会将得到的float32 的数据求一个平均值，这个平均值就是模型在这一组数据上的正确率\n",
    "accuracy = tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #在稍早的版本中一般使用 initialize_all_variables()函数初始化全部变量\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    #准备验证数据，\n",
    "    validate_feed = {x:mnist.validation.images,y_:mnist.validation.labels}\n",
    "    #准备测试数据，\n",
    "    test_feed = {x:mnist.test.images,y_:mnist.test.labels}\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        if i%1000==0:\n",
    "            #计算滑动平均模型在验证数据上的结果。\n",
    "            # 为了能得到百分数输出，需要将得到的validate_accuracy扩大100倍\n",
    "            validate_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "            print(\"After %d trainging step(s) ,validation accuracy\"\n",
    "                  \"using average model is %g%%\"%(i,validate_accuracy*100))\n",
    "\n",
    "        #产生这一轮使用的一个batch的训练数据，并进行训练\n",
    "        #input_data.read_data_sets()函数生成的类提供了train.next_bacth()函数，\n",
    "        #通过设置函数的batch_size参数就可以从所有的训练数据中提读取一小部分作为一个训练batch\n",
    "        xs,ys = mnist.train.next_batch(batch_size=100)\n",
    "        sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "\n",
    "    #使用测试数据集检验神经网络训练之后的最终正确率\n",
    "    # 为了能得到百分数输出，需要将得到的test_accuracy扩大100倍\n",
    "    test_accuracy = sess.run(accuracy,feed_dict=test_feed)\n",
    "    print(\"After %d trainging step(s) ,test accuracy using average\"\n",
    "                  \" model is %g%%\"%(max_steps,test_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv2d的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M after convolution: \n",
      " [[[[  2.]\n",
      "   [  9.]\n",
      "   [  1.]\n",
      "   [  2.]]\n",
      "\n",
      "  [[  2.]\n",
      "   [ 15.]\n",
      "   [  0.]\n",
      "   [  9.]]\n",
      "\n",
      "  [[ -1.]\n",
      "   [ -7.]\n",
      "   [ 16.]\n",
      "   [  5.]]\n",
      "\n",
      "  [[  3.]\n",
      "   [-11.]\n",
      "   [ 20.]\n",
      "   [ -3.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#使用numpy工具初始化一个名为M的数组，形状为2x3，数据类型为float32\n",
    "#并使用numpy的reshape()函数调整输入的格式\n",
    "#注意，M不会被TensorFlow识别为张量\n",
    "M = np.array([[[2],[1],[2],[-1]],[[0],[-1],[3],[0]],\n",
    "              [[2],[1],[-1],[4]],[[-2],[0],[-3],[4]]],dtype=\"float32\").reshape(1, 4, 4, 1)\n",
    "\n",
    "#通过get_variable()函数创建过滤器的权重变量，上面介绍了卷积层\n",
    "#这里声明的参数变量是一个四维矩阵，前面两个维度代表了过滤器的尺寸，\n",
    "#第三个维度表示当前层的深度，第四个维度表示过滤器的深度。\n",
    "filter_weight = tf.get_variable(\"weights\",[2, 2, 1, 1],\n",
    "    initializer = tf.constant_initializer([[-1, 4],[2, 1]]))\n",
    "\n",
    "#通过get_variable()函数创建过滤器的偏置项，代码中[1]表示过滤器的深度。\n",
    "#等于神经网络下一层的深度。\n",
    "biases = tf.get_variable(\"biase\", [1], initializer = tf.constant_initializer(1))\n",
    "\n",
    "\n",
    "x = tf.placeholder('float32', [1,None, None,1])\n",
    "\n",
    "#conv2d()函数实现了卷积层前向传播的算法。\n",
    "#这个函数的第一个参数为当前层的输入矩阵，注意这个矩阵应该是一个四维矩阵，\n",
    "#代表第一个维度的参数对应一个输入batch。如果在输入层，input[0, , , ]表示第一张图片，\n",
    "#input[1, , , ]表示第二张图片，等等。函数第二个参数是卷积层的权重，第三个参数为\n",
    "#不同维度上的步长。虽然第三个参数提供的是一个长度为4 的数组，\n",
    "#但是第一个和最后一个数字要求一定是1，这是因为卷积层的步长只对矩阵的长和宽有效。\n",
    "#最后一个参数是填充(padding的方法，有SAME或VALID 两种选择，\n",
    "#其中SAME 表示添加全0填充，VALID表示不添加。\n",
    "#函数原型conv2d(input,filter,strids,padding,us_cudnn_on_gpu_,data_format,name)\n",
    "conv = tf.nn.conv2d(x, filter_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "#bias_add()函数具有给每一个节点加上偏置项点功能。这里不能直接使用加法的原因是\n",
    "#矩阵上不同位置上的节点都需要加上同样的偏置项。因为过滤器深度为1，\n",
    "#故偏置项只有一个数，结果为3x4的矩阵中每一个值都要加上这个偏置项。\n",
    "#原型bias_add(value,bias,data_format,name)\n",
    "add_bias = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "init_op=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init_op.run()\n",
    "    M_conv=sess.run(add_bias, feed_dict={x: M})\n",
    "\n",
    "    #输出结果并不是一个张量，而是数组\n",
    "    print(\"M after convolution: \\n\", M_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pool的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "M = np.array([[[-2],[2],[0],[3]],\n",
    "              [[1],[2],[-1],[2]],\n",
    "              [[0],[-1],[1],[0]]],dtype=\"float32\").reshape(1, 3, 4, 1)\n",
    "filter_weight = tf.get_variable(\"weights\",[2, 2, 1, 1],\n",
    "    initializer = tf.constant_initializer([[2, 0],[-1, 1]]))\n",
    "biases = tf.get_variable('biases', [1], initializer = tf.constant_initializer(1))\n",
    "x = tf.placeholder('float32', [1, None, None, 1])\n",
    "conv = tf.nn.conv2d(x, filter_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "add_bias = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "#max_pool()函数实现了最大池化层的前向传播过程\n",
    "#原型为max_pool(value,strides,padding,data_format,name)\n",
    "#参数value为输入数据，strides为提供了步长信息，padding提供了是否使用全0填充。\n",
    "pool = tf.nn.max_pool(add_bias, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    M_conv = sess.run(add_bias, feed_dict={x: M})\n",
    "    M_pool = sess.run(pool, feed_dict={x: M})\n",
    "    print(\" after average pooled: \\n\", M_pool)\n",
    "    '''输出内容\n",
    "    after average pooled:\n",
    "    [[[[7.]\n",
    "       [5.]]\n",
    "      [[1.]\n",
    "       [3.]]]]\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片显示的方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "image = tf.gfile.FastGFile(\"/home/jiangziyang/images/duck.png\", 'r').read()\n",
    "with tf.Session() as sess:\n",
    "    img_after_decode = tf.image.decode_png(image)\n",
    "\n",
    "    # 函数原型central_crop(image,central_fraction)\n",
    "    central_cropped = tf.image.central_crop(img_after_decode, 0.4)\n",
    "    plt.imshow(central_cropped.eval())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 09:45:44.237374 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#声明两个变量并计算其加和\n",
    "a = tf.Variable(tf.constant([1.0,2.0],shape=[2]), name=\"a\")\n",
    "b = tf.Variable(tf.constant([3.0,4.0],shape=[2]), name=\"b\")\n",
    "result=a+b\n",
    "\n",
    "#定义初始化全部变量的操作\n",
    "init_op=tf.initialize_all_variables()\n",
    "#定义Saver类对象用于保存模型\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    # 模型保存到/home/jiangziyang/model路径下的model.ckpt文件，其中model是模型的名称\n",
    "    saver.save(sess,\"src/model/model.ckpt\")\n",
    "    # save函数的原型是\n",
    "    # save(self,ses,save_path,global_step,latest_filename,meta_graph_suffix,\n",
    "    #                                            write_meta_graph, write_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist_train的训练模型保存与读取\n",
    "### 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 09:57:24.432219 139957896574784 deprecation.py:323] From <ipython-input-3-1d9772b3ccec>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1030 09:57:24.434207 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1030 09:57:24.437143 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 09:57:24.760637 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1030 09:57:24.778739 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1030 09:57:24.923892 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 09:57:25.286395 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 2.83791.\n",
      "After 1001 training step(s), loss on training batch is 0.264814.\n",
      "After 2001 training step(s), loss on training batch is 0.163542.\n",
      "After 3001 training step(s), loss on training batch is 0.143543.\n",
      "After 4001 training step(s), loss on training batch is 0.11728.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 09:58:01.813363 139957896574784 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5001 training step(s), loss on training batch is 0.103275.\n",
      "After 6001 training step(s), loss on training batch is 0.0902812.\n",
      "After 7001 training step(s), loss on training batch is 0.0841291.\n",
      "After 8001 training step(s), loss on training batch is 0.0823245.\n",
      "After 9001 training step(s), loss on training batch is 0.070709.\n",
      "After 10001 training step(s), loss on training batch is 0.068285.\n",
      "After 11001 training step(s), loss on training batch is 0.0642364.\n",
      "After 12001 training step(s), loss on training batch is 0.0639735.\n",
      "After 13001 training step(s), loss on training batch is 0.0495205.\n",
      "After 14001 training step(s), loss on training batch is 0.051955.\n",
      "After 15001 training step(s), loss on training batch is 0.0476992.\n",
      "After 16001 training step(s), loss on training batch is 0.0423945.\n",
      "After 17001 training step(s), loss on training batch is 0.0389303.\n",
      "After 18001 training step(s), loss on training batch is 0.0531002.\n",
      "After 19001 training step(s), loss on training batch is 0.0395308.\n",
      "After 20001 training step(s), loss on training batch is 0.0410099.\n",
      "After 21001 training step(s), loss on training batch is 0.0358681.\n",
      "After 22001 training step(s), loss on training batch is 0.0387835.\n",
      "After 23001 training step(s), loss on training batch is 0.0415751.\n",
      "After 24001 training step(s), loss on training batch is 0.0373346.\n",
      "After 25001 training step(s), loss on training batch is 0.0325923.\n",
      "After 26001 training step(s), loss on training batch is 0.0338722.\n",
      "After 27001 training step(s), loss on training batch is 0.0340049.\n",
      "After 28001 training step(s), loss on training batch is 0.0403614.\n",
      "After 29001 training step(s), loss on training batch is 0.0307683.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "batch_size = 100\n",
    "learning_rate = 0.8\n",
    "learning_rate_decay = 0.999\n",
    "max_steps = 30000\n",
    "\n",
    "#更改前向传播算法的定义,将得到权重参数和偏执参数的过程封装到了一个函数中\n",
    "def hidden_layer(input_tensor,regularizer,name):\n",
    "#要多多体会使用变量空间来管理变量的方便性。使用get_variable()函数会在训练神经\n",
    "#网络时创建这些变量而在测试过程中通过保存的模型加载这些变量的取值，在测试过程中\n",
    "#可以在加载变量时将滑动平均变量重命名，这样就会在测试过程中使用变量的滑动平均值\n",
    "    with tf.variable_scope(\"hidden_layer\"):\n",
    "        weights = tf.get_variable(\"weights\", [784, 500],\n",
    "                         initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        #如果调用该函数时传入了正则化的方法 ，那么在这里将参数求\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection(\"losses\",regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [500], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_output\"):\n",
    "        weights = tf.get_variable(\"weights\", [500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection(\"losses\",regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer_output = tf.matmul(hidden_layer, weights) + biases\n",
    "    return hidden_layer_output\n",
    "\n",
    "#定义输出输出的部分没变\n",
    "x = tf.placeholder(tf.float32, [None,784],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None,10],name=\"y-output\")\n",
    "\n",
    "#定义L2正则化的办法被提前\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "#将L2正则化的办法传入到hidden_layer()函数中\n",
    "y = hidden_layer(x,regularizer,name=\"y\")\n",
    "\n",
    "training_step = tf.Variable(0,trainable=False)\n",
    "averages_class = tf.train.ExponentialMovingAverage(0.99,training_step)\n",
    "averages_op = averages_class.apply(tf.trainable_variables())\n",
    "\n",
    "#不再定义average_y，因为average_y只在比较正确率时有用，\n",
    "#在模型保存的程序中我们只输出损失\n",
    "#average_y = hidden_layer(x,averages_class,name=\"average_y\",reuse=True)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                 labels=tf.argmax(y_, 1)) #tf.argmax()\n",
    "#计算总损失\n",
    "loss = tf.reduce_mean(cross_entropy)+tf.add_n(tf.get_collection(\"losses\"))\n",
    "\n",
    "laerning_rate = tf.train.exponential_decay(learning_rate,training_step,\n",
    "                mnist.train.num_examples/batch_size,learning_rate_decay)\n",
    "train_step= tf.train.GradientDescentOptimizer(learning_rate).\\\n",
    "                                minimize(loss,global_step=training_step)\n",
    "\n",
    "#也可以采用train_op = tf.group(train_step,averages_op)的形式\n",
    "with tf.control_dependencies([train_step, averages_op]):\n",
    "    train_op = tf.no_op(name=\"train\")\n",
    "\n",
    "#初始化Saver持久化类\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    #进行30000轮到训练\n",
    "    for i in range(max_steps):\n",
    "        x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "        _, loss_value, step = sess.run([train_op, loss, training_step],\n",
    "                                          feed_dict={x: x_train, y_: y_train})\n",
    "\n",
    "        #每隔1000轮训练就输出当前训练batch上的损失函数大小，并保存一次模型\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), loss on training batch is \"\n",
    "                                                    \"%g.\" % (step, loss_value))\n",
    "            #保存模型的时候给出了global_step参数，这样可以让每个模型文件都添加\n",
    "            #代表了训练轮数的后缀，这样做的原因是方便检索\n",
    "            saver.save(sess, \"src/mnist_model/mnist_model.ckpt\",\n",
    "                                                         global_step=training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 10:20:33.400926 140244664342336 deprecation.py:323] From <ipython-input-1-6eee74d2e213>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1030 10:20:33.402638 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1030 10:20:33.404206 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 10:20:33.657950 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1030 10:20:33.664761 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1030 10:20:33.723299 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 10:20:34.012470 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest ckpt is mnist_model.ckpt-29001\n",
      "After 29001 training step(s), validation accuracy = 98.52%\n",
      "After 29001 trainging step(s) ,test accuracy = 98.44%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "\n",
    "# 定义相同的前向传播过程，要保持命名空间和变量名的一致\n",
    "def hidden_layer(input_tensor, regularizer, name):\n",
    "    with tf.variable_scope(\"hidden_layer\",reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\"weights\", [784, 500],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection(\"losses\", regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [500], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_output\",reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\"weights\", [500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection(\"losses\", regularizer(weights))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer=tf.constant_initializer(0.0))\n",
    "        hidden_layer_output = tf.matmul(hidden_layer, weights) + biases\n",
    "    return hidden_layer_output\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "# 因为测试时不必关注正则化损失的值，所以不会传入正则化的办法\n",
    "y = hidden_layer(x, None, name=\"y\")\n",
    "\n",
    "# 计算正确率的过程也基本和第六章的样例一致\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99)\n",
    "\n",
    "# 通过变量重命名的方式加载模型，这里使用了滑动平均类提供的variables_to_restore()\n",
    "# 于是就免去了在前向传播过程中调用求解滑动平均的函数来获取滑动平均值的过程\n",
    "saver = tf.train.Saver(variable_averages.variables_to_restore())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "    test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "    # get_checkpoint_state()函数会通过checkpoint文件自动找到目录中最新模型的文件名\n",
    "    # 函数原型get_checkpoint_state(checkpoint_dir,latest_filename)\n",
    "    ckpt = tf.train.get_checkpoint_state(\"src/mnist_model/\")\n",
    "\n",
    "#     # 加载模型\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    # 通过文件名得到模型保存时迭代的轮数\n",
    "    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    print(\"The latest ckpt is mnist_model.ckpt-%s\" % (global_step))\n",
    "    # 输出The latest ckpt is mnist_model.ckpt-29001\n",
    "\n",
    "    # 计算在验证数据集上的准确率并打印出来\n",
    "    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n",
    "    print(\"After %s training step(s), validation accuracy = %g%%\"\n",
    "          % (global_step, accuracy_score * 100))\n",
    "    # 输出After 29001 training step(s), validation accuracy = 98.62%\n",
    "\n",
    "    # 计算在测试数据集上的准确率并打印出来\n",
    "    test_accuracy = sess.run(accuracy, feed_dict=test_feed)\n",
    "    print(\"After %s trainging step(s) ,test accuracy = %g%%\"\n",
    "          % (global_step, test_accuracy * 100))\n",
    "    # 输出After 29001 trainging step(s) ,test accuracy = 98.51%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型为PB文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 10:28:48.187401 140244664342336 deprecation.py:323] From <ipython-input-2-715c03ad4a1a>:24: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W1030 10:28:48.188592 140244664342336 deprecation.py:323] From /home/liyuan3970/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#graph_util模块定义在tensorflow/python/framework/graph_util.py\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "a = tf.Variable(tf.constant(1.0, shape=[1]), name=\"a\")\n",
    "b = tf.Variable(tf.constant(2.0, shape=[1]), name=\"b\")\n",
    "result = a + b\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # 导出主要记录了TensorFlow计算图上节点信息的GraphDef部分\n",
    "    # 使用get_default_graph()函数获取默认的计算图\n",
    "    graph_def = tf.get_default_graph().as_graph_def()\n",
    "\n",
    "    # convert_variables_to_constants()函数表示用相同值的常量替换计算图中所有变量，\n",
    "    # 原型convert_variables_to_constants(sess,input_graph_def,output_node_names,\n",
    "    #                          variable_names_whitelist, variable_names_blacklist)\n",
    "    # 其中sess是会话，input_graph_def是具有节点的GraphDef对象，output_node_names\n",
    "    # 是要保存的计算图中的计算节点的名称，通常为字符串列表的形式，variable_names_whitelist\n",
    "    # 是要转换为常量的变量名称集合(默认情况下，所有变量都将被转换)，\n",
    "    # variable_names_blacklist是要省略转换为常量的变量名的集合。\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, ['add'])\n",
    "\n",
    "    # 将导出的模型存入.pb文件\n",
    "    with tf.gfile.GFile(\"src/pb/model.pb\", \"wb\") as f:\n",
    "    # SerializeToString()函数用于将获取到的数据取出存到一个string对象中，\n",
    "    # 然后再以二进制流的方式将其写入到磁盘文件中\n",
    "        f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# gfile模块定义在tensorflow/python/platform/gfile.py\n",
    "# 包含GFile、FastGFile和Open三个没有线程锁定的文件I/O包装器类\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 使用FsatGFile类的构造函数返回一个FastGFile类\n",
    "    with gfile.FastGFile(\"src/pb/model.pb\", 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        # 使用FastGFile类的read()函数读取保存的模型文件，并以字符串形式\n",
    "        # 返回文件的内容，之后通过ParseFromString()函数解析文件的内容\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # 使用import_graph_def()函数将graph_def中保存的计算图加载到当前图中\n",
    "    # 原型import_graph_def(graph_def,input_map,return_elements,name,op_dict,\n",
    "    #                                                     producer_op_list)\n",
    "    result = tf.import_graph_def(graph_def, return_elements=[\"add:0\"])\n",
    "\n",
    "    print(sess.run(result))\n",
    "    # 输出为[array([3.], dtype=float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
