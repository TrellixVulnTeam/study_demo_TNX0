{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNist数据集全套操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.99\n",
    "max_steps = 30000\n",
    "\n",
    "def hidden_layer(input_tensor,regularizer,avg_class,resuse):\n",
    "    #创建第一个卷积层，得到特征图大小为32@28x28\n",
    "    with tf.variable_scope(\"C1-conv\",reuse=resuse):\n",
    "        conv1_weights = tf.get_variable(\"weight\", [5, 5, 1, 32],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    #创建第一个池化层，池化后的结果为32@14x14\n",
    "    with tf.name_scope(\"S2-max_pool\",):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    # 创建第二个卷积层，得到特征图大小为64@14x14。注意，第一个池化层之后得到了32个\n",
    "    # 特征图，所以这里设输入的深度为32，我们在这一层选择的卷积核数量为64，所以输出\n",
    "    # 的深度是64，也就是说有64个特征图\n",
    "    with tf.variable_scope(\"C3-conv\",reuse=resuse):\n",
    "        conv2_weights = tf.get_variable(\"weight\", [5, 5, 32, 64],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    #创建第二个池化层，池化后结果为64@7x7\n",
    "    with tf.name_scope(\"S4-max_pool\",):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        #get_shape()函数可以得到这一层维度信息，由于每一层网络的输入输出都是一个batch的矩阵，\n",
    "        #所以通过get_shape()函数得到的维度信息会包含这个batch中数据的个数信息\n",
    "        #shape[1]是长度方向，shape[2]是宽度方向，shape[3]是深度方向\n",
    "        #shape[0]是一个batch中数据的个数，reshape()函数原型reshape(tensor,shape,name)\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        nodes = shape[1] * shape[2] * shape[3]    #nodes=3136\n",
    "        reshaped = tf.reshape(pool2, [shape[0], nodes])\n",
    "\n",
    "    #创建第一个全连层\n",
    "    with tf.variable_scope(\"layer5-full1\",reuse=resuse):\n",
    "        Full_connection1_weights = tf.get_variable(\"weight\", [nodes, 512],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection1_weights))\n",
    "        Full_connection1_biases = tf.get_variable(\"bias\", [512],\n",
    "                                                     initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class ==None:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, Full_connection1_weights) + \\\n",
    "                                                                   Full_connection1_biases)\n",
    "        else:\n",
    "            Full_1 = tf.nn.relu(tf.matmul(reshaped, avg_class.average(Full_connection1_weights))\n",
    "                                                   + avg_class.average(Full_connection1_biases))\n",
    "\n",
    "    #创建第二个全连层\n",
    "    with tf.variable_scope(\"layer6-full2\",reuse=resuse):\n",
    "        Full_connection2_weights = tf.get_variable(\"weight\", [512, 10],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection2_weights))\n",
    "        Full_connection2_biases = tf.get_variable(\"bias\", [10],\n",
    "                                                   initializer=tf.constant_initializer(0.1))\n",
    "        if avg_class == None:\n",
    "            result = tf.matmul(Full_1, Full_connection2_weights) + Full_connection2_biases\n",
    "        else:\n",
    "            result = tf.matmul(Full_1, avg_class.average(Full_connection2_weights)) + \\\n",
    "                                                  avg_class.average(Full_connection2_biases)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size ,28,28,1],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "y = hidden_layer(x,regularizer,avg_class=None,resuse=False)\n",
    "\n",
    "training_step = tf.Variable(0, trainable=False)\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99, training_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "average_y = hidden_layer(x,regularizer,variable_averages,resuse=True)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(learning_rate,\n",
    "                                 training_step, mnist.train.num_examples /batch_size ,\n",
    "                                 learning_rate_decay, staircase=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate). \\\n",
    "    minimize(loss, global_step=training_step)\n",
    "\n",
    "with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "crorent_predicition = tf.equal(tf.arg_max(average_y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n",
    "# with tf.Session() as sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     for i in range(max_steps):\n",
    "#         if i %1000==0:\n",
    "#             x_val, y_val = mnist.validation.next_batch(batch_size)\n",
    "#             reshaped_x2 = np.reshape(x_val, (batch_size,28,28, 1))\n",
    "#             validate_feed = {x: reshaped_x2, y_: y_val}\n",
    "\n",
    "#             validate_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "#             print(\"After %d trainging step(s) ,validation accuracy\"\n",
    "#                   \"using average model is %g%%\" % (i, validate_accuracy * 100))\n",
    "\n",
    "#         x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "\n",
    "#         reshaped_xs = np.reshape(x_train, (batch_size ,28,28,1))\n",
    "#         sess.run(train_op, feed_dict={x: reshaped_xs, y_: y_train})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
