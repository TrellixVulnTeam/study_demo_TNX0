{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-17166ebc04ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from config import resnet_config\n",
    "from data_loader import DataLoader\n",
    "from eval.evaluate import accuracy\n",
    "\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self,\n",
    "                 depth=resnet_config.depth,\n",
    "                 height=config.height,\n",
    "                 width=config.width,\n",
    "                 channel=config.channel,\n",
    "                 num_classes=config.num_classes,\n",
    "                 learning_rate=resnet_config.learning_rate,\n",
    "                 learning_decay_rate=resnet_config.learning_decay_rate,\n",
    "                 learning_decay_steps=resnet_config.learning_decay_steps,\n",
    "                 epoch=resnet_config.epoch,\n",
    "                 batch_size=resnet_config.batch_size,\n",
    "                 model_path=resnet_config.model_path,\n",
    "                 summary_path=resnet_config.summary_path):\n",
    "        \"\"\"\n",
    "\n",
    "        :param depth:\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_decay_rate = learning_decay_rate\n",
    "        self.learning_decay_steps = learning_decay_steps\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.model_path = model_path\n",
    "        self.summary_path = summary_path\n",
    "        self.num_block_dict = {18: [2, 2, 2, 2],\n",
    "                               34: [3, 4, 6, 3],\n",
    "                               50: [3, 4, 6, 3],\n",
    "                               101: [3, 4, 23, 3]}\n",
    "        self.bottleneck_dict = {18: False,\n",
    "                                34: False,\n",
    "                                50: True,\n",
    "                                101: True}\n",
    "        self.filter_out = [64, 128, 256, 512]\n",
    "        self.filter_out_last_layer = [256, 512, 1024, 2048]\n",
    "        self.conv_out_depth = self.filter_out[-1] if self.depth < 50 else self.filter_out_last_layer[-1]\n",
    "        assert self.depth in self.num_block_dict, 'depth should be in [18,34,50,101]'\n",
    "        self.num_block = self.num_block_dict[self.depth]\n",
    "        self.bottleneck = self.bottleneck_dict[self.depth]\n",
    "        self.input_x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='input_y')\n",
    "        self.prediction = None\n",
    "        self.loss = None\n",
    "        self.acc = None\n",
    "        self.global_step = None\n",
    "        self.data_loader = DataLoader()\n",
    "        self.model()\n",
    "\n",
    "    def model(self):\n",
    "        # first convolution layers\n",
    "        x = self.conv(x=self.input_x, k_size=7, filters_out=64, strides=2, activation=True, name='First_Conv')\n",
    "        x = tf.layers.max_pooling2d(x, pool_size=[3, 3], strides=2, padding='same', name='max_pool')\n",
    "        x = self.stack_block(x)\n",
    "        x = tf.layers.average_pooling2d(x, pool_size=x.get_shape()[1:3], strides=1, name='average_pool')\n",
    "        x = tf.reshape(x, [-1, 1 * 1 * self.conv_out_depth])\n",
    "        fc_W = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        logits = tf.layers.dense(inputs=x, units=self.num_classes,kernel_initializer=fc_W)\n",
    "\n",
    "        # 预测值\n",
    "        self.prediction = tf.argmax(logits,axis=-1)\n",
    "        # 计算准确率\n",
    "        self.acc = accuracy(logits, self.input_y)\n",
    "        # 损失值\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y))\n",
    "        # 全局步数\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        # 递减学习率\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=self.learning_rate,\n",
    "                                                   global_step=self.global_step,\n",
    "                                                   decay_rate=self.learning_decay_rate,\n",
    "                                                   decay_steps=self.learning_decay_steps,\n",
    "                                                   staircase=True)\n",
    "        self.optimize = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "    def stack_block(self, input_x):\n",
    "        for stack in range(4):\n",
    "            stack_strides = 1 if stack == 0 else 2\n",
    "            stack_name = 'stack_%s' % stack\n",
    "            with tf.name_scope(stack_name):\n",
    "                for block in range(self.num_block[stack]):\n",
    "                    shortcut = input_x\n",
    "                    block_strides = stack_strides if block == 0 else 1\n",
    "                    block_name = stack_name + '_block_%s' % block\n",
    "                    with tf.name_scope(block_name):\n",
    "                        if self.bottleneck:\n",
    "                            for layer in range(3):\n",
    "                                with tf.name_scope(block_name + '_layer_%s' % layer):\n",
    "                                    filters = self.filter_out[stack] if layer < 2 else self.filter_out_last_layer[stack]\n",
    "                                    k_size = 3 if layer == 1 else 1\n",
    "                                    layer_strides = block_strides if layer < 1 else 1\n",
    "                                    activation = True if layer < 2 else False\n",
    "                                    layer_name = block_name + '_conv_%s' % layer\n",
    "                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,\n",
    "                                                        strides=layer_strides, activation=activation, name=layer_name)\n",
    "                        else:\n",
    "                            for layer in range(2):\n",
    "                                with tf.name_scope(block_name + '_layer_%s' % layer):\n",
    "                                    filters = self.filter_out[stack]\n",
    "                                    k_size = 3\n",
    "                                    layer_strides = block_strides if layer < 1 else 1\n",
    "                                    activation = True if layer < 1 else False\n",
    "                                    layer_name = block_name + '_conv_%s' % layer\n",
    "                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,\n",
    "                                                        strides=layer_strides, activation=activation, name=layer_name)\n",
    "                    shortcut_depth = shortcut.get_shape()[-1]\n",
    "                    input_x_depth = input_x.get_shape()[-1]\n",
    "                    with tf.name_scope('shortcut_connect'):\n",
    "                        if shortcut_depth != input_x_depth:\n",
    "                            connect_k_size = 1\n",
    "                            connect_strides = block_strides\n",
    "                            connect_filter = filters\n",
    "                            shortcut_name = block_name + '_shortcut'\n",
    "                            shortcut = self.conv(x=shortcut, filters_out=connect_filter, k_size=connect_k_size,\n",
    "                                                 strides=connect_strides, activation=False, name=shortcut_name)\n",
    "                        input_x = tf.nn.relu(shortcut + input_x)\n",
    "\n",
    "        return input_x\n",
    "\n",
    "    def conv(self, x, k_size, filters_out, strides, activation, name):\n",
    "        x = tf.layers.conv2d(x, filters=filters_out, kernel_size=k_size, strides=strides, padding='same', name=name)\n",
    "        x = tf.layers.batch_normalization(x, name=name + '_BN')\n",
    "        if activation:\n",
    "            x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_id_list, valid_img, valid_label):\n",
    "        \"\"\"\n",
    "        training model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 模型存储路径初始化\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        if not os.path.exists(self.summary_path):\n",
    "            os.makedirs(self.summary_path)\n",
    "\n",
    "        # train_steps初始化\n",
    "        train_steps = 0\n",
    "        best_valid_acc = 0.0\n",
    "\n",
    "        # summary初始化\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # session初始化\n",
    "        sess = tf.Session()\n",
    "        writer = tf.summary.FileWriter(self.summary_path, sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(self.epoch):\n",
    "            shuffle_id_list = random.sample(train_id_list.tolist(), len(train_id_list))\n",
    "            batch_num = int(np.ceil(len(shuffle_id_list) / self.batch_size))\n",
    "            train_id_batch = np.array_split(shuffle_id_list, batch_num)\n",
    "            for i in range(batch_num):\n",
    "                this_batch = train_id_batch[i]\n",
    "                batch_img, batch_label = self.data_loader.get_batch_data(this_batch)\n",
    "                train_steps += 1\n",
    "                feed_dict = {self.input_x: batch_img, self.input_y: batch_label}\n",
    "                _, train_loss, train_acc = sess.run([self.optimize, self.loss, self.acc], feed_dict=feed_dict)\n",
    "                if train_steps % 1 == 0:\n",
    "                    val_loss, val_acc = sess.run([self.loss, self.acc],\n",
    "                                                 feed_dict={self.input_x: valid_img, self.input_y: valid_label})\n",
    "                    msg = 'epoch:%s | steps:%s | train_loss:%.4f | val_loss:%.4f | train_acc:%.4f | val_acc:%.4f' % (\n",
    "                        epoch, train_steps, train_loss, val_loss, train_acc, val_acc)\n",
    "                    print(msg)\n",
    "                    summary = sess.run(merged, feed_dict={self.input_x: valid_img, self.input_y: valid_label})\n",
    "                    writer.add_summary(summary, global_step=train_steps)\n",
    "                    if val_acc >= best_valid_acc:\n",
    "                        best_valid_acc = val_acc\n",
    "                        saver.save(sess, save_path=self.model_path, global_step=train_steps)\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        predicting\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        prediction = sess.run(self.prediction, feed_dict={self.input_x: x})\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "o\n",
    "\n",
    "class Block(collections.namedtuple(\"block\", [\"name\", \"residual_unit\", \"args\"])):\n",
    "    \"A named tuple describing a ResNet Block\"\n",
    "    # namedtuple()函数原型为：\n",
    "    # collections.namedtuple(typename,field_names,verbose,rename)\n",
    "\n",
    "\n",
    "def conv2d_same(inputs, num_outputs, kernel_size, stride, scope=None):\n",
    "    # 如果步长为1，则直接使用padding=\"SAME\"的方式进行卷积操作\n",
    "    # 一般步长不为1的情况出现在残差学习块的最后一个卷积操作中\n",
    "    if stride == 1:\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1,\n",
    "                           padding=\"SAME\", scope=scope)\n",
    "    else:\n",
    "        pad_begin = (kernel_size - 1) // 2\n",
    "        pad_end = kernel_size - 1 - pad_begin\n",
    "\n",
    "        # pad()函数用于对矩阵进行定制填充\n",
    "        # 在这里用于对inputs进行向上填充pad_begin行0，向下填充pad_end行0，\n",
    "        # 向左填充pad_begin行0，向右填充pad_end行0\n",
    "        inputs = tf.pad(inputs,\n",
    "                        [[0, 0], [pad_begin, pad_end], [pad_begin, pad_end], [0, 0]])\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n",
    "                           padding=\"VALID\", scope=scope)\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def residual_unit(inputs, depth, depth_residual, stride, outputs_collections=None,\n",
    "                  scope=None):\n",
    "    with tf.variable_scope(scope, \"residual_v2\", [inputs]) as sc:\n",
    "\n",
    "        # 输入的通道数，取inputs的形状的最后一个元素\n",
    "        depth_input = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "\n",
    "        # 使用slim.batch_norm()函数进行BatchNormalization操作\n",
    "        preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\"preac\")\n",
    "\n",
    "        # 如果本块的depth值(depth参数)等于上一个块的depth(depth_input)，则考虑进行降采样操作，\n",
    "        # 如果depth值不等于depth_input，则使用conv2d()函数使输入通道数和输出通道数一致\n",
    "        if depth == depth_input:\n",
    "            # 如果stride等于1，则不进行降采样操作，如果stride不等于1，则使用max_pool2d\n",
    "            # 进行步长为stride且池化核为1x1的降采样操作\n",
    "            if stride == 1:\n",
    "                identity = inputs\n",
    "            else:\n",
    "                identity = slim.max_pool2d(inputs, [1, 1], stride=stride, scope=\"shortcut\")\n",
    "        else:\n",
    "            identity = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None,\n",
    "                                   activation_fn=None, scope=\"shortcut\")\n",
    "\n",
    "        # 在一个残差学习块中的3个卷积层\n",
    "        residual = slim.conv2d(preact, depth_residual, [1, 1], stride=1, scope=\"conv1\")\n",
    "        residual = conv2d_same(residual, depth_residual, 3, stride, scope=\"conv2\")\n",
    "        residual = slim.conv2d(residual, depth, [1, 1], stride=1, normalizer_fn=None,\n",
    "                               activation_fn=None, scope=\"conv3\")\n",
    "\n",
    "        # 将identity的结果和residual的结果相加\n",
    "        output = identity + residual\n",
    "\n",
    "        result = slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def resnet_v2_152(inputs, num_classes, reuse=None, scope=\"resnet_v2_152\"):\n",
    "    blocks = [\n",
    "        Block(\"block1\", residual_unit, [(256, 64, 1), (256, 64, 1), (256, 64, 2)]),\n",
    "        Block(\"block2\", residual_unit, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n",
    "        Block(\"block3\", residual_unit, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
    "        Block(\"block4\", residual_unit, [(2048, 512, 1)] * 3)]\n",
    "    return resnet_v2(inputs, blocks, num_classes, reuse=reuse, scope=scope)\n",
    "\n",
    "\n",
    "def resnet_v2(inputs, blocks, num_classes, reuse=None, scope=None):\n",
    "    with tf.variable_scope(scope, \"resnet_v2\", [inputs], reuse=reuse) as sc:\n",
    "        end_points_collection = sc.original_name_scope + \"_end_points\"\n",
    "\n",
    "        # 对函数residual_unit()的outputs_collections参数使用参数空间\n",
    "        with slim.arg_scope([residual_unit], outputs_collections=end_points_collection):\n",
    "\n",
    "            # 创建ResNet的第一个卷积层和池化层，卷积核大小7x7，深度64，池化核大侠3x3\n",
    "            with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None):\n",
    "                net = conv2d_same(inputs, 64, 7, stride=2, scope=\"conv1\")\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\"pool1\")\n",
    "\n",
    "            # 在两个嵌套的for循环内调用residual_unit()函数堆砌ResNet的结构\n",
    "            for block in blocks:\n",
    "                # block.name分别为block1、block2、block3和block4\n",
    "                with tf.variable_scope(block.name, \"block\", [net]) as sc:\n",
    "\n",
    "                    # tuple_value为Block类的args参数中的每一个元组值，\n",
    "                    # i是这些元组在每一个Block的args参数中的序号\n",
    "                    for i, tuple_value in enumerate(block.args):\n",
    "                        # i的值从0开始，对于第一个unit，i需要加1\n",
    "                        with tf.variable_scope(\"unit_%d\" % (i + 1), values=[net]):\n",
    "                            # 每一个元组都有3个数组成，将这三个数作为参数传递到Block类的\n",
    "                            # residual_unit参数中，在定义blockss时，这个参数就是函数residual_unit()\n",
    "                            depth, depth_bottleneck, stride = tuple_value\n",
    "                            net = block.residual_unit(net, depth=depth, depth_residual=depth_bottleneck,\n",
    "                                                      stride=stride)\n",
    "                    # net就是每一个块的结构\n",
    "                    net = slim.utils.collect_named_outputs(end_points_collection, sc.name, net)\n",
    "\n",
    "            # 对net使用slim.batch_norm()函数进行BatchNormalization操作\n",
    "            net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\"postnorm\")\n",
    "\n",
    "            # 创建全局平均池化层\n",
    "            net = tf.reduce_mean(net, [1, 2], name=\"pool5\", keep_dims=True)\n",
    "\n",
    "            # 如果定义了num_classes，则通过1x1池化的方式获得数目为num_classes的输出\n",
    "            if num_classes is not None:\n",
    "                net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                                  normalizer_fn=None, scope=\"logits\")\n",
    "\n",
    "            return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ResNet_struct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dfd246650624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mResNet_struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ResNet_struct'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import ResNet_struct\n",
    "from tensorflow.contrib import slim\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "num_steps_burn_in = 10\n",
    "total_duration = 0.0\n",
    "total_duration_squared = 0.0\n",
    "inputs = tf.random_uniform((batch_size, 224, 224, 3))\n",
    "\n",
    "def arg_scope(is_training=True,weight_decay=0.0001,batch_norm_decay=0.997,\n",
    "                           batch_norm_epsilon=1e-5,batch_norm_scale=True):\n",
    "\n",
    "    batch_norm_params = {\"is_training\": is_training,\n",
    "                         \"decay\": batch_norm_decay,\n",
    "                         \"epsilon\": batch_norm_epsilon,\n",
    "                         \"scale\": batch_norm_scale,\n",
    "                         \"updates_collections\": tf.GraphKeys.UPDATE_OPS}\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                        #weights_initializer用于指定权重的初始化程序\n",
    "                        weights_initializer=slim.variance_scaling_initializer(),\n",
    "                        #weights_regularizer为权重可选的正则化程序\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                        #activation_fn用于激活函数的指定，默认的为ReLU函数\n",
    "                        #normalizer_params用于指定正则化函数的参数\n",
    "                        activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params=batch_norm_params):\n",
    "\n",
    "        #定义slim.batch_norm()函数的参数空间\n",
    "        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "            # slim.max_pool2d()函数的参数空间\n",
    "            with slim.arg_scope([slim.max_pool2d], padding=\"SAME\") as arg_scope:\n",
    "                return arg_scope\n",
    "\n",
    "# 定义模型的前向传播过程，这被限制在一个参数空间中\n",
    "with slim.arg_scope(arg_scope(is_training=False)):\n",
    "    net = ResNet_struct.resnet_v2_152(inputs, 1000)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    #运行前向传播测试过程\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = sess.run(net)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            if i % 10 == 0:\n",
    "                print('%s: step %d, duration = %.3f' %\n",
    "                      (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    average_time = total_duration / num_batches\n",
    "\n",
    "    #打印前向传播的运算时间信息\n",
    "    print('%s: Forward across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "          (datetime.now(), num_batches, average_time,\n",
    "           math.sqrt(total_duration_squared / num_batches-average_time*average_time)))\n",
    "\n",
    "\n",
    "'''打印的内容\n",
    "2018-04-28 15:44:25.253434: step 0, duration = 1.039\n",
    "2018-04-28 15:44:35.616892: step 10, duration = 1.037\n",
    "2018-04-28 15:44:45.981536: step 20, duration = 1.035\n",
    "2018-04-28 15:44:56.349566: step 30, duration = 1.036\n",
    "2018-04-28 15:45:06.728368: step 40, duration = 1.035\n",
    "2018-04-28 15:45:17.089299: step 50, duration = 1.035\n",
    "2018-04-28 15:45:27.456285: step 60, duration = 1.037\n",
    "2018-04-28 15:45:37.822637: step 70, duration = 1.035\n",
    "2018-04-28 15:45:48.192688: step 80, duration = 1.035\n",
    "2018-04-28 15:45:58.555936: step 90, duration = 1.035\n",
    "2018-04-28 15:46:07.886232: Forward across 100 steps, 1.037 +/- 0.002 sec / batch\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
