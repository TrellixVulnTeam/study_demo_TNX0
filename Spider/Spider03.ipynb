{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取腾讯二级子页面的数据信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "class TencentSpider(object):\n",
    "  def __init__(self):\n",
    "    self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "\n",
    "  # 获取工作职责和工作要求(发请求并解析二级页面)\n",
    "  def get_job_duty(self,url):\n",
    "    # 获取二级页面响应内容\n",
    "    res = requests.get(url,headers=self.headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    html = res.text\n",
    "    # 正则提取职责和要求\n",
    "    p = re.compile('<tr class=\"c\">.*?<ul class=\"squareli\">(.*?)</ul>',re.S)\n",
    "    job_info = p.findall(html)\n",
    "    # job_info : ['<li>1.职责一</li>\\n<li>','']\n",
    "    duty = job_info[0].strip().replace('<li>','').replace('</li>','')\n",
    "    requirement = job_info[1].strip().replace('<li>','').replace('</li','')\n",
    "    # 返回处理好的职责和要求字符串\n",
    "    return duty,requirement\n",
    "\n",
    "\n",
    "\n",
    "  # 获取一级子界面职位信息\n",
    "  def get_job_info(self,url,params):\n",
    "    res = requests.get(url,params=params,headers=self.headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    html = res.text\n",
    "    # 交给解析函数去做解析\n",
    "    self.parse_job_info(html)\n",
    "\n",
    "  # 解析职位信息\n",
    "  def parse_job_info(self,html):\n",
    "    p = re.compile('<td class=\"l square\">.*?href=\"(.*?)\">(.*?)</a>.*?<td>(.*?)</td>.*?<td>(.*?)</td>.*?<td>(.*?)</td>.*?<td>(.*?)</td>',re.S)\n",
    "    job_info_list = p.findall(html)\n",
    "    # job_info_list:[('链接','名称','类别','人数','地点','时间'),(),(),(),()]\n",
    "    for job in job_info_list:\n",
    "      job_link = 'https://hr.tencent.com/'+job[0]\n",
    "      job_name = job[1]\n",
    "      job_type = job[2]\n",
    "      job_number = job[3]\n",
    "      job_address = job[4]\n",
    "      job_time = job[5]\n",
    "      job_duty,job_requirement = self.get_job_duty(job_link)\n",
    "\n",
    "\n",
    "      d = {\n",
    "        '职位链接:' : job_link,\n",
    "        '职位名称:' : job_name,\n",
    "        '职位类别:' : job_type,\n",
    "        '招聘人数:' : job_number,\n",
    "        '招聘地点:' : job_address,\n",
    "        '发布时间:' : job_time,\n",
    "        '工作职责:' : job_duty,\n",
    "        '工作要求:' : job_requirement\n",
    "      }\n",
    "      print(d)\n",
    "      print('*' * 30)\n",
    "\n",
    "  # 数据处理\n",
    "  def write_page(self):\n",
    "    pass\n",
    "\n",
    "  # 主函数\n",
    "  def work_on(self):\n",
    "    job = input('请输入职位方向:')\n",
    "\n",
    "    for pn in range(0,2001,10):\n",
    "      url = 'https://hr.tencent.com/position.php?'\n",
    "      params = {\n",
    "          'keywords': job,\n",
    "          'start' : str(pn)\n",
    "        }\n",
    "      self.get_job_info(url,params)\n",
    "      time.sleep(2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  start = time.time()\n",
    "  spider = TencentSpider()\n",
    "  spider.work_on()\n",
    "  end = time.time()\n",
    "  print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 拼接url信息\n",
    "import requests\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0'}\n",
    "url = 'http://www.baidu.com/s?'\n",
    "\n",
    "key = input('请输入要搜索的内容:')\n",
    "params = {\n",
    "    'wd' : key,\n",
    "    'pn' : '10'\n",
    "  }\n",
    "# 三步走,自动对params编码,后和基准url进行拼接\n",
    "res = requests.get(url,params=params,headers=headers)\n",
    "res.encoding = 'utf-8'\n",
    "html = res.text\n",
    "\n",
    "print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 设置代理\n",
    "import requests\n",
    "\n",
    "# 测试网址,能显示出口IP\n",
    "url = 'http://httpbin.org/get'\n",
    "headers = {'User-Agent' : 'Mozilla/5.0'}\n",
    "# 定义一个代理\n",
    "proxies = {\n",
    "    'http':'http://159.224.13.29:61366',\n",
    "    'https':'https://159.224.13.29:61366'\n",
    "  }\n",
    "# proxies = {\n",
    "#     'http':'http://309435365:szayclhp@116.255.191.105:16816'\n",
    "# }\n",
    "\n",
    "# 发请求\n",
    "res = requests.get(url,\n",
    "                   headers=headers,\n",
    "                   proxies=proxies)\n",
    "res.encoding = 'utf-8'\n",
    "html = res.text\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##设置代理　和密码\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "class NoteSpider(object):\n",
    "  def __init__(self):\n",
    "    self.headers = {'User-Agent':'Mozilla/5.0'}\n",
    "    # 使用普通代理\n",
    "    self.proxies = {\n",
    "          'http':'http://111.177.183.54:9999',\n",
    "          'https':'https://111.177.183.54:9999'\n",
    "    }\n",
    "    # web客户端验证参数\n",
    "    self.auth = ('tarenacode','code_2013')\n",
    "\n",
    "  # 获取解析页面\n",
    "  def get_page(self,url):\n",
    "    # 获取页面\n",
    "    res = requests.get(url,\n",
    "                       headers=self.headers,\n",
    "                       auth=self.auth,\n",
    "                       proxies=self.proxies)\n",
    "    res.encoding = 'utf-8'\n",
    "    html = res.text\n",
    "    # 解析页面\n",
    "    p = re.compile('<a href=\"(.*?)/.*?</a>',re.S)\n",
    "    r_list = p.findall(html)\n",
    "    print(r_list)\n",
    "    # r_list : ['..','AIDCode','','']\n",
    "    self.write_page(r_list)\n",
    "\n",
    "  # 保存数据\n",
    "  def write_page(self,r_list):\n",
    "    with open('Note.csv','w') as f:\n",
    "      # 创建写入对象\n",
    "      writer = csv.writer(f)\n",
    "      for r in r_list:\n",
    "        if r != '..':\n",
    "          writer.writerow([r])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  spider = NoteSpider()\n",
    "  spider.get_page('http://code.tarena.com.cn/')\n",
    "\n",
    "\n",
    "# 111.177.183.198:9999\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xpath\n",
    "\n",
    "```python\n",
    "\n",
    "1、xpath解析模块\n",
    "  1、在XML文档中查找信息的语言,同样适用于HTML文档的检索\n",
    "  2、xpath辅助工具\n",
    "    1、Chrome插件 ：Xpath Helper\n",
    "       打开/关闭 ：ctrl + shift + x\n",
    "    2、Firefox插件：Xpath Checker\n",
    "    3、xpath编辑工具：XML Quire\n",
    "  3、浏览器插件xpath helper安装\n",
    "    ** 方法一 **\n",
    "      把 插件.crx 鼠标拖拽到 浏览器开发者模式释放\n",
    "    ** 方法二 **\n",
    "    1、把 插件.crx 改为 .rar\n",
    "    2、解压 插件.rar 解压到当前文件夹\n",
    "    3、浏览器-更多工具-扩展程序-开发者模式-加载已解压的扩展程序-选择(解压的路径文件夹)\n",
    "    4、重启浏览器\n",
    "  4、xpath匹配演示\n",
    "    1、查找所有的book节点 ：//book\n",
    "    2、查找所有book下title子节点中,lang属性值为'en'的节点\n",
    "      //book/title[@lang=\"en\"]\n",
    "    3、查找bookstore下的第2个book节点下title子节点\n",
    "      //bookstore/book[2]/title\n",
    "    4、查找所有book/title中lang属性的值\n",
    "      //book/title/@lang\n",
    "  5、选取节点\n",
    "    1、// ：所有节点中查找\n",
    "            //price  //book//price\n",
    "    2、@  ：获取节点属性值\n",
    "          **条件: //div[@class=\"movie-item-info\">]\n",
    "\t  **取值: //div//a/@src\n",
    "    3、|  ：匹配多路径\n",
    "            xpath表达式1 | xpath表达式2\n",
    "\t  //tr[@class=\"even\"] | //tr[@class=\"odd\"]\n",
    "  6、函数\n",
    "    1、contains()\n",
    "       匹配一个属性值中包含某些字符串的节点\n",
    "       //title[contains(@lang,'e')]\n",
    "       //div[contains(@id,'qiushi_tag_')]\n",
    "    2、text() ：获取文本\n",
    "       //book/title/text()\n",
    "2、lxml库及xpath使用\n",
    "  1、lxml安装 ：sudo pip3 install lxml\n",
    "  2、使用流程\n",
    "     1、导模块 ：from lxml import etree\n",
    "     2、创建解析对象 ：parse_html=etree.HTML(html)\n",
    "     3、调用xpath匹配\n",
    "        r_list=parse_html.xpath('xpath表达式')\n",
    "     *** 只要调用了xpath,结果一定是列表 ***\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "html = '''\n",
    "<div class=\"wrapper\">\n",
    "\t<i class=\"iconfont icon-back\" id=\"back\"></i>\n",
    "\t<a href=\"/\" id=\"channel\">新浪社会</a>\n",
    "\t<ul id=\"nav\">\n",
    "\t\t<li><a href=\"http://domestic.firefox.sina.com/\" title=\"国内\">国内</a></li>\n",
    "\t\t<li><a href=\"http://world.firefox.sina.com/\" title=\"国际\">国际</a></li>\n",
    "\t\t<li><a href=\"http://mil.firefox.sina.com/\" title=\"军事\">军事</a></li>\n",
    "\t\t<li><a href=\"http://photo.firefox.sina.com/\" title=\"图片\">图片</a></li>\n",
    "\t\t<li><a href=\"http://society.firefox.sina.com/\" title=\"社会\">社会</a></li>\n",
    "\t\t<li><a href=\"http://ent.firefox.sina.com/\" title=\"娱乐\">娱乐</a></li>\n",
    "\t\t<li><a href=\"http://tech.firefox.sina.com/\" title=\"科技\">科技</a></li>\n",
    "\t\t<li><a href=\"http://sports.firefox.sina.com/\" title=\"体育\">体育</a></li>\n",
    "\t\t<li><a href=\"http://finance.firefox.sina.com/\" title=\"财经\">财经</a></li>\n",
    "\t\t<li><a href=\"http://auto.firefox.sina.com/\" title=\"汽车\">汽车</a></li>\n",
    "\t</ul>\n",
    "\t<i class=\"iconfont icon-liebiao\" id=\"menu\"></i>\n",
    "</div>'''\n",
    "\n",
    "# 返回所有a节点的文本内容['新浪社会','国际',...]\n",
    "parse_html = etree.HTML(html)\n",
    "r_list = parse_html.xpath('//a/text()')\n",
    "#print(r_list)\n",
    "\n",
    "# 提取所有a节点的href的属性值\n",
    "parse_html = etree.HTML(html)\n",
    "r_list = parse_html.xpath('//a/@href')\n",
    "\n",
    "#print(r_list)\n",
    "# 获取['图片','军事',...] 不包括 '新浪社会'\n",
    "parse_html = etree.HTML(html)\n",
    "r_list = parse_html.xpath(\n",
    "        '//div[@class=\"wrapper\"]/ul/li/a/text()')\n",
    "print(r_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
