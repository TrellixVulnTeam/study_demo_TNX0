{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 豆瓣电影ajax升级\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class DoubanSpider(object):\n",
    "  def __init__(self):\n",
    "    self.url = 'https://movie.douban.com/j/chart/top_list?'\n",
    "    self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}\n",
    "\n",
    "  # 获取页面\n",
    "  def get_page(self,params):\n",
    "    res = requests.get(self.url,params=params,\n",
    "                       headers=self.headers)\n",
    "    html = res.text\n",
    "    # html(json格式的字符串) : [{},{},{},{}]\n",
    "    self.parse_page(html)\n",
    "\n",
    "  # 解析页面\n",
    "  def parse_page(self,html):\n",
    "    # 把json格式的字符串转为python数据类型\n",
    "    r_list = json.loads(html)\n",
    "    for r in r_list:\n",
    "      name = r['title']\n",
    "      score = r['score']\n",
    "      print(name,score)\n",
    "\n",
    "  # 保存页面\n",
    "  def write_page(self):\n",
    "    pass\n",
    "\n",
    "  # 主函数\n",
    "  def work_on(self):\n",
    "    print('\\033[31m**************************\\033[0m')\n",
    "    print('\\033[31m| 　剧情 |  喜剧  |  爱情  |\\033[0m')\n",
    "    print('\\033[31m**************************\\033[0m')\n",
    "    # 存储所有电影类型\n",
    "    kinds = ['剧情','喜剧','爱情']\n",
    "    # 定义字典,保存类型对应的值\n",
    "    kdict = {'剧情':'11','喜剧':'24','爱情':'13'}\n",
    "    kind = input('请输入电影类型:')\n",
    "    if kind in kinds:\n",
    "      n = input('请输入要爬取的电影数量:')\n",
    "      params = {\n",
    "          'type': kdict[kind],\n",
    "          'interval_id': '100:90',\n",
    "          'action': '',\n",
    "          'start': '0',\n",
    "          'limit': n\n",
    "      }\n",
    "      self.get_page(params)\n",
    "    else:\n",
    "      print('电影类型不存在')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  spider = DoubanSpider()\n",
    "  spider.work_on()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 京东爬虫\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import csv\n",
    "\n",
    "class JdSpider(object):\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.url = 'https://www.jd.com/'\n",
    "        self.page = 1\n",
    "    # 发请求获取商品信息,发送内容到文本框,点击搜索按钮\n",
    "    def get_page(self,key):\n",
    "        self.driver.get(self.url)\n",
    "        self.driver.find_element_by_class_name('text').send_keys(key)\n",
    "        self.driver.find_element_by_class_name('button').click()\n",
    "        # 休眠3秒,等待页面加载\n",
    "        time.sleep(4)\n",
    "    # 解析页面\n",
    "    def parse_page(self):\n",
    "        # 执行JS脚本,进度条拉到最下面(Ajax动态加载)\n",
    "        self.driver.execute_script(\n",
    "            'window.scrollTo(0,document.body.scrollHeight)'\n",
    "        )\n",
    "        time.sleep(3)\n",
    "        # xpath匹配所有的商品信息\n",
    "        r_list = self.driver.find_elements_by_xpath(\n",
    "                             '//div[@id=\"J_goodsList\"]//li')\n",
    "        for r in r_list:\n",
    "            # info_list : ['￥29','拍拍','书名','评论','专营店']\n",
    "            info_list = r.text.split('\\n')\n",
    "            if info_list[1] == '拍拍':\n",
    "                price = info_list[0]\n",
    "                name = info_list[2]\n",
    "                comment = info_list[3]\n",
    "                shop = info_list[4]\n",
    "            else:\n",
    "                price = info_list[0]\n",
    "                name = info_list[1]\n",
    "                comment = info_list[2]\n",
    "                shop = info_list[3]\n",
    "            L = [price,comment,shop,name]\n",
    "            self.write_csv(L)\n",
    "    # 保存数据\n",
    "    def write_csv(self,L):\n",
    "        with open('product.csv','a',newline='',encoding='gb18030') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(L)\n",
    "    # 主函数\n",
    "    def work_on(self):\n",
    "        key = input('请输入商品:')\n",
    "        self.get_page(key)\n",
    "        while True:\n",
    "            self.parse_page()\n",
    "            print('第%d页爬取完成' % self.page)\n",
    "            self.page += 1\n",
    "            # 点击下一页,如果为 -1 说明不是最后1页\n",
    "            if self.driver.page_source.find('pn-next disabled') == -1:\n",
    "                self.driver.find_element_by_class_name('pn-next').click()\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    spider = JdSpider()\n",
    "    spider.work_on()\n",
    "    end = time.time()\n",
    "    print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "# ￥28.70\n",
    "# 拍拍\n",
    "# 奇妙的爬虫 正版书籍 新华书店发货 二手99新\n",
    "# 0条评价\n",
    "# 古的旧书图书专营店\n",
    "# **********************\n",
    "# ￥97.50\n",
    "# 包邮 玩转Django 2.0+玩转Python网络爬虫书籍 黄永祥 区域包邮基于Python3从零基础到项目实战书籍深入剖析Django2.0书籍\n",
    "# 20+条评价\n",
    "# 蓝墨水图书专营店\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''此程序存在问题，显性等待导致商品节点加载不完全'''\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import csv\n",
    "# 显性等待相关模块和类导入\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "class JdSpider(object):\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.url = 'https://www.jd.com/'\n",
    "        self.page = 1\n",
    "    # 发请求获取商品信息,发送内容到文本框,点击搜索按钮\n",
    "    def get_page(self,key):\n",
    "        self.driver.get(self.url)\n",
    "        self.driver.find_element_by_class_name('text').send_keys(key)\n",
    "        self.driver.find_element_by_class_name('button').click()\n",
    "        # 设置显性等待,等待页面ul节点加载\n",
    "        # 10代表最长超时时间\n",
    "        wait = WebDriverWait(self.driver,10)\n",
    "        wait.until(EC.presence_of_element_located(\n",
    "            (By.ID,'J_goodsList')))\n",
    "\n",
    "    # 解析页面\n",
    "    def parse_page(self):\n",
    "        # 执行JS脚本,进度条拉到最下面(Ajax动态加载)\n",
    "        self.driver.execute_script(\n",
    "            'window.scrollTo(0,document.body.scrollHeight)'\n",
    "        )\n",
    "        wait = WebDriverWait(self.driver,10)\n",
    "        wait.until(EC.presence_of_element_located(\n",
    "            (By.ID,'J_goodsList')))\n",
    "        # xpath匹配所有的商品信息\n",
    "        r_list = self.driver.find_elements_by_xpath(\n",
    "                             '//div[@id=\"J_goodsList\"]//li')\n",
    "        for r in r_list:\n",
    "            # info_list : ['￥29','拍拍','书名','评论','专营店']\n",
    "            info_list = r.text.split('\\n')\n",
    "            if info_list[1] == '拍拍':\n",
    "                price = info_list[0]\n",
    "                name = info_list[2]\n",
    "                comment = info_list[3]\n",
    "                shop = info_list[4]\n",
    "            else:\n",
    "                price = info_list[0]\n",
    "                name = info_list[1]\n",
    "                comment = info_list[2]\n",
    "                shop = info_list[3]\n",
    "            L = [price,comment,shop,name]\n",
    "            self.write_csv(L)\n",
    "    # 保存数据\n",
    "    def write_csv(self,L):\n",
    "        with open('product2.csv','a',newline='',encoding='gb18030') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(L)\n",
    "    # 主函数\n",
    "    def work_on(self):\n",
    "        key = input('请输入商品:')\n",
    "        self.get_page(key)\n",
    "        while True:\n",
    "            self.parse_page()\n",
    "            print('第%d页爬取完成' % self.page)\n",
    "            self.page += 1\n",
    "            # 点击下一页,如果为 -1 说明不是最后1页\n",
    "            if self.driver.page_source.find('pn-next disabled') == -1:\n",
    "                self.driver.find_element_by_class_name('pn-next').click()\n",
    "                wait = WebDriverWait(self.driver, 10)\n",
    "                wait.until(EC.presence_of_element_located(\n",
    "                    (By.ID, 'J_goodsList')))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    spider = JdSpider()\n",
    "    spider.work_on()\n",
    "    end = time.time()\n",
    "    print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "# ￥28.70\n",
    "# 拍拍\n",
    "# 奇妙的爬虫 正版书籍 新华书店发货 二手99新\n",
    "# 0条评价\n",
    "# 古的旧书图书专营店\n",
    "# **********************\n",
    "# ￥97.50\n",
    "# 包邮 玩转Django 2.0+玩转Python网络爬虫书籍 黄永祥 区域包邮基于Python3从零基础到项目实战书籍深入剖析Django2.0书籍\n",
    "# 20+条评价\n",
    "# 蓝墨水图书专营店\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多线程爬去小米应用商店\n",
    "import requests\n",
    "from threading import Thread\n",
    "from multiprocessing import Queue\n",
    "import json\n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "class XiaomiSpider(object):\n",
    "    def __init__(self):\n",
    "        # baseurl为抓包工具抓到的地址(去掉查询参数的)\n",
    "        self.baseurl = 'http://app.mi.com/categotyAllListApi?'\n",
    "        self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "        # 两个队列(url队列和解析队列)\n",
    "        self.url_queue = Queue()\n",
    "        self.parse_queue = Queue()\n",
    "    # URL入队列(拼10个url放入队列)\n",
    "    def get_url(self):\n",
    "        for i in range(50):\n",
    "            params = {\n",
    "                'page': str(i),\n",
    "                'categoryId': '2',\n",
    "                'pageSize': '30'\n",
    "            }\n",
    "            params = parse.urlencode(params)\n",
    "            # 拼接url地址,并入队列\n",
    "            url = self.baseurl + params\n",
    "            self.url_queue.put(url)\n",
    "\n",
    "    # 采集线程事件函数(从url队列中获取地址,发请求获取响应,放到解析队列)\n",
    "    def get_html(self):\n",
    "        while True:\n",
    "            if not self.url_queue.empty():\n",
    "                url = self.url_queue.get()\n",
    "                res = requests.get(url,headers=self.headers)\n",
    "                res.encoding = 'utf-8'\n",
    "                html = res.text\n",
    "                # 把html放到解析队列中\n",
    "                self.parse_queue.put(html)\n",
    "            else:\n",
    "                break\n",
    "    # 解析线程事件函数(解析+保存数据)\n",
    "    def parse_html(self):\n",
    "        while True:\n",
    "            if not self.parse_queue.empty():\n",
    "                html = self.parse_queue.get()\n",
    "                # html ：json格式的字符串,转为python数据类型\n",
    "                html = json.loads(html)\n",
    "                with open('小米.txt','a') as f:\n",
    "                    for h in html['data']:\n",
    "                        name = h['displayName']\n",
    "                        link = 'http://app.mi.com/details?id=' + h['packageName']\n",
    "                        f.write(name + '\\t' + link + '\\n')\n",
    "            else:\n",
    "                break\n",
    "    # 主函数\n",
    "    def work_on(self):\n",
    "        # url入队列\n",
    "        self.get_url()\n",
    "        c_list = []\n",
    "        p_list = []\n",
    "        # 创建多个采集线程\n",
    "        for i in range(10):\n",
    "            t = Thread(target=self.get_html)\n",
    "            c_list.append(t)\n",
    "            t.start()\n",
    "        for c in c_list:\n",
    "            c.join()\n",
    "        # 创建多个解析线程\n",
    "        for i in range(10):\n",
    "            t = Thread(target=self.parse_html)\n",
    "            p_list.append(t)\n",
    "            t.start()\n",
    "        for p in p_list:\n",
    "            p.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    spider = XiaomiSpider()\n",
    "    spider.work_on()\n",
    "    end = time.time()\n",
    "    print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建多进程爬取小米应用商城\n",
    "import requests\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Queue\n",
    "import json\n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "class XiaomiSpider(object):\n",
    "    def __init__(self):\n",
    "        # baseurl为抓包工具抓到的地址(去掉查询参数的)\n",
    "        self.baseurl = 'http://app.mi.com/categotyAllListApi?'\n",
    "        self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "        # 两个队列(url队列和解析队列)\n",
    "        self.url_queue = Queue()\n",
    "        self.parse_queue = Queue()\n",
    "    # URL入队列(拼10个url放入队列)\n",
    "    def get_url(self):\n",
    "        for i in range(50):\n",
    "            params = {\n",
    "                'page': str(i),\n",
    "                'categoryId': '2',\n",
    "                'pageSize': '30'\n",
    "            }\n",
    "            params = parse.urlencode(params)\n",
    "            # 拼接url地址,并入队列\n",
    "            url = self.baseurl + params\n",
    "            self.url_queue.put(url)\n",
    "\n",
    "    # 采集进程事件函数(从url队列中获取地址,发请求获取响应,放到解析队列)\n",
    "    def get_html(self):\n",
    "        while True:\n",
    "            if not self.url_queue.empty():\n",
    "                url = self.url_queue.get()\n",
    "                res = requests.get(url,headers=self.headers)\n",
    "                res.encoding = 'utf-8'\n",
    "                html = res.text\n",
    "                # 把html放到解析队列中\n",
    "                self.parse_queue.put(html)\n",
    "            else:\n",
    "                break\n",
    "    # 解析进程事件函数(解析+保存数据)\n",
    "    def parse_html(self):\n",
    "        while True:\n",
    "            if not self.parse_queue.empty():\n",
    "                html = self.parse_queue.get()\n",
    "                # html ：json格式的字符串,转为python数据类型\n",
    "                html = json.loads(html)\n",
    "                with open('小米.txt','a') as f:\n",
    "                    for h in html['data']:\n",
    "                        name = h['displayName']\n",
    "                        link = 'http://app.mi.com/details?id=' + h['packageName']\n",
    "                        f.write(name + '\\t' + link + '\\n')\n",
    "            else:\n",
    "                break\n",
    "    # 主函数\n",
    "    def work_on(self):\n",
    "        # url入队列\n",
    "        self.get_url()\n",
    "        c_list = []\n",
    "        p_list = []\n",
    "        # 创建多个采集进程\n",
    "        for i in range(4):\n",
    "            t = Process(target=self.get_html)\n",
    "            c_list.append(t)\n",
    "            t.start()\n",
    "        for c in c_list:\n",
    "            c.join()\n",
    "        # 创建多个解析线程\n",
    "        for i in range(4):\n",
    "            t = Process(target=self.parse_html)\n",
    "            p_list.append(t)\n",
    "            t.start()\n",
    "        for p in p_list:\n",
    "            p.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    spider = XiaomiSpider()\n",
    "    spider.work_on()\n",
    "    end = time.time()\n",
    "    print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 　一边采集一般解析\n",
    "import requests\n",
    "from threading import Thread\n",
    "from multiprocessing import Queue\n",
    "import json\n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "class XiaomiSpider(object):\n",
    "    def __init__(self):\n",
    "        # baseurl为抓包工具抓到的地址(去掉查询参数的)\n",
    "        self.baseurl = 'http://app.mi.com/categotyAllListApi?'\n",
    "        self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "        # 两个队列(url队列和解析队列)\n",
    "        self.url_queue = Queue()\n",
    "        self.parse_queue = Queue()\n",
    "    # URL入队列(拼10个url放入队列)\n",
    "    def get_url(self):\n",
    "        for i in range(50):\n",
    "            params = {\n",
    "                'page': str(i),\n",
    "                'categoryId': '2',\n",
    "                'pageSize': '30'\n",
    "            }\n",
    "            params = parse.urlencode(params)\n",
    "            # 拼接url地址,并入队列\n",
    "            url = self.baseurl + params\n",
    "            self.url_queue.put(url)\n",
    "\n",
    "    # 采集线程事件函数(从url队列中获取地址,发请求获取响应,放到解析队列)\n",
    "    def get_html(self):\n",
    "        while True:\n",
    "            if not self.url_queue.empty():\n",
    "                url = self.url_queue.get()\n",
    "                res = requests.get(url,headers=self.headers)\n",
    "                res.encoding = 'utf-8'\n",
    "                html = res.text\n",
    "                # 把html放到解析队列中\n",
    "                self.parse_queue.put(html)\n",
    "            else:\n",
    "                break\n",
    "    # 解析线程事件函数(解析+保存数据)\n",
    "    def parse_html(self):\n",
    "        while True:\n",
    "            try:\n",
    "                # get()不到值一定会阻塞\n",
    "                html = self.parse_queue.get(block=True,timeout=2)\n",
    "                # html ：json格式的字符串,转为python数据类型\n",
    "                html = json.loads(html)\n",
    "                with open('小米.txt','a') as f:\n",
    "                    for h in html['data']:\n",
    "                        name = h['displayName']\n",
    "                        link = 'http://app.mi.com/details?id=' + h['packageName']\n",
    "                        f.write(name + '\\t' + link + '\\n')\n",
    "            except:\n",
    "                break\n",
    "    # 主函数\n",
    "    def work_on(self):\n",
    "        # url入队列\n",
    "        self.get_url()\n",
    "        all_list = []\n",
    "        # 创建多个采集线程\n",
    "        for i in range(10):\n",
    "            t = Thread(target=self.get_html)\n",
    "            all_list.append(t)\n",
    "            t.start()\n",
    "        # 创建多个解析线程\n",
    "        for i in range(10):\n",
    "            t = Thread(target=self.parse_html)\n",
    "            all_list.append(t)\n",
    "            t.start()\n",
    "        # 统一回收所有线程\n",
    "        for p in all_list:\n",
    "            p.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    spider = XiaomiSpider()\n",
    "    spider.work_on()\n",
    "    end = time.time()\n",
    "    print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "风云雄霸天下\n",
      "风云雄霸天下\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''<div class=\"test\">风云雄霸天下</div>'''\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "r_list = soup.find_all('div',attrs={'class':'test'})\n",
    "for r in r_list:\n",
    "    print(r.text)\n",
    "    print(r.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第二梦\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''\n",
    "<div class=\"test\">风云雄霸天下</div>\n",
    "<div class=\"test\">聂风</div>\n",
    "<div class=\"test2\">\n",
    "    <span>第二梦</span>\n",
    "</div>\n",
    "'''\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "r_list = soup.find_all('div',attrs={'class':'test2'})\n",
    "for r in r_list:\n",
    "    # 获取class属性值为test2的div节点下面的span节点文本内容\n",
    "    print(r.span.string)\n",
    "\n",
    "# find_all ：找所有,列表\n",
    "# find     ：找到第一个就返回,节点对象\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
