{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import time\n",
    "import pymongo\n",
    "from lxml import etree\n",
    "\n",
    "class MaoyanSpider(object):\n",
    "  def __init__(self):\n",
    "    self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "    # 用来计数\n",
    "    self.page = 1\n",
    "    # 连接对象\n",
    "    self.conn = pymongo.MongoClient('localhost',27017)\n",
    "    # 库对象\n",
    "    self.db = self.conn['maoyandb']\n",
    "    # 集合对象\n",
    "    self.myset = self.db['top100']\n",
    "\n",
    "  # 获取页面\n",
    "  def get_page(self,url):\n",
    "    req = request.Request(url,headers=self.headers)\n",
    "    res = request.urlopen(req)\n",
    "    html = res.read().decode('utf-8')\n",
    "    # 直接调用解析函数,去对html做解析\n",
    "    self.parse_page(html)\n",
    "\n",
    "  # 解析页面\n",
    "  def parse_page(self,html):\n",
    "    # 创建解析对象\n",
    "    parse_html = etree.HTML(html)\n",
    "    # 基准xpath匹配\n",
    "    filmobj_list = parse_html.xpath('//dl[@class=\"board-wrapper\"]//dd')\n",
    "    # filmobj_list : ['element dd at ','']\n",
    "    # 循环遍历\n",
    "    for film in filmobj_list:\n",
    "      name = film.xpath('./a/@title')[0].strip()\n",
    "      star = film.xpath('.//p[@class=\"star\"]/text()')[0].strip()\n",
    "      time = film.xpath('.//p[@class=\"releasetime\"]/text()')[0].strip()\n",
    "      d = {\n",
    "        '电影名称:': name,\n",
    "        '电影主演:': star,\n",
    "        '上映时间:': time\n",
    "      }\n",
    "      print(d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # 保存数据\n",
    "  def write_mongo(self,r_list):\n",
    "    for r_t in r_list:\n",
    "      d = {\n",
    "        '电影名称:' : r_t[0].strip(),\n",
    "        '电影主演:' : r_t[1].strip(),\n",
    "        '上映时间:' : r_t[2].strip()\n",
    "      }\n",
    "      # 插入数据库\n",
    "      self.myset.insert_one(d)\n",
    "\n",
    "  # 主函数\n",
    "  def work_on(self):\n",
    "    for pn in range(0,41,10):\n",
    "      url = 'https://maoyan.com/board/4?offset=%s'\\\n",
    "                                          % str(pn)\n",
    "      self.get_page(url)\n",
    "      print('第%d页爬取成功' % self.page)\n",
    "      self.page += 1\n",
    "      time.sleep(2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  begin = time.time()\n",
    "  spider = MaoyanSpider()\n",
    "  spider.work_on()\n",
    "  end = time.time()\n",
    "  print('执行时间:%.2f' % (end-begin))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import time\n",
    "\n",
    "class TencentSpider(object):\n",
    "  def __init__(self):\n",
    "    self.headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)'}\n",
    "\n",
    "  # 获取工作职责和工作要求(发请求并解析二级页面)\n",
    "  def get_job_duty(self,url):\n",
    "    # 获取二级页面响应内容\n",
    "    res = requests.get(url,headers=self.headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    html = res.text\n",
    "    # xpath提取职责和要求\n",
    "    parse_html = etree.HTML(html)\n",
    "    jobobj_list = parse_html.xpath(\n",
    "      '//tr[@class=\"c\"]/td/ul[@class=\"squareli\"]')\n",
    "    # ['element ul at 地址','element ul at 地址']\n",
    "    duty = '\\n'.join(jobobj_list[0].xpath(\n",
    "                              './/li/text()'))\n",
    "    requirement = '\\n'.join(jobobj_list[1].xpath(\n",
    "                              './/li/text()'))\n",
    "    return duty,requirement\n",
    "\n",
    "\n",
    "\n",
    "  # 获取一级子界面职位信息\n",
    "  def get_job_info(self,url,params):\n",
    "    res = requests.get(url,params=params,headers=self.headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    html = res.text\n",
    "    # 交给解析函数去做解析\n",
    "    self.parse_job_info(html)\n",
    "\n",
    "  # 解析职位信息\n",
    "  def parse_job_info(self,html):\n",
    "    # 创建解析对象\n",
    "    parse_html = etree.HTML(html)\n",
    "    # 基准xpath,匹配所有职位节点对象tr\n",
    "    job_info_list = parse_html.xpath(\n",
    "       '//tr[@class=\"even\"] | //tr[@class=\"odd\"]')\n",
    "    # job_info_list:[tr对象1,tr对象2,...,tr对象10]\n",
    "    for tr in job_info_list:\n",
    "      job_link = 'https://hr.tencent.com/'+ \\\n",
    "                  tr.xpath('./td[1]/a/@href')[0]\n",
    "      job_name = tr.xpath('./td[1]/a/text()')[0]\n",
    "      job_type = tr.xpath('./td[2]/text()')[0]\n",
    "      job_number = tr.xpath('./td[3]/text()')[0]\n",
    "      job_address = tr.xpath('./td[4]/text()')[0]\n",
    "      job_time = tr.xpath('./td[5]/text()')[0]\n",
    "      job_duty,job_requirement = self.get_job_duty(job_link)\n",
    "\n",
    "\n",
    "      d = {\n",
    "        '职位链接:' : job_link,\n",
    "        '职位名称:' : job_name,\n",
    "        '职位类别:' : job_type,\n",
    "        '招聘人数:' : job_number,\n",
    "        '招聘地点:' : job_address,\n",
    "        '发布时间:' : job_time,\n",
    "        '工作职责:' : job_duty,\n",
    "        '工作要求:' : job_requirement\n",
    "      }\n",
    "      print(d)\n",
    "      print('*' * 30)\n",
    "\n",
    "  # 数据处理\n",
    "  def write_page(self):\n",
    "    pass\n",
    "\n",
    "  # 主函数\n",
    "  def work_on(self):\n",
    "    job = input('请输入职位方向:')\n",
    "\n",
    "    for pn in range(0,2001,10):\n",
    "      url = 'https://hr.tencent.com/position.php?'\n",
    "      params = {\n",
    "          'keywords': job,\n",
    "          'start' : str(pn)\n",
    "        }\n",
    "      self.get_job_info(url,params)\n",
    "      time.sleep(2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  start = time.time()\n",
    "  spider = TencentSpider()\n",
    "  spider.work_on()\n",
    "  end = time.time()\n",
    "  print('执行时间:%.2f' % (end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from hashlib import md5\n",
    "import time\n",
    "import random\n",
    "\n",
    "# url一定要写F12抓包抓到的POST的地址\n",
    "url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule'\n",
    "headers = {\n",
    "        # 下面三个是反爬检查较多的三个字段\n",
    "        'Cookie':'OUTFOX_SEARCH_USER_ID=1516386930@10.169.0.84; OUTFOX_SEARCH_USER_ID_NCOO=760569518.7197; JSESSIONID=aaa1dHsc8I4yNgrvW7LNw; td_cookie=18446744069709674982; ___rl__test__cookies=1554368672202',\n",
    "        'Referer':'http://fanyi.youdao.com/',\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}\n",
    "\n",
    "# ts\n",
    "ts = str(int(time.time()*1000))\n",
    "# salt\n",
    "salt = str(int(time.time()*1000))+\\\n",
    "                   str(random.randint(0,10))\n",
    "# sign\n",
    "key = input('请输入要翻译的单词:')\n",
    "string = \"fanyideskweb\" + key + salt + \\\n",
    "                   \"1L5ja}w$puC.v_Kz3@yYn\"\n",
    "sign = md5()\n",
    "sign.update(string.encode('utf-8'))\n",
    "sign = sign.hexdigest()\n",
    "# 定义form表单数据为字典\n",
    "data = {\n",
    "        'i': key,\n",
    "        'from': 'AUTO',\n",
    "        'to': 'AUTO',\n",
    "        'smartresult': 'dict',\n",
    "        'client': 'fanyideskweb',\n",
    "        'salt': salt,\n",
    "        'sign': sign,\n",
    "        'ts': ts,\n",
    "        'bv': 'd6c3cd962e29b66abe48fcb8f4dd7f7d',\n",
    "        'doctype':'json',\n",
    "        'version': '2.1',\n",
    "        'keyfrom': 'fanyi.web',\n",
    "        'action': 'FY_BY_REALTlME',\n",
    "        'typoResult': 'false',\n",
    "}\n",
    "\n",
    "# 发请求获取响应内容\n",
    "res = requests.post(url,data=data,headers=headers)\n",
    "res.encoding = 'utf-8'\n",
    "html = res.text\n",
    "\n",
    "print(html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
