{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "```python\n",
    "\n",
    "1、scrapy框架\n",
    "\n",
    "  1、定义 ：异步处理框架,可配置和可扩展程度相当高,使用最广泛的爬虫框架\n",
    "\n",
    "2、scrapy框架五大组件\n",
    "  * 引擎(Engine) ：整个框架核心\n",
    "  * 调度器(Scheduler) ：接受从引擎发过来的URL\n",
    "  * 下载器(Downloader)：获取响应对象\n",
    "  * 爬虫文件(Spider)  ：解析数据\n",
    "  * 项目管道(Item Pipeline) ：数据处理\n",
    "  ** 下载器中间件(Downloader Middlewares)\n",
    "     请求从调度器 -> 下载器,做拦截处理\n",
    "  ** 蜘蛛中间件(Spider Middlewares)\n",
    "     响应从下载器 -> 爬虫程序,做拦截处理\n",
    "  ** item ：定义爬取的数据结构\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3、制作爬虫项目步骤\n",
    "  * 新建项目\n",
    "    scrapy startproject 项目名\n",
    "  * 创建爬虫文件\n",
    "    cd 项目目录\n",
    "    scrapy genspider 文件名 域名\n",
    "  * 明确目标(items.py)\n",
    "  * 写爬虫文件(数据提取)\n",
    "  * 数据处理(pipelines.py)\n",
    "  * 全局配置(settings.py)\n",
    "  * 运行爬虫\n",
    "    scrapy crawl 爬虫名\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4、项目目录结构\n",
    "  Baidu\n",
    "  ├── Baidu              # 项目目录\n",
    "  │   ├── items.py       # 定义数据结构\n",
    "  │   ├── middlewares.py # 中间件\n",
    "  │   ├── pipelines.py   # 数据处理\n",
    "  │   ├── settings.py    # 全局配置\n",
    "  │   └── spiders\n",
    "  │       └── baidu.py   # 爬虫文件\n",
    "  └── scrapy.cfg         # 基本配置文件\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5、settings.py详解\n",
    "  * USER_AGENT = 'Mozilla/5.0'\n",
    "  * 是否遵守robots协议,一定改为False\n",
    "    ROBOTSTXT_OBEY = False\n",
    "  * 设置最大并发量(默认16)\n",
    "    CONCURRENT_REQUESTS = 10\n",
    "  * 下载延迟时间\n",
    "    DOWNLOAD_DELAY = 0.2\n",
    "  * 请求头headers\n",
    "    DEFAULT_REQUEST_HEADERS = {}\n",
    "  * 项目管道(项目目录名.pipelines.类名)\n",
    "    优先级1-1000,数字越小优先级越高\n",
    "    ITEM_PIPELINES = {\n",
    "       'Baidu.pipelines.BaiduPipeline': 300,\n",
    "       'Baidu.pipelines.BaiduMongoPipeline': 100,\n",
    "    }\n",
    "6、抓去百度首页源码,保存到文件中\n",
    "  1、scrapy startproject Baidu\n",
    "  2、cd Baidu\n",
    "  3、scrapy genspider baidu www.baidu.com\n",
    "  4、items.py(此项目不用改)\n",
    "  5、baidu.py(爬虫文件)\n",
    "     def parse(self,response):\n",
    "        print('***************')\n",
    "  6、pipeline.py(此项目不用改)\n",
    "  7、settings.py\n",
    "     ROBOTSTXT_OBEY = False\n",
    "     DEFAULT_REQUEST_HEADERS = {'User-Agent':''}\n",
    "  8、scrapy crawl baidu (在项目目录下执行)\n",
    "7、pycharm执行scrapy项目\n",
    "  * 创建begin.py(和scrapy.cfg同路径)\n",
    "  * begin.py\n",
    "     from scrapy import cmdline\n",
    "     cmdline.execute('scrapy crawl baidu'.split())\n",
    "8、yield回顾\n",
    "  1、作用 ：把1个函数当作1个生成器来使用\n",
    "  2、特点 ：让函数暂停,等待下一次调用\n",
    "9、Csdn页面抓取\n",
    "  1、URL：https://blog.csdn.net/java123456789010/article/details/89061997\n",
    "  2、标题、发布时间、阅读数\n",
    "     标题 ： //h1[@class=\"title-article\"]/text()\n",
    "     时间 ： //span[@class=\"time\"]/text()\n",
    "     数量 ： //span[@class=\"read-count\"]/text()\n",
    "  3、实现\n",
    "     * Csdn\n",
    "     * csdn\n",
    "10、知识点\n",
    "  * extract() ：获取选择器对象中的文本内容\n",
    "    response.xpath('')\n",
    "      结果 ：[<selector...data='文本',<selector..]\n",
    "    response.xpath('').extract()\n",
    "      结果 ：['文本1','文本2']\n",
    "  * pipelines.py中必须有1个函数叫：\n",
    "    def process_item(self,item,spider):\n",
    "        return item\n",
    "  * 警告级别(日志文件settings.py)\n",
    "    LOG_LEVEL = ''\n",
    "    LOG_FILE = '文件名.log'\n",
    "    ** LOG_FILE一旦设置,所有信息将会写入到该文件*\n",
    "    ** 设置LOG_LEVEL后,只会显示该级别及以上信息**\n",
    "    **5层日志级别(从到到低排列)**\n",
    "    CRITICAL ：严重错误\n",
    "    ERROR    ：普通错误\n",
    "    WARNING  ：警告信息\n",
    "    INFO     ：一般信息\n",
    "    DEBUG    ：调试信息\n",
    "11、盗墓笔记小说抓取\n",
    "  1、起始URL ：http://www.daomubiji.com/\n",
    "  2、目标\n",
    "     * 抓取系列小说的 \n",
    "       标题、章节数量、章节名称、章节链接\n",
    "     * 把所有章节小说内容抓取到本地文件\n",
    "       七星鲁王-第一章-血尸.txt\n",
    "  3、准备工作(xpath)\n",
    "     ******* 一级界面********\t\t//article[@class=\"article-content\"]/a/@href\n",
    "     ******** 二级界面*******\n",
    "     * r_list = response.xpath('//article[@class=\"excerpt excerpt-c3\"])\n",
    "      结果 ：[节点对象1,节点对象2]\n",
    "     获取info ：info = article.xpath('./a/text()').extract()[0].split()\n",
    "     * for r in r_list:\n",
    "         卷名 ：info[0]\n",
    "         数量 ：info[1]\n",
    "         名称 ：info[2]\n",
    "         章节链接 ：r.xpath('./a/@href').extract()[0] \n",
    "     \n",
    "     *********三级页面************\n",
    "      //article[@class=\"article-content\"]/p/text()\n",
    "      ['段落1','段落2','段落3','...']\n",
    "      '\\n'.join(L)\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
