{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scray框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from Daomu.items import DaomuItem\n",
    "\n",
    "class DaomuSpider(scrapy.Spider):\n",
    "    name = 'daomu'\n",
    "    allowed_domains = ['daomubiji.com']\n",
    "    start_urls = ['http://www.daomubiji.com/']\n",
    "\n",
    "    # 一级页面解析,获取所有小说的链接\n",
    "    def parse(self, response):\n",
    "        # 盗墓笔记全集链接列表['http://...','http://...']\n",
    "        one_links_list = response.xpath('//ul[@class=\"sub-menu\"]//a/@href').extract()\n",
    "        print(one_links_list)\n",
    "        # 把链接交给调度器入队列\n",
    "        for one_link in one_links_list:\n",
    "            yield scrapy.Request(\n",
    "                        one_link,\n",
    "                        callback=self.parse_two_link\n",
    "                )\n",
    "\n",
    "    # 解析二级页面方法\n",
    "    def parse_two_link(self,response):\n",
    "        # 基准xpath,匹配所有节点对象的列表\n",
    "        article_list = response.xpath('//article[@class=\"excerpt excerpt-c3\"]')\n",
    "        for article in article_list:\n",
    "            # 创建item对象\n",
    "            item = DaomuItem()\n",
    "            info = article.xpath('./a/text()')\\\n",
    "                                    .extract()[0].split()\n",
    "            # info:['七星鲁王','第一章,'血尸']\n",
    "            item['zh_link'] = article.xpath('./a/@href') \\\n",
    "                .extract()[0]\n",
    "            if len(info) == 3:\n",
    "                item['juan_name'] = info[0]\n",
    "                item['zh_number'] = info[1]\n",
    "                item['zh_name'] = info[2]\n",
    "            elif len(info) == 2:\n",
    "                item['juan_name'] = info[0]\n",
    "                item['zh_number'] = info[1]\n",
    "                item['zh_name'] = '无题'\n",
    "            elif len(info) == 4:\n",
    "                item['juan_name'] = info[0]+info[1]\n",
    "                item['zh_number'] = info[2]\n",
    "                item['zh_name'] = info[3]\n",
    "            else:\n",
    "                item['juan_name'] = '未匹配'\n",
    "                item['zh_number'] = '未匹配'\n",
    "                item['zh_name'] = '未匹配'\n",
    "\n",
    "            # 将章节链接交给调度器\n",
    "            # 必须把item传递到下一个函数,利用meta参数\n",
    "            yield scrapy.Request(\n",
    "                item['zh_link'],\n",
    "                meta={'item':item},\n",
    "                callback=self.parse_three_link\n",
    "            )\n",
    "    # 解析三级页面\n",
    "    def parse_three_link(self,response):\n",
    "        item = response.meta['item']\n",
    "        content = response.xpath('//article[@class=\"article-content\"]/p/text()').extract()\n",
    "        content = '\\n'.join(content)\n",
    "        item['zh_content'] = content\n",
    "\n",
    "        yield item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from urllib import parse\n",
    "import json\n",
    "from So.items import SoItem\n",
    "\n",
    "class SoSpider(scrapy.Spider):\n",
    "    name = 'so'\n",
    "    allowed_domains = ['image.so.com']\n",
    "    # 重写start_requests()方法,自定义URL地址和解析方法\n",
    "    # 不一定非得是parse函数\n",
    "    # 构造请求,去交给调度器\n",
    "    def start_requests(self):\n",
    "        # 拼接URL地址\n",
    "        baseurl = 'http://image.so.com/zj?'\n",
    "        for i in range(0,301,30):\n",
    "            params = {\n",
    "                'ch' : 'beauty',\n",
    "                'sn' : str(i),\n",
    "                'listtype' : 'new',\n",
    "                'temp' : '1'\n",
    "            }\n",
    "            url = baseurl + parse.urlencode(params)\n",
    "            # 构造请求,交给调度器\n",
    "            yield scrapy.Request(url,callback=self.parse_so)\n",
    "\n",
    "    # 定义解析函数\n",
    "    def parse_so(self,response):\n",
    "        # 获取响应内容\n",
    "        html = response.text\n",
    "        # 内容为json,转为python数据类型\n",
    "        html = json.loads(html)\n",
    "        for img in html['list']:\n",
    "            # 创建item对象\n",
    "            item = SoItem()\n",
    "            item['img_link'] = img['qhimg_url']\n",
    "            # 交给管道\n",
    "            yield item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from Tencent.items import TencentItem\n",
    "\n",
    "class TencentSpider(scrapy.Spider):\n",
    "    name = 'tencent'\n",
    "    allowed_domains = ['hr.tencent.com']\n",
    "    start_urls = [\n",
    "        'https://hr.tencent.com/position.php?start=0'\n",
    "    ]\n",
    "    baseurl = 'https://hr.tencent.com/position.php?start='\n",
    "\n",
    "    def parse(self, response):\n",
    "        #　把所有页的URL地址交给调度器\n",
    "        for page in range(0,21,10):\n",
    "            url = self.baseurl + str(page)\n",
    "            yield scrapy.Request(\n",
    "                    url,\n",
    "                    callback=self.parse_one_link\n",
    "                )\n",
    "    # 一级页面解析函数\n",
    "    def parse_one_link(self,response):\n",
    "        # 基准xpath,匹配节点对象列表\n",
    "        job_list = response.xpath('//tr[@class=\"even\"] | //tr[@class=\"odd\"]')\n",
    "        # 依次遍历每个职位对象\n",
    "        for job in job_list:\n",
    "            # 创建item对象\n",
    "            item = TencentItem()\n",
    "            item['zh_name'] = job.xpath('./td[1]/a/text()').extract()[0]\n",
    "            # 因为有的职位类别不存在\n",
    "            item['zh_type'] = job.xpath('./td[2]/text()').extract()\n",
    "            if item['zh_type']:\n",
    "                item['zh_type'] = item['zh_type'][0]\n",
    "            else:\n",
    "                item['zh_type'] = '无'\n",
    "\n",
    "            item['zh_number'] = job.xpath('./td[3]/text()').extract()[0]\n",
    "            item['zh_address'] = job.xpath('./td[4]/text()').extract()[0]\n",
    "            item['zh_time'] = job.xpath('./td[5]/text()').extract()[0]\n",
    "            item['zh_link'] = 'https://hr.tencent.com/' + \\\n",
    "                       job.xpath('./td[1]/a/@href').extract()[0]\n",
    "\n",
    "            yield scrapy.Request(\n",
    "                    item['zh_link'],\n",
    "                    meta={'item':item},\n",
    "                    callback=self.parse_two_link\n",
    "                )\n",
    "    # 解析二级页面的函数\n",
    "    def parse_two_link(self,response):\n",
    "        item = response.meta['item']\n",
    "        # 基准xpath,匹配工作职责和要求的两个节点对象\n",
    "        duty_list = response.xpath('//tr[@class=\"c\"]/td/ul[@class=\"squareli\"]')\n",
    "        # 工作职责\n",
    "        job_duty = '\\n'.join(duty_list[0].xpath('.//li/text()').extract())\n",
    "        # 工作要求\n",
    "        job_requirement = '\\n'.join(duty_list[1].xpath('.//li/text()').extract())\n",
    "        item['zh_duty'] = job_duty\n",
    "        item['zh_requirement'] = job_requirement\n",
    "\n",
    "        yield item\n",
    "# //tr[@class=\"c\"]/td/ul[@class=\"squareli\"]\n",
    "# '\\n'.join(L[0].xpath('.//li/text()').extract())\n",
    "# '\\n'.join(L[1].xpath('.//li/text()').extract())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
